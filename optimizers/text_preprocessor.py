# -*- coding: utf-8 -*-
"""
æ–‡æœ¬é¢„å¤„ç†å™¨

è´Ÿè´£æ–‡æœ¬æ¸…ç†ã€æ ¼å¼åŒ–å’Œä¼˜åŒ–ï¼Œä¸ºåç»­å¤„ç†åšå‡†å¤‡
"""

import re
from typing import Dict, List, Any, Optional, Tuple
import logging

from utils.logger import LoggerMixin

logger = logging.getLogger(__name__)

class TextPreprocessor(LoggerMixin):
    """æ–‡æœ¬é¢„å¤„ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ–‡æœ¬é¢„å¤„ç†å™¨
        
        Args:
            config: é¢„å¤„ç†é…ç½®
        """
        self.config = config
        self.min_section_length = config.get('min_section_length', 100)
        self.max_section_length = config.get('max_section_length', 3000)
        self.compression_ratio = config.get('compression_ratio', 0.7)
        
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self._init_patterns()
    
    def _init_patterns(self):
        """åˆå§‹åŒ–æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        # ç« èŠ‚æ ‡é¢˜æ¨¡å¼
        self.section_patterns = {
            'abstract': re.compile(r'\b(?:abstract|æ‘˜è¦|summary)\b', re.IGNORECASE),
            'introduction': re.compile(r'\b(?:introduction|å¼•è¨€|å‰è¨€|èƒŒæ™¯|background)\b', re.IGNORECASE),
            'methods': re.compile(r'\b(?:methods?|methodology|ææ–™ä¸æ–¹æ³•|æ–¹æ³•|materials?\s+and\s+methods?)\b', re.IGNORECASE),
            'results': re.compile(r'\b(?:results?|findings|ç»“æœ|å‘ç°)\b', re.IGNORECASE),
            'discussion': re.compile(r'\b(?:discussion|è®¨è®º|åˆ†æ)\b', re.IGNORECASE),
            'conclusion': re.compile(r'\b(?:conclusions?|ç»“è®º|æ€»ç»“)\b', re.IGNORECASE),
            'references': re.compile(r'\b(?:references?|reference\s+list|bibliography|å‚è€ƒæ–‡çŒ®)\b', re.IGNORECASE),
            'acknowledgments': re.compile(r'\b(?:acknowledgments?|acknowledgements?|è‡´è°¢)\b', re.IGNORECASE),
            'funding': re.compile(r'\b(?:funding|financial\s+support|èµ„åŠ©|åŸºé‡‘)\b', re.IGNORECASE)
        }
        
        # æ¸…ç†æ¨¡å¼
        self.cleanup_patterns = {
            'multiple_spaces': re.compile(r'\s+'),
            'multiple_newlines': re.compile(r'\n\s*\n\s*\n+'),
            'email': re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
            'url': re.compile(r'https?://[^\s<>"{}|\\^`[\]]+'),
            'doi': re.compile(r'doi:\s*10\.\d+/[^\s]+', re.IGNORECASE),
            'figure_ref': re.compile(r'\b(?:fig(?:ure)?\s*\.?\s*\d+|å›¾\s*\d+)\b', re.IGNORECASE),
            'table_ref': re.compile(r'\b(?:table\s*\.?\s*\d+|è¡¨\s*\d+)\b', re.IGNORECASE),
            'citation': re.compile(r'\[\d+(?:[-â€“â€”,]\d+)*\]|\(\d+(?:[-â€“â€”,]\d+)*\)'),
            'page_numbers': re.compile(r'\b(?:page|p\.)\s*\d+\b', re.IGNORECASE)
        }
    
    def clean_text(self, text: str, preserve_structure: bool = True) -> str:
        """
        æ¸…ç†æ–‡æœ¬å†…å®¹
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            preserve_structure: æ˜¯å¦ä¿ç•™ç»“æ„
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        cleaned_text = text
        
        # ç§»é™¤URLå’Œé‚®ç®±
        cleaned_text = self.cleanup_patterns['url'].sub('', cleaned_text)
        cleaned_text = self.cleanup_patterns['email'].sub('', cleaned_text)
        
        # ç®€åŒ–å¼•ç”¨æ ¼å¼
        cleaned_text = self.cleanup_patterns['citation'].sub('[REF]', cleaned_text)
        cleaned_text = self.cleanup_patterns['figure_ref'].sub('[FIGURE]', cleaned_text)
        cleaned_text = self.cleanup_patterns['table_ref'].sub('[TABLE]', cleaned_text)
        
        # ç§»é™¤é¡µç 
        cleaned_text = self.cleanup_patterns['page_numbers'].sub('', cleaned_text)
        
        # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        cleaned_text = self.cleanup_patterns['multiple_spaces'].sub(' ', cleaned_text)
        
        if preserve_structure:
            # ä¿ç•™æ®µè½ç»“æ„ï¼Œä½†é™åˆ¶è¿ç»­æ¢è¡Œ
            cleaned_text = self.cleanup_patterns['multiple_newlines'].sub('\n\n', cleaned_text)
        else:
            # ç§»é™¤æ‰€æœ‰æ¢è¡Œï¼Œè½¬ä¸ºå•è¡Œ
            cleaned_text = cleaned_text.replace('\n', ' ')
            cleaned_text = self.cleanup_patterns['multiple_spaces'].sub(' ', cleaned_text)
        
        return cleaned_text.strip()
    
    def identify_sections(self, text: str) -> Dict[str, Tuple[int, int]]:
        """
        è¯†åˆ«æ–‡æœ¬ä¸­çš„ç« èŠ‚ä½ç½®
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            ç« èŠ‚ä½ç½®å­—å…¸ {ç« èŠ‚å: (å¼€å§‹ä½ç½®, ç»“æŸä½ç½®)}
        """
        sections = {}
        text_lower = text.lower()
        
        # æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„ç« èŠ‚å¼€å§‹ä½ç½®
        section_positions = []
        
        for section_name, pattern in self.section_patterns.items():
            for match in pattern.finditer(text_lower):
                start_pos = match.start()
                
                # æ£€æŸ¥æ˜¯å¦åœ¨è¡Œé¦–æˆ–å‰é¢æ˜¯æ¢è¡Œç¬¦
                if start_pos == 0 or text[start_pos-1] in '\n\r':
                    section_positions.append((start_pos, section_name))
        
        # æŒ‰ä½ç½®æ’åº
        section_positions.sort(key=lambda x: x[0])
        
        # ç¡®å®šæ¯ä¸ªç« èŠ‚çš„ç»“æŸä½ç½®
        for i, (start_pos, section_name) in enumerate(section_positions):
            if i + 1 < len(section_positions):
                end_pos = section_positions[i + 1][0]
            else:
                end_pos = len(text)
            
            # æ£€æŸ¥ç« èŠ‚é•¿åº¦
            section_length = end_pos - start_pos
            if section_length >= self.min_section_length:
                sections[section_name] = (start_pos, end_pos)
        
        return sections
    
    def extract_sections(self, text: str, 
                        target_sections: Optional[List[str]] = None) -> Dict[str, str]:
        """
        æå–ç‰¹å®šç« èŠ‚çš„æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            target_sections: ç›®æ ‡ç« èŠ‚åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰ç« èŠ‚
            
        Returns:
            ç« èŠ‚æ–‡æœ¬å­—å…¸
        """
        if not text:
            return {}
        
        # è¯†åˆ«ç« èŠ‚ä½ç½®
        section_positions = self.identify_sections(text)
        
        if not section_positions:
            # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ç« èŠ‚ï¼Œè¿”å›æ•´ä¸ªæ–‡æœ¬
            return {'full_text': text}
        
        # æå–ç›®æ ‡ç« èŠ‚
        if target_sections is None:
            target_sections = list(section_positions.keys())
        
        extracted_sections = {}
        
        for section_name in target_sections:
            if section_name in section_positions:
                start_pos, end_pos = section_positions[section_name]
                section_text = text[start_pos:end_pos].strip()
                
                # æ¸…ç†ç« èŠ‚æ–‡æœ¬
                section_text = self.clean_text(section_text, preserve_structure=True)
                
                if len(section_text) >= self.min_section_length:
                    extracted_sections[section_name] = section_text
        
        return extracted_sections
    
    def compress_text(self, text: str, target_ratio: Optional[float] = None) -> str:
        """
        æ™ºèƒ½å‹ç¼©æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            target_ratio: ç›®æ ‡å‹ç¼©æ¯”ä¾‹
            
        Returns:
            å‹ç¼©åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        target_ratio = target_ratio or self.compression_ratio
        target_length = int(len(text) * target_ratio)
        
        if len(text) <= target_length:
            return text
        
        # æŒ‰å¥å­åˆ†å‰²
        sentences = self._split_sentences(text)
        
        if not sentences:
            # å¦‚æœæ— æ³•åˆ†å‰²å¥å­ï¼Œæˆªå–å‰éƒ¨åˆ†
            return text[:target_length] + "..."
        
        # è®¡ç®—å¥å­é‡è¦æ€§åˆ†æ•°
        sentence_scores = self._score_sentences(sentences)
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶é€‰æ‹©é‡è¦å¥å­
        scored_sentences = list(zip(sentences, sentence_scores, range(len(sentences))))
        scored_sentences.sort(key=lambda x: (-x[1], x[2]))  # æŒ‰åˆ†æ•°é™åºï¼Œä½ç½®å‡åº
        
        # é€‰æ‹©å¥å­ç›´åˆ°è¾¾åˆ°ç›®æ ‡é•¿åº¦
        selected_sentences = []
        current_length = 0
        
        for sentence, score, original_index in scored_sentences:
            if current_length + len(sentence) <= target_length:
                selected_sentences.append((sentence, original_index))
                current_length += len(sentence)
            else:
                break
        
        # æŒ‰åŸå§‹é¡ºåºæ’åˆ—é€‰ä¸­çš„å¥å­
        selected_sentences.sort(key=lambda x: x[1])
        compressed_text = ' '.join([sentence for sentence, _ in selected_sentences])
        
        return compressed_text
    
    def _split_sentences(self, text: str) -> List[str]:
        """
        åˆ†å‰²å¥å­
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            å¥å­åˆ—è¡¨
        """
        # ç®€å•çš„å¥å­åˆ†å‰²ï¼ˆå¯ä»¥æ”¹è¿›ï¼‰
        sentence_pattern = re.compile(r'[.!?ã€‚ï¼ï¼Ÿ]+\s+')
        sentences = sentence_pattern.split(text)
        
        # è¿‡æ»¤ç©ºå¥å­å’Œè¿‡çŸ­å¥å­
        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        
        return sentences
    
    def _score_sentences(self, sentences: List[str]) -> List[float]:
        """
        è®¡ç®—å¥å­é‡è¦æ€§åˆ†æ•°
        
        Args:
            sentences: å¥å­åˆ—è¡¨
            
        Returns:
            åˆ†æ•°åˆ—è¡¨
        """
        scores = []
        
        # å…³é”®è¯æƒé‡
        important_keywords = [
            'method', 'result', 'conclusion', 'significant', 'important',
            'æ–¹æ³•', 'ç»“æœ', 'ç»“è®º', 'æ˜¾è‘—', 'é‡è¦', 'å‘ç°', 'è¡¨æ˜', 'è¯æ˜'
        ]
        
        for sentence in sentences:
            score = 0.0
            sentence_lower = sentence.lower()
            
            # é•¿åº¦åˆ†æ•°ï¼ˆä¸­ç­‰é•¿åº¦å¥å­å¾—åˆ†æ›´é«˜ï¼‰
            length_score = 1.0 - abs(len(sentence) - 100) / 200
            score += max(0, length_score) * 0.3
            
            # å…³é”®è¯åˆ†æ•°
            keyword_count = sum(1 for keyword in important_keywords 
                              if keyword in sentence_lower)
            score += keyword_count * 0.4
            
            # æ•°å­—åˆ†æ•°ï¼ˆåŒ…å«æ•°å­—çš„å¥å­å¯èƒ½æ›´é‡è¦ï¼‰
            if re.search(r'\d+', sentence):
                score += 0.2
            
            # ä½ç½®åˆ†æ•°ï¼ˆå¼€å¤´å’Œç»“å°¾çš„å¥å­æ›´é‡è¦ï¼‰
            # è¿™ä¸ªåœ¨è°ƒç”¨å‡½æ•°ä¸­å¤„ç†
            
            scores.append(score)
        
        return scores
    
    def optimize_for_llm(self, text: str, max_tokens: int = 4000) -> str:
        """
        ä¸ºLLMä¼˜åŒ–æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰
            
        Returns:
            ä¼˜åŒ–åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ç²—ç•¥ä¼°è®¡ï¼š1ä¸ªtokençº¦ç­‰äº4ä¸ªå­—ç¬¦ï¼ˆä¸­è‹±æ–‡æ··åˆï¼‰
        max_chars = max_tokens * 4
        
        if len(text) <= max_chars:
            return self.clean_text(text)
        
        self.logger.debug(f"æ–‡æœ¬è¿‡é•¿ï¼ˆ{len(text)}å­—ç¬¦ï¼‰ï¼Œå¼€å§‹ä¼˜åŒ–...")
        
        # æå–å…³é”®ç« èŠ‚
        key_sections = ['abstract', 'introduction', 'methods', 'results', 'discussion', 'conclusion']
        sections = self.extract_sections(text, key_sections)
        
        if not sections:
            # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ç« èŠ‚ï¼Œç›´æ¥å‹ç¼©
            return self.compress_text(text, max_chars / len(text))
        
        # æŒ‰é‡è¦æ€§åˆ†é…å­—ç¬¦é…é¢
        section_quotas = {
            'abstract': 0.15,
            'introduction': 0.20,
            'methods': 0.25,
            'results': 0.25,
            'discussion': 0.15
        }
        
        optimized_sections = {}
        total_quota_used = 0
        
        for section_name, section_text in sections.items():
            quota = section_quotas.get(section_name, 0.1)
            section_max_chars = int(max_chars * quota)
            
            if len(section_text) > section_max_chars:
                optimized_text = self.compress_text(section_text, section_max_chars / len(section_text))
            else:
                optimized_text = self.clean_text(section_text)
            
            optimized_sections[section_name] = optimized_text
            total_quota_used += len(optimized_text)
        
        # ç»„åˆä¼˜åŒ–åçš„æ–‡æœ¬
        result_parts = []
        for section_name in key_sections:
            if section_name in optimized_sections:
                result_parts.append(f"=== {section_name.upper()} ===")
                result_parts.append(optimized_sections[section_name])
                result_parts.append("")
        
        optimized_text = "\n".join(result_parts).strip()
        
        self.logger.debug(f"æ–‡æœ¬ä¼˜åŒ–å®Œæˆï¼š{len(text)} -> {len(optimized_text)} å­—ç¬¦")
        
        return optimized_text
    
    def preprocess_batch(self, papers: List[Dict[str, Any]], 
                        max_tokens: int = 4000) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡é¢„å¤„ç†æ–‡çŒ®æ–‡æœ¬
        
        Args:
            papers: æ–‡çŒ®åˆ—è¡¨
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            é¢„å¤„ç†åçš„æ–‡çŒ®åˆ—è¡¨
        """
        self.logger.info(f"ğŸ“ å¼€å§‹æ‰¹é‡é¢„å¤„ç†æ–‡æœ¬ï¼Œå…± {len(papers)} ç¯‡æ–‡çŒ®")
        
        processed_papers = []
        
        for i, paper in enumerate(papers, 1):
            self.logger.debug(f"é¢„å¤„ç†ç¬¬ {i}/{len(papers)} ç¯‡æ–‡çŒ®...")
            
            full_text = paper.get('full_text', '')
            
            if full_text:
                try:
                    # ä¼˜åŒ–æ–‡æœ¬
                    optimized_text = self.optimize_for_llm(full_text, max_tokens)
                    
                    # æ›´æ–°æ–‡çŒ®è®°å½•
                    processed_paper = paper.copy()
                    processed_paper['full_text'] = optimized_text
                    processed_paper['original_text_length'] = len(full_text)
                    processed_paper['optimized_text_length'] = len(optimized_text)
                    processed_paper['compression_ratio'] = len(optimized_text) / len(full_text) if full_text else 1.0
                    
                    processed_papers.append(processed_paper)
                    
                except Exception as e:
                    pmid = paper.get('PMID', 'Unknown')
                    self.logger.error(f"âŒ é¢„å¤„ç†æ–‡çŒ® {pmid} å¤±è´¥: {e}")
                    processed_papers.append(paper)  # ä¿ç•™åŸå§‹æ–‡çŒ®
            else:
                processed_papers.append(paper)  # æ²¡æœ‰å…¨æ–‡çš„æ–‡çŒ®ä¿æŒä¸å˜
        
        # ç»Ÿè®¡ç»“æœ
        optimized_count = sum(1 for p in processed_papers if 'compression_ratio' in p)
        avg_compression = sum(p.get('compression_ratio', 1.0) for p in processed_papers) / len(processed_papers)
        
        self.logger.info(f"âœ… æ‰¹é‡é¢„å¤„ç†å®Œæˆ: {optimized_count}/{len(papers)} ç¯‡ä¼˜åŒ–")
        self.logger.info(f"ğŸ“Š å¹³å‡å‹ç¼©æ¯”: {avg_compression:.2f}")
        
        return processed_papers