# -*- coding: utf-8 -*-
"""
æ–‡æœ¬æå–æ¨¡å—

è´Ÿè´£ä» PMC å…¨æ–‡å’Œ PDF æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹
åŒ…å«æ™ºèƒ½ç« èŠ‚ç­›é€‰å’Œæ–‡æœ¬ä¼˜åŒ–åŠŸèƒ½
"""

import os
import json
import time
import requests
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import re

from utils.logger import LoggerMixin
from utils.file_handler import FileHandler
from utils.api_manager import api_manager

logger = logging.getLogger(__name__)


class TextExtractor(LoggerMixin):
    """æ–‡æœ¬æå–å™¨"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ–‡æœ¬æå–å™¨

        Args:
            config: æå–é…ç½®
        """
        self.config = config
        # ç§»é™¤æ–‡æœ¬é•¿åº¦é™åˆ¶ï¼Œæ”¯æŒå…¨æ–‡æå–
        self.section_filters = config.get('section_filters', [])
        self.exclude_sections = config.get('exclude_sections', [])
        self.key_section_ratio = config.get('key_section_ratio', {})

        # BioC API ç¼“å­˜é…ç½®
        self.enable_bioc_cache = config.get('enable_bioc_cache', True)
        self.cache_dir = Path(config.get('cache_dir', 'cache/bioc'))
        self.cache_ttl = config.get('cache_ttl', 86400)  # 24 å°æ—¶

        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        if self.enable_bioc_cache:
            self.cache_dir.mkdir(parents=True, exist_ok=True)

        # å»¶è¿Ÿå¯¼å…¥é‡é‡çº§åº“
        self._fitz = None
        self._pdf2image = None
        self._pytesseract = None
        self._PIL_Image = None

    def _get_bioc_cache_path(self, pmid: str, format_type: str = "json") -> Path:
        """
        è·å– BioC æ–‡æ¡£ç¼“å­˜è·¯å¾„

        Args:
            pmid: æ–‡çŒ® PMID
            format_type: æ–‡æ¡£æ ¼å¼

        Returns:
            ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        # ä½¿ç”¨ PMID å’Œæ ¼å¼ç±»å‹ç”Ÿæˆå”¯ä¸€çš„ç¼“å­˜æ–‡ä»¶å
        cache_key = f"{pmid}_{format_type}"
        cache_hash = hashlib.md5(cache_key.encode()).hexdigest()
        return self.cache_dir / f"bioc_{cache_hash}.json"

    def _is_cache_valid(self, cache_path: Path) -> bool:
        """
        æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ

        Args:
            cache_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„

        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        if not cache_path.exists():
            return False

        # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        file_age = time.time() - cache_path.stat().st_mtime
        return file_age < self.cache_ttl

    def _load_cached_bioc_document(self, pmid: str, format_type: str = "json") -> Optional[Dict[str, Any]]:
        """
        ä»ç¼“å­˜åŠ è½½ BioC æ–‡æ¡£

        Args:
            pmid: æ–‡çŒ® PMID
            format_type: æ–‡æ¡£æ ¼å¼

        Returns:
            BioC æ–‡æ¡£æˆ– None
        """
        if not self.enable_bioc_cache:
            return None

        cache_path = self._get_bioc_cache_path(pmid, format_type)

        if self._is_cache_valid(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    document = json.load(f)
                self.logger.debug(f"ä»ç¼“å­˜åŠ è½½ BioC æ–‡æ¡£: PMID {pmid}")
                return document
            except Exception as e:
                self.logger.warning(f"åŠ è½½ç¼“å­˜æ–‡æ¡£å¤±è´¥: {e}")
                # åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
                try:
                    cache_path.unlink()
                except:
                    pass

        return None

    def _cache_bioc_document(self, pmid: str, document: Dict[str, Any], format_type: str = "json") -> None:
        """
        ç¼“å­˜ BioC æ–‡æ¡£

        Args:
            pmid: æ–‡çŒ® PMID
            document: BioC æ–‡æ¡£
            format_type: æ–‡æ¡£æ ¼å¼
        """
        if not self.enable_bioc_cache:
            return

        try:
            cache_path = self._get_bioc_cache_path(pmid, format_type)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(document, f, ensure_ascii=False, indent=2)
            self.logger.debug(f"BioC æ–‡æ¡£å·²ç¼“å­˜: PMID {pmid}")
        except Exception as e:
            self.logger.warning(f"ç¼“å­˜ BioC æ–‡æ¡£å¤±è´¥: {e}")

    def _validate_bioc_document(self, document: Dict[str, Any]) -> bool:
        """
        éªŒè¯ BioC æ–‡æ¡£ç»“æ„

        Args:
            document: BioC æ–‡æ¡£

        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # æ£€æŸ¥åŸºæœ¬ç»“æ„
            if not isinstance(document, dict):
                return False

            if "documents" not in document or not isinstance(document["documents"], list):
                return False

            if len(document["documents"]) == 0:
                return False

            doc = document["documents"][0]
            if "passages" not in doc or not isinstance(doc["passages"], list):
                return False

            # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„ç« èŠ‚
            has_title = any(passage.get("infons", {}).get("section_type") == "TITLE" for passage in doc["passages"])

            return True

        except Exception:
            return False

    def _import_pdf_libraries(self):
        """å»¶è¿Ÿå¯¼å…¥ PDF å¤„ç†åº“"""
        if self._fitz is None:
            try:
                import fitz
                self._fitz = fitz
                self.logger.debug("âœ… æˆåŠŸå¯¼å…¥ PyMuPDF åº“")
            except ImportError:
                self.logger.warning("âš ï¸ PyMuPDF åº“æœªå®‰è£…ï¼Œ PDF å¤„ç†åŠŸèƒ½å°†å—é™")

        if self._pdf2image is None:
            try:
                from pdf2image import convert_from_path
                self._pdf2image = convert_from_path
                self.logger.debug("âœ… æˆåŠŸå¯¼å…¥ pdf2image åº“")
            except ImportError:
                self.logger.warning("âš ï¸ pdf2image åº“æœªå®‰è£…ï¼Œ OCR åŠŸèƒ½å°†å—é™")

        if self._pytesseract is None:
            try:
                import pytesseract
                self._pytesseract = pytesseract
                self.logger.debug("âœ… æˆåŠŸå¯¼å…¥ pytesseract åº“")
            except ImportError:
                self.logger.warning("âš ï¸ pytesseract åº“æœªå®‰è£…ï¼Œ OCR åŠŸèƒ½å°†å—é™")

        if self._PIL_Image is None:
            try:
                from PIL import Image
                self._PIL_Image = Image
                self.logger.debug("âœ… æˆåŠŸå¯¼å…¥ PIL åº“")
            except ImportError:
                self.logger.warning("âš ï¸ PIL åº“æœªå®‰è£…ï¼Œå›¾åƒå¤„ç†åŠŸèƒ½å°†å—é™")

    def fetch_bioc_document(self,
                            pmid: str,
                            format_type: str = "json",
                            encoding: str = "unicode",
                            max_retries: int = 3) -> Optional[Dict[str, Any]]:
        """
        ä» NCBI BioC API è·å–ç”Ÿç‰©åŒ»å­¦æ–‡çŒ®æ•°æ®

        Args:
            pmid: æ–‡çŒ® PMID
            format_type: è¿”å›æ ¼å¼ï¼Œ'xml' æˆ–'json'
            encoding: ç¼–ç æ ¼å¼ï¼Œ'unicode' æˆ–'ascii'
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            BioC æ–‡æ¡£çš„ JSON å¯¹è±¡ï¼Œå¤±è´¥è¿”å› None
        """
        # é¦–å…ˆå°è¯•ä»ç¼“å­˜åŠ è½½
        cached_doc = self._load_cached_bioc_document(pmid, format_type)
        if cached_doc and self._validate_bioc_document(cached_doc):
            return cached_doc

        url = f"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_{format_type}/{pmid}/{encoding}"

        for attempt in range(max_retries):
            try:
                self.logger.debug(f"æ­£åœ¨è·å– PMID {pmid} çš„ BioC æ•°æ® ... (å°è¯• {attempt + 1}/{max_retries})")

                response = api_manager.get(
                    url,
                    timeout=30,
                    api_name='pubmed_no_key'  # BioC API æ²¡æœ‰ key é™åˆ¶ï¼Œä½¿ç”¨è¾ƒå®½æ¾çš„é™æµ
                )

                if response.status_code == 200:
                    data = response.json()
                    if isinstance(data, list) and len(data) > 0:
                        document = data[0]

                        # éªŒè¯æ–‡æ¡£ç»“æ„
                        if self._validate_bioc_document(document):
                            # ç¼“å­˜æœ‰æ•ˆæ–‡æ¡£
                            self._cache_bioc_document(pmid, document, format_type)
                            self.logger.debug(f"âœ… æˆåŠŸè·å–å¹¶éªŒè¯ PMID {pmid} çš„ BioC æ•°æ®")
                            return document
                        else:
                            self.logger.warning(f"âš ï¸ PMID {pmid} çš„ BioC æ–‡æ¡£ç»“æ„éªŒè¯å¤±è´¥")
                            return None
                    else:
                        self.logger.warning(f"âš ï¸ PMID {pmid} çš„ BioC æ•°æ®æ ¼å¼å¼‚å¸¸")
                        return None
                else:
                    # å¤„ç†ç‰¹å®šçš„ HTTP çŠ¶æ€ç 
                    if response.status_code == 404:
                        self.logger.info(f"ğŸ“„ PMID {pmid} æ— å¯ç”¨ PMC å…¨æ–‡")
                        return None
                    elif response.status_code == 429:
                        self.logger.warning(f"âš ï¸ API è¯·æ±‚é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾…åé‡è¯• ...")
                        if attempt < max_retries - 1:
                            time.sleep(2**attempt)  # æŒ‡æ•°é€€é¿
                            continue
                    elif response.status_code >= 500:
                        self.logger.warning(f"âš ï¸ æœåŠ¡å™¨é”™è¯¯ï¼ŒçŠ¶æ€ç : {response.status_code}")
                        if attempt < max_retries - 1:
                            time.sleep(2**attempt)
                            continue
                    else:
                        self.logger.warning(f"âš ï¸ è·å– PMID {pmid} çš„ BioC æ•°æ®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                        return None

            except requests.exceptions.Timeout:
                self.logger.warning(f"âš ï¸ è·å– PMID {pmid} çš„ BioC æ•°æ®è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(2**attempt)
                    continue
            except requests.exceptions.ConnectionError:
                self.logger.warning(f"âš ï¸ ç½‘ç»œè¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(2**attempt)
                    continue
            except Exception as e:
                self.logger.warning(f"âš ï¸ è·å– PMID {pmid} çš„ BioC æ•°æ®æ—¶å‡ºé”™: {e}")
                if attempt < max_retries - 1:
                    time.sleep(1)
                    continue

        self.logger.error(f"âŒ è·å– PMID {pmid} çš„ BioC æ•°æ®æœ€ç»ˆå¤±è´¥")
        return None

    def extract_meta_info(self, bioc_document: Dict[str, Any]) -> str:
        """
        ä» BioC æ–‡æ¡£ä¸­æå–å…ƒæ•°æ®ä¿¡æ¯

        Args:
            bioc_document: BioC æ–‡æ¡£ JSON

        Returns:
            æ ¼å¼åŒ–çš„å…ƒæ•°æ®å­—ç¬¦ä¸²
        """
        try:
            # éªŒè¯æ–‡æ¡£ç»“æ„
            if not self._validate_bioc_document(bioc_document):
                return "æ–‡æ¡£ç»“æ„éªŒè¯å¤±è´¥"

            doc = bioc_document["documents"][0]
            title_passage = None

            # æŸ¥æ‰¾æ ‡é¢˜ç« èŠ‚
            for passage in doc["passages"]:
                if passage.get("infons", {}).get("section_type") == "TITLE":
                    title_passage = passage
                    break

            if not title_passage:
                return "æœªæ‰¾åˆ°æ ‡é¢˜ç« èŠ‚"

            metadata = title_passage["infons"]
            title_text = title_passage.get("text", "æ— æ ‡é¢˜").strip()

            # å®‰å…¨è·å–å…³é”®è¯
            keywords = self._safe_get_metadata_field(metadata, 'kwd', 'N/A')
            if isinstance(keywords, list):
                keywords = ';'.join(str(k) for k in keywords)

            # å¤„ç†ä½œè€…ä¿¡æ¯
            authors = self._extract_authors_from_metadata(metadata)

            # è·å–å…¶ä»–å…ƒæ•°æ®å­—æ®µ
            doi = self._safe_get_metadata_field(metadata, 'article-id_doi', 'N/A')
            pmid = self._safe_get_metadata_field(metadata, 'article-id_pmid', 'N/A')
            pmcid = self._safe_get_metadata_field(metadata, 'article-id_pmc', 'N/A')
            year = self._safe_get_metadata_field(metadata, 'year', 'N/A')
            source = self._safe_get_metadata_field(metadata, 'source', 'N/A')
            volume = self._safe_get_metadata_field(metadata, 'volume', 'N/A')
            issue = self._safe_get_metadata_field(metadata, 'issue', 'N/A')

            # æ ¼å¼åŒ–å…ƒæ•°æ®æ–‡æœ¬
            meta_text = f""" æ ‡é¢˜: {title_text}
                            DOI: {doi}
                            PMID: {pmid}
                            PMCID: PMC{pmcid}
                            å¹´ä»½: {year}
                            æœŸåˆŠ: {source}, å·å· {volume}, æœŸå· {issue}
                            å…³é”®è¯: {keywords}
                            ä½œè€…: {','.join(authors) if authors else 'N/A'}"""

            return meta_text

        except Exception as e:
            self.logger.warning(f"æå–å…ƒæ•°æ®ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return f"å…ƒæ•°æ®æå–å¤±è´¥: {str(e)}"

    def _safe_get_metadata_field(self, metadata: Dict[str, Any], field: str, default: str = 'N/A') -> str:
        """
        å®‰å…¨è·å–å…ƒæ•°æ®å­—æ®µ

        Args:
            metadata: å…ƒæ•°æ®å­—å…¸
            field: å­—æ®µå
            default: é»˜è®¤å€¼

        Returns:
            å­—æ®µå€¼æˆ–é»˜è®¤å€¼
        """
        try:
            value = metadata.get(field, default)
            if value is None or value == '':
                return default
            return str(value).strip()
        except Exception:
            return default

    def _extract_authors_from_metadata(self, metadata: Dict[str, Any]) -> List[str]:
        """
        ä»å…ƒæ•°æ®ä¸­æå–ä½œè€…ä¿¡æ¯

        Args:
            metadata: å…ƒæ•°æ®å­—å…¸

        Returns:
            æ ¼å¼åŒ–çš„ä½œè€…åˆ—è¡¨
        """
        authors = []

        try:
            # æŸ¥æ‰¾æ‰€æœ‰ä½œè€…å­—æ®µ
            author_fields = {key: value for key, value in metadata.items() if key.startswith('name_')}

            # æŒ‰ç¼–å·æ’åº
            sorted_fields = sorted(author_fields.items(), key=lambda x: x[0])

            for key, value in sorted_fields:
                if not value:
                    continue

                # è§£æä½œè€…ä¿¡æ¯æ ¼å¼ï¼šsurname:given_name;initials
                parts = value.split(';')
                surname = parts[0].split(':')[1] if ':' in parts[0] else parts[0]
                surname = surname.strip()

                if len(parts) > 1 and ':' in parts[1]:
                    given_names = parts[1].split(':')[1].strip()
                    formatted_name = f"{given_names} {surname}"
                else:
                    formatted_name = surname

                authors.append(formatted_name)

        except Exception as e:
            self.logger.warning(f"è§£æä½œè€…ä¿¡æ¯æ—¶å‡ºé”™: {e}")

        return authors

    def extract_full_text_from_bioc(self, bioc_document: Dict[str, Any]) -> str:
        """
        ä» BioC æ–‡æ¡£ä¸­æå–å…¨æ–‡å†…å®¹

        Args:
            bioc_document: BioC æ–‡æ¡£ JSON

        Returns:
            å…¨æ–‡å†…å®¹å­—ç¬¦ä¸²
        """
        try:
            # éªŒè¯æ–‡æ¡£ç»“æ„
            if not self._validate_bioc_document(bioc_document):
                self.logger.warning("BioC æ–‡æ¡£ç»“æ„éªŒè¯å¤±è´¥ï¼Œæ— æ³•æå–å…¨æ–‡")
                return ""

            doc = bioc_document["documents"][0]
            passages = doc.get("passages", [])

            if not passages:
                self.logger.warning("æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç« èŠ‚")
                return ""

            # è·å–æ‰€æœ‰ç« èŠ‚ç±»å‹å¹¶æ’åº
            section_types = self._get_ordered_section_types(passages)
            self.logger.debug(f"æå–ç« èŠ‚ç±»å‹: {section_types}")

            # æŒ‰ç« èŠ‚æå–æ–‡æœ¬
            section_texts = {}
            total_chars = 0

            for section_type in section_types:
                section_text = self._extract_section_text(passages, section_type)
                if section_text.strip():
                    section_texts[section_type] = section_text
                    total_chars += len(section_text)

                    # ä¸å†æ£€æŸ¥æ–‡æœ¬é•¿åº¦é™åˆ¶ï¼Œæ”¯æŒå…¨æ–‡æå–

            # ç»„è£…å…¨æ–‡å†…å®¹
            full_text = self._assemble_full_text(section_texts)

            self.logger.info(f"æˆåŠŸæå–å…¨æ–‡ï¼Œå…± {len(full_text)} å­—ç¬¦ï¼Œ{len(section_texts)} ä¸ªç« èŠ‚")
            return full_text.strip()

        except Exception as e:
            self.logger.error(f"ä» BioC æ–‡æ¡£æå–å…¨æ–‡æ—¶å‡ºé”™: {e}")
            return ""

    def _get_ordered_section_types(self, passages: List[Dict[str, Any]]) -> List[str]:
        """
        è·å–æŒ‰ä¼˜å…ˆçº§æ’åºçš„ç« èŠ‚ç±»å‹

        Args:
            passages: BioC ç« èŠ‚åˆ—è¡¨

        Returns:
            æ’åºåçš„ç« èŠ‚ç±»å‹åˆ—è¡¨
        """
        section_types = []
        seen_types = set()

        # å®šä¹‰ç« èŠ‚ä¼˜å…ˆçº§é¡ºåº
        section_priority = [
            "TITLE", "ABSTRACT", "INTRO", "METHODS", "RESULTS", "DISCUSS", "CONCL", "ACK_FUND", "REF", "FIG", "TABLE", "SUPPL"
        ]

        # æ”¶é›†æ‰€æœ‰ç« èŠ‚ç±»å‹
        for passage in passages:
            section_type = passage.get("infons", {}).get("section_type", "")
            if (section_type and section_type not in seen_types and section_type not in self.exclude_sections):
                section_types.append(section_type)
                seen_types.add(section_type)

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        def get_priority(section_type):
            section_type_upper = section_type.upper()
            for i, priority in enumerate(section_priority):
                if priority in section_type_upper:
                    return i
            return len(section_priority)  # æœªå®šä¹‰ä¼˜å…ˆçº§çš„ç« èŠ‚æ”¾åœ¨æœ€å

        return sorted(section_types, key=get_priority)

    def _extract_section_text(self, passages: List[Dict[str, Any]], section_type: str) -> str:
        """
        æå–ç‰¹å®šç« èŠ‚çš„æ–‡æœ¬

        Args:
            passages: BioC ç« èŠ‚åˆ—è¡¨
            section_type: ç« èŠ‚ç±»å‹

        Returns:
            ç« èŠ‚æ–‡æœ¬
        """
        section_text = ""

        for passage in passages:
            if passage.get("infons", {}).get("section_type") == section_type:
                text = passage.get("text", "").strip()
                if text:
                    section_text += text + "\n\n"

        return section_text.strip()

    def _assemble_full_text(self, section_texts: Dict[str, str]) -> str:
        """
        ç»„è£…å…¨æ–‡å†…å®¹ä¸ºæ ‡å‡† Markdown æ ¼å¼

        Args:
            section_texts: ç« èŠ‚æ–‡æœ¬å­—å…¸

        Returns:
            ç»„è£…åçš„ Markdown æ ¼å¼å…¨æ–‡
        """
        full_text_parts = []

        # å®šä¹‰ç« èŠ‚çš„ä¼˜å…ˆçº§é¡ºåº
        section_priority = [
            "ABSTRACT", "TITLE", "INTRODUCTION", "METHODS", "RESULTS", "DISCUSSION", "CONCLUSION", "ACKNOWLEDGMENTS",
            "REFERENCES", "FIGURES", "TABLES", "SUPPLEMENTARY"
        ]

        # æŒ‰ä¼˜å…ˆçº§æ’åºç« èŠ‚ï¼Œæœªåœ¨ä¼˜å…ˆçº§åˆ—è¡¨ä¸­çš„ç« èŠ‚æŒ‰å­—æ¯é¡ºåºæ’åœ¨æœ€å
        sorted_sections = []

        # å…ˆæ·»åŠ æœ‰ä¼˜å…ˆçº§çš„ç« èŠ‚
        for section_type in section_priority:
            if section_type in section_texts:
                sorted_sections.append(section_type)

        # å†æ·»åŠ å…¶ä»–ç« èŠ‚
        other_sections = [s for s in section_texts.keys() if s not in section_priority]
        other_sections.sort()
        sorted_sections.extend(other_sections)

        for section_type in sorted_sections:
            text = section_texts[section_type].strip()
            if not text:
                continue

            # æ ¼å¼åŒ–ç« èŠ‚æ ‡é¢˜ä¸º Markdown æ ¼å¼
            section_title = self._format_section_title(section_type)
            full_text_parts.append(f"\n\n{section_title}\n\n{text}")

        return "".join(full_text_parts).strip()

    def _format_section_title(self, section_type: str) -> str:
        """
        æ ¼å¼åŒ–ç« èŠ‚æ ‡é¢˜ä¸ºæ ‡å‡† Markdown æ ¼å¼

        Args:
            section_type: ç« èŠ‚ç±»å‹

        Returns:
            æ ¼å¼åŒ–çš„ Markdown ç« èŠ‚æ ‡é¢˜
        """
        # ç« èŠ‚ç±»å‹æ˜ å°„åˆ°æ ‡å‡†åç§°å’Œ Markdown çº§åˆ«
        section_config = {
            "TITLE": {
                "name": "Title",
                "level": 1
            },
            "ABSTRACT": {
                "name": "Abstract",
                "level": 2
            },
            "INTRO": {
                "name": "Introduction",
                "level": 2
            },
            "INTRODUCTION": {
                "name": "Introduction",
                "level": 2
            },
            "METHODS": {
                "name": "Methods",
                "level": 2
            },
            "METHOD": {
                "name": "Methods",
                "level": 2
            },
            "MATERIALS AND METHODS": {
                "name": "Materials and Methods",
                "level": 2
            },
            "RESULTS": {
                "name": "Results",
                "level": 2
            },
            "DISCUSS": {
                "name": "Discussion",
                "level": 2
            },
            "DISCUSSION": {
                "name": "Discussion",
                "level": 2
            },
            "CONCL": {
                "name": "Conclusion",
                "level": 2
            },
            "CONCLUSION": {
                "name": "Conclusion",
                "level": 2
            },
            "CONCLUSIONS": {
                "name": "Conclusions",
                "level": 2
            },
            "ACK": {
                "name": "Acknowledgments",
                "level": 2
            },
            "ACK_FUND": {
                "name": "Acknowledgments",
                "level": 2
            },
            "ACKNOWLEDGMENTS": {
                "name": "Acknowledgments",
                "level": 2
            },
            "REF": {
                "name": "References",
                "level": 2
            },
            "REFERENCES": {
                "name": "References",
                "level": 2
            },
            "FIG": {
                "name": "Figures",
                "level": 2
            },
            "FIGURES": {
                "name": "Figures",
                "level": 2
            },
            "TABLE": {
                "name": "Tables",
                "level": 2
            },
            "TAB": {
                "name": "Tables",
                "level": 2
            },
            "TABLES": {
                "name": "Tables",
                "level": 2
            },
            "SUPPL": {
                "name": "Supplementary Materials",
                "level": 2
            },
            "SUPPLEMENTARY": {
                "name": "Supplementary Materials",
                "level": 2
            }
        }

        # è·å–ç« èŠ‚é…ç½®ï¼Œé»˜è®¤ä½¿ç”¨åŸæ–‡ä½œä¸º 2 çº§æ ‡é¢˜
        config = section_config.get(section_type.upper(), {"name": section_type, "level": 2})

        # ç”Ÿæˆ Markdown æ ‡é¢˜
        markdown_level = "#" * config["level"]
        return f"{markdown_level} {config['name']}"

    def generate_markdown_document(self, paper: Dict[str, Any], full_text: str) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„ Markdown æ–‡æ¡£

        Args:
            paper: æ–‡çŒ®ä¿¡æ¯
            full_text: æå–çš„å…¨æ–‡å†…å®¹

        Returns:
            å®Œæ•´çš„ Markdown æ–‡æ¡£å­—ç¬¦ä¸²
        """
        pmid = paper.get('PMID', '')
        title = paper.get('Title', 'Unknown Title')
        authors = paper.get('Authors', 'Unknown Authors')
        journal = paper.get('Journal_Title', 'Unknown Journal')
        year = paper.get('Year_of_Publication', '')
        doi = paper.get('DOI', '')
        publication_date = paper.get('Publication_Date', '')

        # æ„å»º Markdown æ–‡æ¡£å¤´éƒ¨
        markdown_parts = []

        # ä¸»æ ‡é¢˜
        markdown_parts.append(f"# {title}")

        # æ–‡çŒ®å…ƒä¿¡æ¯è¡¨æ ¼
        if pmid or authors or journal or year or doi or publication_date:
            markdown_parts.append("\n## Publication Information\n")
            markdown_parts.append("| Field | Value |")
            markdown_parts.append("|-------|-------|")

            if pmid:
                markdown_parts.append(f"| PMID | {pmid} |")
            if authors:
                # é™åˆ¶ä½œè€…æ˜¾ç¤ºé•¿åº¦
                authors_display = authors if len(authors) <= 200 else authors[:200] + "..."
                markdown_parts.append(f"| Authors | {authors_display} |")
            if journal:
                markdown_parts.append(f"| Journal | {journal} |")
            if year:
                markdown_parts.append(f"| Year | {year} |")
            if doi:
                markdown_parts.append(f"| DOI | [{doi}](https://doi.org/{doi}) |")
            if publication_date:
                markdown_parts.append(f"| Publication Date | {publication_date} |")

        # æ·»åŠ å…¨æ–‡å†…å®¹
        if full_text.strip():
            markdown_parts.append(f"\n## Full Text\n\n{full_text}")
        else:
            markdown_parts.append("\n## Full Text\n\n*No full text available*")

        # æ–‡æ¡£å°¾éƒ¨
        markdown_parts.append(f"\n---\n")
        markdown_parts.append(f"*Document generated by PubMiner*")
        markdown_parts.append(f"*PMID: {pmid}*")
        if doi:
            markdown_parts.append(f"*DOI: [{doi}](https://doi.org/{doi})*")

        return "\n".join(markdown_parts)

    def extract_from_pdf(self, pdf_path: Union[str, Path], min_chars: int = 1000) -> str:
        """
        ä» PDF æ–‡ä»¶æå–æ–‡æœ¬

        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„
            min_chars: æœ€å°å­—ç¬¦æ•°é˜ˆå€¼

        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹
        """
        self._import_pdf_libraries()

        if self._fitz is None:
            self.logger.error("âŒ PyMuPDF åº“æœªå®‰è£…ï¼Œæ— æ³•å¤„ç† PDF æ–‡ä»¶")
            return ""

        pdf_path = Path(pdf_path)

        if not pdf_path.exists():
            self.logger.error(f"âŒ PDF æ–‡ä»¶ä¸å­˜åœ¨ : {pdf_path}")
            return ""

        try:
            self.logger.debug(f"ğŸ” å°è¯•ç›´æ¥æå– PDF æ–‡æœ¬ : {pdf_path.name}")

            # å°è¯•ç›´æ¥æå–æ–‡æœ¬
            doc = self._fitz.open(str(pdf_path))
            text = "\n".join([page.get_text() for page in doc])
            doc.close()

            effective_chars = len(''.join(text.split()))
            zh_count = sum('\u4e00' <= c <= '\u9fff' for c in text)
            en_count = sum(c.isalpha() for c in text)

            self.logger.debug(f"æå–åˆ° {len(text)} ä¸ªå­—ç¬¦ï¼ˆæœ‰æ•ˆ : {effective_chars}, ä¸­æ–‡ : {zh_count}, è‹±æ–‡ : {en_count}ï¼‰")

            # åˆ¤æ–­æå–è´¨é‡
            if (effective_chars >= min_chars or (effective_chars > 500 and (zh_count > 100 or en_count > 300))):
                self.logger.debug(f"âœ… PDF æ–‡æœ¬æå–æˆåŠŸ")
                return text

            # å¦‚æœæ–‡æœ¬è´¨é‡ä¸å¤Ÿï¼Œå°è¯• OCR
            self.logger.debug(f"âš ï¸ æå–æ–‡æœ¬è´¨é‡ä¸è¶³ï¼Œå°è¯• OCR...")
            return self._ocr_from_pdf(pdf_path)

        except Exception as e:
            self.logger.error(f"âŒ PDF æ–‡æœ¬æå–å¤±è´¥ : {e}")
            return ""

    def _ocr_from_pdf(self, pdf_path: Path) -> str:
        """
        ä½¿ç”¨ OCR ä» PDF æå–æ–‡æœ¬

        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„

        Returns:
            OCR è¯†åˆ«çš„æ–‡æœ¬
        """
        if (self._pdf2image is None or self._pytesseract is None or self._PIL_Image is None):
            self.logger.warning("âš ï¸ OCR ç›¸å…³åº“æœªå®‰è£…ï¼Œè·³è¿‡ OCR å¤„ç†")
            return ""

        try:
            self.logger.debug(f"ğŸ” å¼€å§‹ OCR è¯†åˆ« : {pdf_path.name}")

            # è½¬æ¢ PDF ä¸ºå›¾åƒ
            images = self._pdf2image(str(pdf_path), dpi=200)
            text_all = ""

            for idx, img in enumerate(images, 1):
                self.logger.debug(f"æ­£åœ¨è¯†åˆ«ç¬¬ {idx}/{len(images)} é¡µ ...")
                try:
                    # ä½¿ç”¨ä¸­è‹±æ–‡æ··åˆè¯†åˆ«
                    text = self._pytesseract.image_to_string(img, lang='chi_sim+eng')
                    text_all += f"\n---- ç¬¬ {idx} é¡µ ----\n{text}\n"
                except Exception as e:
                    self.logger.warning(f"ç¬¬ {idx} é¡µ OCR è¯†åˆ«å¤±è´¥ : {e}")
                    continue

            self.logger.debug(f"âœ… OCR è¯†åˆ«å®Œæˆï¼Œæå–äº† {len(text_all)} ä¸ªå­—ç¬¦")
            return text_all

        except Exception as e:
            self.logger.error(f"âŒ OCR è¯†åˆ«å¤±è´¥ : {e}")
            return ""

    def _identify_key_sections(self, text: str) -> Dict[str, str]:
        """
        è¯†åˆ«æ–‡æœ¬ä¸­çš„å…³é”®ç« èŠ‚

        Args:
            text: åŸå§‹æ–‡æœ¬ (string)

        Returns:
            ç« èŠ‚å­—å…¸ Dict[str, str]: {section_name: section_content}
        """
        # === 1. æ ‡å‡†åŒ–æ–‡æœ¬ ===
        text = re.sub(r'\r', '\n', text)  # æ ‡å‡†åŒ–æ¢è¡Œç¬¦
        text = re.sub(r'\n{2,}', '\n\n', text)  # æŠ˜å å¤šä½™çš„ç©ºè¡Œ
        text_lower = text.lower()

        # === 2. å®šä¹‰éƒ¨åˆ†å…³é”®è¯æ¨¡å¼ ===
        section_patterns = {
            "abstract": [r"abstract", r"summary"],
            "introduction": [r"introduction", r"background", r"objective[s]?", r"aim[s]?"],
            "methods": [r"materials and methods", r"methods?", r"methodology", r"study design", r"experimental procedures"],
            "results": [r"results?", r"findings", r"outcomes", r"observations"],
            "discussion": [r"discussion", r"interpretation", r"analysis"],
            "conclusion": [r"conclusion[s]?", r"summary", r"final remarks"],
            "acknowledgments": [r"acknowledg?ments?", r"funding", r"support"],
            "references": [r"references", r"bibliography"],
        }

        # === 3. å®šä½æ‰€æœ‰ç« èŠ‚æ ‡é¢˜ ===
        matches: List[Tuple[str, int]] = []
        for name, patterns in section_patterns.items():
            for pat in patterns:
                # åŒ¹é…æ ‡é¢˜å¦‚ "Introduction", "Introduction:"ï¼Œæˆ–è¡Œé¦–
                regex = rf"(^|\n)\s*{pat}\s*[:.]?\s*(\n|$)"
                for m in re.finditer(regex, text_lower):
                    matches.append((name, m.start()))

        if not matches:
            return {}

        # === 4. æŒ‰ä½ç½®æ’åº ===
        matches.sort(key=lambda x: x[1])

        # === 5. æå–ç« èŠ‚ ===
        sections: Dict[str, str] = {}
        for i, (name, start_pos) in enumerate(matches):
            end_pos = matches[i + 1][1] if i + 1 < len(matches) else len(text)
            section_text = text[start_pos:end_pos].strip()
            # è¿‡æ»¤è¯¯æŠ¥çš„æœ€å°å†…å®¹é•¿åº¦ï¼ˆé™ä½é˜ˆå€¼ä»¥æé«˜å…¼å®¹æ€§ï¼‰
            if len(section_text) > 20:  # ä» 100 é™ä½åˆ° 20 ï¼Œæé«˜å…¼å®¹æ€§
                sections[name] = section_text

        return sections

    def filter_and_optimize_text(self, text: str, max_length: Optional[int] = None) -> str:
        """
        æ™ºèƒ½ç­›é€‰å’Œä¼˜åŒ–æ–‡æœ¬å†…å®¹ï¼ˆä»…è¿›è¡Œç« èŠ‚è¿‡æ»¤ï¼Œä¸è¿›è¡Œé•¿åº¦é™åˆ¶ï¼‰

        Args:
            text: åŸå§‹æ–‡æœ¬
            max_length: å…¼å®¹æ€§å‚æ•°ï¼Œå·²å¼ƒç”¨

        Returns:
            ä¼˜åŒ–åçš„æ–‡æœ¬ï¼ˆå®Œæ•´å…¨æ–‡ï¼‰
        """
        if not text or not text.strip():
            return ""

        # ä¸å†è¿›è¡Œé•¿åº¦é™åˆ¶ï¼Œè¿”å›å®Œæ•´æ–‡æœ¬
        self.logger.debug(f"è¿”å›å®Œæ•´æ–‡æœ¬ï¼Œé•¿åº¦ï¼š{len(text)} å­—ç¬¦")
        return text

    def extract_text_from_paper(self, paper: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä»å•ç¯‡æ–‡çŒ®ä¸­æå–æ–‡æœ¬

        Args:
            paper: æ–‡çŒ®è®°å½•

        Returns:
            åŒ…å«å…¨æ–‡çš„æ–‡çŒ®è®°å½•
        """
        pmid = paper.get('PMID', '')
        title = paper.get('Title', 'Unknown')

        self.logger.debug(f"ğŸ” æå–æ–‡çŒ®æ–‡æœ¬ : {pmid} - {title[:50]}...")

        full_text = ""
        text_source = "none"

        # ä¼˜å…ˆå°è¯•ä» PMC è·å–å…¨æ–‡
        if pmid:
            bioc_doc = self.fetch_bioc_document(pmid)
            if bioc_doc:
                meta_info = self.extract_meta_info(bioc_doc)
                full_text = self.extract_full_text_from_bioc(bioc_doc)
                if full_text:
                    full_text = meta_info + "\n\n" + full_text
                    text_source = "pmc"
                    self.logger.debug(f"âœ… ä» PMC è·å–å…¨æ–‡æˆåŠŸ : {len(full_text)} å­—ç¬¦")

        # å¦‚æœ PMC æ²¡æœ‰å…¨æ–‡ï¼Œå°è¯•ä» PDF è·å–ï¼ˆå¦‚æœæä¾›äº† PDF è·¯å¾„ï¼‰
        if not full_text and 'pdf_path' in paper:
            pdf_path = paper['pdf_path']
            if pdf_path and Path(pdf_path).exists():
                full_text = self.extract_from_pdf(pdf_path)
                if full_text:
                    text_source = "pdf"
                    self.logger.debug(f"âœ… ä» PDF è·å–å…¨æ–‡æˆåŠŸ : {len(full_text)} å­—ç¬¦")

        # å¦‚æœéƒ½æ²¡æœ‰å…¨æ–‡ï¼Œä½¿ç”¨æ‘˜è¦
        if not full_text:
            abstract = paper.get('Abstract', '')
            if abstract and abstract != 'NA':
                full_text = f"æ ‡é¢˜ : {title}\n\n æ‘˜è¦ : {abstract}"
                text_source = "abstract"
                self.logger.debug(f"âœ… ä½¿ç”¨æ‘˜è¦ä½œä¸ºæ–‡æœ¬ : {len(full_text)} å­—ç¬¦")

        # ä¸å†è¿›è¡Œæ–‡æœ¬é•¿åº¦ä¼˜åŒ–ï¼Œä¿æŒå…¨æ–‡
        if full_text:
            full_text = self.filter_and_optimize_text(full_text)

        # æ›´æ–°æ–‡çŒ®è®°å½•
        paper_with_text = paper.copy()
        paper_with_text.update({
            'full_text': full_text,
            'text_source': text_source,
            'text_length': len(full_text) if full_text else 0
        })

        return paper_with_text

    def extract_batch(self, papers: List[Dict[str, Any]], max_workers: int = 4) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡æå–æ–‡çŒ®æ–‡æœ¬

        Args:
            papers: æ–‡çŒ®åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°

        Returns:
            åŒ…å«å…¨æ–‡çš„æ–‡çŒ®åˆ—è¡¨
        """
        self.logger.info(f"ğŸ“„ å¼€å§‹æ‰¹é‡æå–æ–‡æœ¬ï¼Œå…± {len(papers)} ç¯‡æ–‡çŒ®")

        results = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_paper = {executor.submit(self.extract_text_from_paper, paper): paper for paper in papers}

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_paper):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    paper = future_to_paper[future]
                    pmid = paper.get('PMID', 'Unknown')
                    self.logger.error(f"âŒ æå–æ–‡çŒ® {pmid} çš„æ–‡æœ¬å¤±è´¥ : {e}")
                    # æ·»åŠ å¤±è´¥è®°å½•
                    paper_with_error = paper.copy()
                    paper_with_error.update({
                        'full_text': '',
                        'text_source': 'error',
                        'text_length': 0,
                        'extraction_error': str(e)
                    })
                    results.append(paper_with_error)

        # ç»Ÿè®¡ç»“æœ
        successful = len([r for r in results if r.get('full_text')])
        self.logger.info(f"âœ… æ–‡æœ¬æå–å®Œæˆ : {successful}/{len(papers)} ç¯‡æˆåŠŸ")

        return results
