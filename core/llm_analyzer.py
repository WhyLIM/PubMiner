# -*- coding: utf-8 -*-
"""
å¤§è¯­è¨€æ¨¡å‹åˆ†ææ¨¡å—

è´Ÿè´£è°ƒç”¨å„ç§ LLM API è¿›è¡Œæ–‡çŒ®å†…å®¹åˆ†æå’Œç»“æ„åŒ–ä¿¡æ¯æå–
æ”¯æŒå¤šå‚å•† APIï¼ŒåŒ…å«æ™ºèƒ½æç¤ºå·¥ç¨‹å’Œç»“æœéªŒè¯
"""

import json
import time
import asyncio
from typing import Dict, List, Any, Optional, Union
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import re
from datetime import datetime

from utils.logger import LoggerMixin
from utils.api_manager import api_manager

logger = logging.getLogger(__name__)

class LLMAnalyzer(LoggerMixin):
    """å¤§è¯­è¨€æ¨¡å‹åˆ†æå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–LLMåˆ†æå™¨
        
        Args:
            config: LLM é…ç½®
        """
        self.config = config
        self.api_base = config.get('api_base', '')
        self.api_key = config.get('api_key', '')
        self.model = config.get('model', '')
        self.temperature = config.get('temperature', 0.1)
        self.max_tokens = config.get('max_tokens', 4000)
        self.timeout = config.get('timeout', 60)
        
        # æ ¹æ® API base åˆ¤æ–­æä¾›å•†
        self.provider = self._detect_provider()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_tokens_used': 0,
            'start_time': datetime.now()
        }
        
        self.logger.info(f"åˆå§‹åŒ– LLM åˆ†æå™¨: {self.provider} - {self.model}")
    
    def _detect_provider(self) -> str:
        """æ ¹æ® API base æ£€æµ‹æä¾›å•†"""
        api_base_lower = self.api_base.lower()
        
        if 'openai.com' in api_base_lower:
            return 'openai'
        elif 'deepseek.com' in api_base_lower:
            return 'deepseek'
        elif 'dashscope.aliyuncs.com' in api_base_lower:
            return 'qwen'
        elif 'volces.com' in api_base_lower:
            return 'volcengine'
        else:
            return 'unknown'
    
    def _build_headers(self) -> Dict[str, str]:
        """æ„å»ºè¯·æ±‚å¤´"""
        headers = {
            'Content-Type': 'application/json',
            'User-Agent': 'PubMiner/1.0'
        }
        
        if self.api_key:
            headers['Authorization'] = f'Bearer {self.api_key}'
        
        return headers
    
    def _build_system_prompt(self, template: Dict[str, Any], language: str = "English") -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤ºè¯
        
        Args:
            template: æå–æ¨¡æ¿
            language: è¾“å‡ºè¯­è¨€ (Chinese, English, etc.)
            
        Returns:
            ç³»ç»Ÿæç¤ºè¯
        """
        template_name = template.get('name', 'ä¿¡æ¯æå–')
        template_desc = template.get('description', '')
        
        system_prompt = f"""You are a professional medical literature analysis assistant, specializing in extracting structured information from academic papers.

Task: {template_name}
Description: {template_desc}

Important requirements:
1. Output results strictly in JSON format
2. Only extract information explicitly mentioned in the paper; do not fabricate any content
3. If a certain field is not mentioned in the paper, fill in "Not Mentioned"
4. Maintain the accuracy and objectivity of the extracted information
5. For numerical information, maintain the precise expression of the original text

Output format requirements:
- Use standard JSON format
- Field names use English
- Content use {language}
- Ensure the JSON format is correct and can be parsed by programs"""

        return system_prompt
    
    def _build_user_prompt(self, text: str, template: Dict[str, Any]) -> str:
        """
        æ„å»ºç”¨æˆ·æç¤ºè¯
        
        Args:
            text: æ–‡çŒ®æ–‡æœ¬
            template: æå–æ¨¡æ¿
            
        Returns:
            ç”¨æˆ·æç¤ºè¯
        """
        fields = template.get('fields', {})
        
        # æ„å»ºå­—æ®µè¯´æ˜
        field_descriptions = []
        json_example = {}
        
        for field_key, field_info in fields.items():
            field_name = field_info.get('name', field_key)
            field_desc = field_info.get('description', '')
            prompt_hint = field_info.get('prompt_hint', '')
            
            description = f"- {field_key}: {field_name}"
            if field_desc:
                description += f" - {field_desc}"
            if prompt_hint:
                description += f" (æå–æç¤º: {prompt_hint})"
            
            field_descriptions.append(description)
            json_example[field_key] = "è¯·ä»è®ºæ–‡ä¸­æå–ç›¸å…³ä¿¡æ¯"
        
        user_prompt = f"""è¯·ä»ä»¥ä¸‹å­¦æœ¯è®ºæ–‡ä¸­æå–æŒ‡å®šçš„ç»“æ„åŒ–ä¿¡æ¯ï¼š

éœ€è¦æå–çš„å­—æ®µï¼š
{chr(10).join(field_descriptions)}

è¾“å‡ºJSONæ ¼å¼ç¤ºä¾‹ï¼š
```json
{json.dumps(json_example, ensure_ascii=False, indent=2)}
```

è®ºæ–‡å…¨æ–‡ï¼š
{text}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°JSONæ ¼å¼è¾“å‡ºæå–ç»“æœï¼š"""

        return user_prompt
    
    def _clean_json_response(self, response_text: str) -> str:
        """
        æ¸…ç†LLMå“åº”ï¼Œæå–JSONéƒ¨åˆ†
        
        Args:
            response_text: åŸå§‹å“åº”æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„JSONå­—ç¬¦ä¸²
        """
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        response_text = re.sub(r'```json\s*', '', response_text)
        response_text = re.sub(r'```\s*$', '', response_text)
        
        # æŸ¥æ‰¾JSONå¯¹è±¡
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            return json_match.group(0).strip()
        
        return response_text.strip()
    
    def _validate_extraction_result(self, result: Dict[str, Any], 
                                  template: Dict[str, Any]) -> Dict[str, Any]:
        """
        éªŒè¯æå–ç»“æœçš„æœ‰æ•ˆæ€§
        
        Args:
            result: æå–ç»“æœ
            template: æå–æ¨¡æ¿
            
        Returns:
            éªŒè¯åçš„ç»“æœ
        """
        fields = template.get('fields', {})
        validated_result = {}
        
        for field_key, field_info in fields.items():
            field_name = field_info.get('name', field_key)
            is_required = field_info.get('required', False)
            
            value = result.get(field_key, '')
            
            # å¤„ç†ç©ºå€¼
            if not value or value in ['', 'N/A', 'NA', 'null', 'None']:
                if is_required:
                    self.logger.warning(f"å¿…éœ€å­—æ®µ '{field_name}' æœªæå–åˆ°æœ‰æ•ˆå€¼")
                value = 'æœªæåŠ'
            
            # é•¿åº¦é™åˆ¶
            if isinstance(value, str) and len(value) > 1000:
                value = value[:997] + '...'
                self.logger.debug(f"å­—æ®µ '{field_name}' å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­")
            
            validated_result[field_key] = value
        
        return validated_result
    
    @api_manager.with_retry(max_retries=3, retry_delay=2.0)
    def _call_llm_api(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """
        è°ƒç”¨LLM API
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            APIå“åº”ç»“æœ
        """
        payload = {
            'model': self.model,
            'messages': messages,
            'temperature': self.temperature,
            'max_tokens': self.max_tokens
        }
        
        headers = self._build_headers()
        
        # åº”ç”¨APIé™æµ
        api_name = self.provider
        if api_name in api_manager.rate_limiters:
            api_manager.rate_limiters[api_name].wait_if_needed()
        
        response = api_manager.post(
            url=f"{self.api_base}/chat/completions",
            headers=headers,
            json_data=payload,
            timeout=self.timeout,
            api_name=api_name
        )
        
        return response.json()
    
    def analyze_single_paper(self, paper: Dict[str, Any], 
                           template: Dict[str, Any],
                           language: str = "English") -> Dict[str, Any]:
        """
        åˆ†æå•ç¯‡æ–‡çŒ®
        
        Args:
            paper: æ–‡çŒ®è®°å½•
            template: æå–æ¨¡æ¿
            language: è¾“å‡ºè¯­è¨€ (Chinese, English, etc.)
            
        Returns:
            åŒ…å«æå–ä¿¡æ¯çš„æ–‡çŒ®è®°å½•
        """
        pmid = paper.get('PMID', 'Unknown')
        title = paper.get('Title', 'Unknown')[:50]
        full_text = paper.get('full_text', '')
        
        self.logger.debug(f"ğŸ§  åˆ†ææ–‡çŒ®: {pmid} - {title}...")
        
        # å¦‚æœæ²¡æœ‰å…¨æ–‡ï¼Œå°è¯•ä½¿ç”¨æ‘˜è¦è¿›è¡Œåˆ†æ
        if not full_text:
            abstract = paper.get('Abstract', paper.get('abstract', '')).strip()
            if not abstract:
                self.logger.warning(f"âš ï¸ æ–‡çŒ® {pmid} æ—¢æ²¡æœ‰å…¨æ–‡ä¹Ÿæ²¡æœ‰æ‘˜è¦ï¼Œè·³è¿‡åˆ†æ")
                result = paper.copy()
                result['extraction_status'] = 'no_content'
                return result
            else:
                self.logger.info(f"ğŸ“ æ–‡çŒ® {pmid} ä½¿ç”¨æ‘˜è¦è¿›è¡Œåˆ†æ")
                full_text = "Title: " + title + "Abstract: " + abstract
        
        try:
            # æ„å»ºæç¤ºè¯
            system_prompt = self._build_system_prompt(template, language)
            user_prompt = self._build_user_prompt(full_text, template)
            
            messages = [
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_prompt}
            ]
            
            # è°ƒç”¨API
            self.stats['total_requests'] += 1
            start_time = time.time()
            
            api_response = self._call_llm_api(messages)
            
            # å¤„ç†å“åº”
            if 'choices' in api_response and len(api_response['choices']) > 0:
                response_content = api_response['choices'][0]['message']['content']
                
                # æ¸…ç†å¹¶è§£æJSON
                json_content = self._clean_json_response(response_content)
                extraction_result = json.loads(json_content)
                
                # éªŒè¯ç»“æœ
                validated_result = self._validate_extraction_result(extraction_result, template)
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats['successful_requests'] += 1
                if 'usage' in api_response:
                    self.stats['total_tokens_used'] += api_response['usage'].get('total_tokens', 0)
                
                # åˆå¹¶ç»“æœ
                result = paper.copy()
                result.update(validated_result)
                result['extraction_status'] = 'success'
                result['extraction_time'] = time.time() - start_time
                
                self.logger.debug(f"âœ… åˆ†æå®Œæˆ: {pmid}")
                return result
                
            else:
                raise ValueError(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {api_response}")
                
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ JSONè§£æå¤±è´¥ {pmid}: {e}")
            self.stats['failed_requests'] += 1
            result = paper.copy()
            result['extraction_status'] = 'json_error'
            result['extraction_error'] = str(e)
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ†ææ–‡çŒ® {pmid} å¤±è´¥: {e}")
            self.stats['failed_requests'] += 1
            result = paper.copy()
            result['extraction_status'] = 'api_error'
            result['extraction_error'] = str(e)
            return result
    
    def analyze_batch(self, papers: List[Dict[str, Any]], 
                     template: Dict[str, Any],
                     batch_size: int = 10,
                     max_workers: int = 4,
                     language: str = "English") -> List[Dict[str, Any]]:
        """
        æ‰¹é‡åˆ†ææ–‡çŒ®
        
        Args:
            papers: æ–‡çŒ®åˆ—è¡¨
            template: æå–æ¨¡æ¿
            batch_size: æ‰¹å¤„ç†å¤§å°
            max_workers: æœ€å¤§å¹¶å‘æ•°
            language: è¾“å‡ºè¯­è¨€ (Chinese, English, etc.)
            
        Returns:
            åŒ…å«æå–ä¿¡æ¯çš„æ–‡çŒ®åˆ—è¡¨
        """
        self.logger.info(f"ğŸ§  å¼€å§‹æ‰¹é‡åˆ†æï¼Œå…± {len(papers)} ç¯‡æ–‡çŒ®")
        self.logger.info(f"ä½¿ç”¨æ¨¡æ¿: {template.get('name', 'Unknown')}")
        self.logger.info(f"æ‰¹å¤„ç†å¤§å°: {batch_size}, å¹¶å‘æ•°: {max_workers}")
        
        results = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(papers), batch_size):
            batch_papers = papers[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(papers) + batch_size - 1) // batch_size
            
            self.logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_papers)} ç¯‡)")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤ä»»åŠ¡
                future_to_paper = {
                    executor.submit(self.analyze_single_paper, paper, template, language): paper
                    for paper in batch_papers
                }
                
                # æ”¶é›†ç»“æœ
                batch_results = []
                for future in as_completed(future_to_paper):
                    try:
                        result = future.result()
                        batch_results.append(result)
                    except Exception as e:
                        paper = future_to_paper[future]
                        pmid = paper.get('PMID', 'Unknown')
                        self.logger.error(f"âŒ å¤„ç†æ–‡çŒ® {pmid} æ—¶å‡ºç°å¼‚å¸¸: {e}")
                        
                        # åˆ›å»ºé”™è¯¯è®°å½•
                        error_result = paper.copy()
                        error_result['extraction_status'] = 'processing_error'
                        error_result['extraction_error'] = str(e)
                        batch_results.append(error_result)
                
                results.extend(batch_results)
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            if i + batch_size < len(papers):
                self.logger.info("â¸ï¸ æ‰¹æ¬¡é—´ä¼‘æ¯ 5 ç§’...")
                time.sleep(5)
        
        # ç»Ÿè®¡ç»“æœ
        successful = len([r for r in results if r.get('extraction_status') == 'success'])
        failed = len(results) - successful
        
        elapsed_time = (datetime.now() - self.stats['start_time']).total_seconds()
        
        self.logger.info(f"âœ… æ‰¹é‡åˆ†æå®Œæˆ!")
        self.logger.info(f"ğŸ“Š æˆåŠŸ: {successful}/{len(papers)} ç¯‡")
        self.logger.info(f"âŒ å¤±è´¥: {failed} ç¯‡")
        self.logger.info(f"â±ï¸ ç”¨æ—¶: {elapsed_time:.1f} ç§’")
        self.logger.info(f"ğŸª™ Token ä½¿ç”¨: {self.stats['total_tokens_used']}")
        
        return results
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–åˆ†æç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        elapsed_time = (datetime.now() - self.stats['start_time']).total_seconds()
        
        return {
            'provider': self.provider,
            'model': self.model,
            'total_requests': self.stats['total_requests'],
            'successful_requests': self.stats['successful_requests'],
            'failed_requests': self.stats['failed_requests'],
            'success_rate': (self.stats['successful_requests'] / self.stats['total_requests'] 
                           if self.stats['total_requests'] > 0 else 0),
            'total_tokens_used': self.stats['total_tokens_used'],
            'elapsed_time': elapsed_time,
            'requests_per_minute': (self.stats['total_requests'] / (elapsed_time / 60) 
                                  if elapsed_time > 0 else 0)
        }