#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Query Manager - æŸ¥è¯¢é…ç½®ç®¡ç†å™¨
æ”¯æŒä»é…ç½®æ–‡ä»¶åŠ è½½å¤æ‚æŸ¥è¯¢ä»»åŠ¡ï¼Œç±»ä¼¼PubExçš„åŠŸèƒ½
"""

import json
import os
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from tqdm import tqdm

from .config_manager import ConfigManager
from utils.logger import setup_logger


class QueryManager:
    """æŸ¥è¯¢é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_manager: ConfigManager):
        """åˆå§‹åŒ–æŸ¥è¯¢ç®¡ç†å™¨"""
        self.config_manager = config_manager
        self.logger = setup_logger()
        
    def load_query_config(self, config_file: str) -> Dict[str, Any]:
        """
        åŠ è½½æŸ¥è¯¢é…ç½®æ–‡ä»¶
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: æŸ¥è¯¢é…ç½®å­—å…¸
            
        Raises:
            FileNotFoundError: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨
            ValueError: é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯
        """
        if not os.path.exists(config_file):
            raise FileNotFoundError(f"Query config file not found: {config_file}")
            
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON format in config file: {e}")
            
        # éªŒè¯é…ç½®æ–‡ä»¶ç»“æ„
        self._validate_config(config)
        
        return config
        
    def _validate_config(self, config: Dict[str, Any]) -> None:
        """
        éªŒè¯é…ç½®æ–‡ä»¶æ ¼å¼
        
        Args:
            config: é…ç½®å­—å…¸
            
        Raises:
            ValueError: é…ç½®æ ¼å¼é”™è¯¯
        """
        if 'query_tasks' not in config:
            raise ValueError("Missing 'query_tasks' in config file")
            
        if not isinstance(config['query_tasks'], list):
            raise ValueError("'query_tasks' must be a list")
            
        if not config['query_tasks']:
            raise ValueError("'query_tasks' cannot be empty")
            
        # éªŒè¯æ¯ä¸ªä»»åŠ¡çš„å¿…éœ€å­—æ®µ
        required_fields = ['name', 'query']
        for i, task in enumerate(config['query_tasks']):
            for field in required_fields:
                if field not in task:
                    raise ValueError(f"Task {i+1} missing required field: {field}")
                    
    def create_example_config(self, output_file: str = "query_config_example.json") -> None:
        """
        åˆ›å»ºç¤ºä¾‹æŸ¥è¯¢é…ç½®æ–‡ä»¶
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        example_config = {
            "_comment": "PubMiner æ‰¹é‡æŸ¥è¯¢é…ç½®æ–‡ä»¶ç¤ºä¾‹",
            "_max_results_options": {
                "è¯´æ˜": "max_results è®¾ç½®é€‰é¡¹",
                "è·å–æ‰€æœ‰ç»“æœ": "è®¾ä¸º null æˆ– -1",
                "é™åˆ¶æ•°é‡": "è®¾ä¸ºå…·ä½“æ•°å­—ï¼Œå¦‚ 100, 500 ç­‰"
            },
            "query_tasks": [
                {
                    "name": "COVID-19 and Diabetes Research (Limited)",
                    "query": "(COVID-19[ti] OR SARS-CoV-2[ti]) AND (diabetes[ti] OR diabetic[ti]) AND (\"2020/01/01\"[Date - Publication] : \"2024/12/31\"[Date - Publication])",
                    "max_results": 50,
                    "include_fulltext": False,
                    "output_file": "covid_diabetes_limited.csv",
                    "description": "Research on COVID-19 and diabetes relationship (limited to 50 papers)"
                },
                {
                    "name": "Aging Biomarkers Study (All Results)",
                    "query": "(aging[ti] OR senescence[ti]) AND (biomarker[ti] OR marker[ti]) AND (\"2023/01/01\"[Date - Publication] : \"2024/12/31\"[Date - Publication])",
                    "max_results": None,
                    "include_fulltext": True,
                    "output_file": "aging_biomarkers_all.csv",
                    "description": "Aging-related biomarker research (è·å–æ‰€æœ‰ç»“æœ)",
                    "custom_fields": [
                        "What biomarkers are studied in this article",
                        "Single biomarker or combination of biomarkers",
                        "Biomarker category (Protein/DNA/RNA/Other)",
                        "Study population ethnicity",
                        "Gender ratio of study samples"
                    ]
                },
                {
                    "name": "Cancer Immunotherapy (Unlimited)",
                    "query": "cancer[ti] AND immunotherapy[ti] AND (\"2024/01/01\"[Date - Publication] : \"2024/12/31\"[Date - Publication])",
                    "max_results": -1,
                    "include_fulltext": False,
                    "output_file": "cancer_immunotherapy_unlimited.csv",
                    "description": "Cancer immunotherapy research (ä½¿ç”¨ -1 è·å–æ‰€æœ‰ç»“æœ)"
                }
            ],
            "default_settings": {
                "max_results": None,
                "include_fulltext": False,
                "output_dir": "results/batch_queries",
                "task_wait_time": 5,
                "retry_failed_tasks": True
            }
        }
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(example_config, f, indent=4, ensure_ascii=False)
            
        self.logger.info(f"âœ… Example query config created: {output_file}")
        print(f"âœ… Example query config created: {output_file}")
        print("ğŸ“ Please modify the config file according to your needs")
        
    def execute_batch_queries(self, config_file: str, pubminer) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œæ‰¹é‡æŸ¥è¯¢ä»»åŠ¡
        
        Args:
            config_file: æŸ¥è¯¢é…ç½®æ–‡ä»¶è·¯å¾„
            pubminer: PubMinerå®ä¾‹
            
        Returns:
            List[Dict]: æ‰§è¡Œç»“æœåˆ—è¡¨
        """
        # åŠ è½½é…ç½®
        config = self.load_query_config(config_file)
        query_tasks = config['query_tasks']
        default_settings = config.get('default_settings', {})
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = default_settings.get('output_dir', 'results/batch_queries')
        os.makedirs(output_dir, exist_ok=True)
        
        # æ‰§è¡Œç»“æœ
        execution_results = []
        
        # åˆ›å»ºæ€»ä½“è¿›åº¦æ¡
        task_progress = tqdm(total=len(query_tasks), desc="ğŸ¯ Batch Query Progress", unit="task")
        
        self.logger.info(f"ğŸš€ Starting batch query execution with {len(query_tasks)} tasks")
        print(f"ğŸš€ Starting batch query execution with {len(query_tasks)} tasks")
        print(f"ğŸ“ Output directory: {output_dir}")
        print("=" * 80)
        
        for i, task in enumerate(query_tasks, 1):
            task_name = task.get('name', f'Task {i}')
            query = task['query']
            
            # ä»»åŠ¡å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼å¡«å……
            max_results = task.get('max_results', default_settings.get('max_results', 100))
            
            # å¤„ç†æ— é™åˆ¶è·å–çš„æƒ…å†µ
            if max_results is None or max_results == -1:
                max_results = None  # None è¡¨ç¤ºè·å–æ‰€æœ‰ç»“æœ
                max_results_display = "æ‰€æœ‰ç»“æœ"
            else:
                max_results_display = str(max_results)
            include_fulltext = task.get('include_fulltext', default_settings.get('include_fulltext', False))
            output_file = task.get('output_file', f'query_task_{i}.csv')
            custom_fields = task.get('custom_fields', [])
            description = task.get('description', '')
            
            # ç¡®ä¿è¾“å‡ºæ–‡ä»¶åœ¨æ­£ç¡®çš„ç›®å½•ä¸‹
            if not os.path.isabs(output_file):
                output_file = os.path.join(output_dir, output_file)
                
            print(f"\nğŸ¯ Task {i}/{len(query_tasks)}: {task_name}")
            print(f"ğŸ“ Description: {description}")
            print(f"ğŸ” Query: {query}")
            print(f"ğŸ“Š Max results: {max_results_display}")
            print(f"ğŸ“„ Include fulltext: {include_fulltext}")
            print(f"ğŸ“ Output file: {output_file}")
            if custom_fields:
                print(f"ğŸ”§ Custom fields: {len(custom_fields)} fields")
            print("-" * 60)
            
            # è®°å½•ä»»åŠ¡å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            try:
                # è®¾ç½®æå–æ¨¡æ¿
                if custom_fields:
                    # åˆ›å»ºä¸´æ—¶è‡ªå®šä¹‰æ¨¡æ¿
                    custom_template = {
                        "template_name": f"custom_task_{i}",
                        "description": f"Custom template for task: {task_name}",
                        "fields": {}
                    }
                    
                    for j, field in enumerate(custom_fields, 1):
                        field_key = f"custom_field_{j}"
                        custom_template["fields"][field_key] = {
                            "description": field,
                            "type": "string",
                            "required": False
                        }
                    
                    # ä¸´æ—¶ä¿å­˜è‡ªå®šä¹‰æ¨¡æ¿
                    temp_template_file = f"temp_template_task_{i}.json"
                    with open(temp_template_file, 'w', encoding='utf-8') as f:
                        json.dump(custom_template, f, indent=2, ensure_ascii=False)
                    
                    # æ‰§è¡Œå¸¦è‡ªå®šä¹‰å­—æ®µçš„åˆ†æ
                    language = task.get('language', default_settings.get('language'))
                    results = pubminer.analyze_by_query(
                        query=query,
                        max_results=max_results,
                        include_fulltext=include_fulltext,
                        custom_template_file=temp_template_file,
                        language=language
                    )
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if os.path.exists(temp_template_file):
                        os.remove(temp_template_file)
                else:
                    # æ‰§è¡Œæ ‡å‡†åˆ†æ
                    language = task.get('language', default_settings.get('language'))
                    results = pubminer.analyze_by_query(
                        query=query,
                        max_results=max_results,
                        include_fulltext=include_fulltext,
                        language=language
                    )
                
                # ä¿å­˜ç»“æœ
                if results:
                    pubminer.save_results(results, output_file)
                    
                # è®°å½•æ‰§è¡Œç»“æœ
                execution_time = time.time() - start_time
                result_info = {
                    'task_id': i,
                    'task_name': task_name,
                    'query': query,
                    'status': 'success',
                    'results_count': len(results) if results else 0,
                    'execution_time': execution_time,
                    'output_file': output_file,
                    'error': None
                }
                
                print(f"âœ… Task completed: {len(results) if results else 0} papers retrieved")
                print(f"â±ï¸ Execution time: {execution_time:.2f}s")
                
            except Exception as e:
                # è®°å½•é”™è¯¯
                execution_time = time.time() - start_time
                result_info = {
                    'task_id': i,
                    'task_name': task_name,
                    'query': query,
                    'status': 'failed',
                    'results_count': 0,
                    'execution_time': execution_time,
                    'output_file': output_file,
                    'error': str(e)
                }
                
                self.logger.error(f"âŒ Task {i} failed: {e}")
                print(f"âŒ Task failed: {e}")
                
                # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦é‡è¯•
                if default_settings.get('retry_failed_tasks', False):
                    print("ğŸ”„ Retrying failed task...")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ é‡è¯•é€»è¾‘
                
            execution_results.append(result_info)
            task_progress.update(1)
            
            # ä»»åŠ¡é—´ç­‰å¾…
            if i < len(query_tasks):
                wait_time = default_settings.get('task_wait_time', 5)
                if wait_time > 0:
                    print(f"\nâ¸ï¸ Task completed, waiting {wait_time}s before next task...")
                    time.sleep(wait_time)
        
        task_progress.close()
        
        # ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š
        self._generate_execution_report(execution_results, output_dir)
        
        print(f"\nğŸ‰ All {len(query_tasks)} query tasks completed!")
        print(f"ğŸ“Š Results saved in: {output_dir}")
        
        return execution_results
        
    def _generate_execution_report(self, results: List[Dict[str, Any]], output_dir: str) -> None:
        """
        ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š
        
        Args:
            results: æ‰§è¡Œç»“æœåˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        """
        report_file = os.path.join(output_dir, 'execution_report.json')
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_tasks = len(results)
        successful_tasks = len([r for r in results if r['status'] == 'success'])
        failed_tasks = total_tasks - successful_tasks
        total_papers = sum(r['results_count'] for r in results)
        total_time = sum(r['execution_time'] for r in results)
        
        report = {
            'execution_summary': {
                'total_tasks': total_tasks,
                'successful_tasks': successful_tasks,
                'failed_tasks': failed_tasks,
                'success_rate': f"{successful_tasks/total_tasks*100:.1f}%" if total_tasks > 0 else "0%",
                'total_papers_retrieved': total_papers,
                'total_execution_time': f"{total_time:.2f}s",
                'average_time_per_task': f"{total_time/total_tasks:.2f}s" if total_tasks > 0 else "0s"
            },
            'task_details': results,
            'generated_at': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            
        self.logger.info(f"ğŸ“‹ Execution report saved: {report_file}")
        print(f"ğŸ“‹ Execution report saved: {report_file}")
        print(f"ğŸ“Š Summary: {successful_tasks}/{total_tasks} tasks successful, {total_papers} papers total")