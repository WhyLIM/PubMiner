# -*- coding: utf-8 -*-
"""
PDF ä¸‹è½½æ¨¡å—

è´Ÿè´£ä»å¤šä¸ªæºä¸‹è½½æ–‡çŒ® PDF æ–‡ä»¶ï¼ŒåŒ…æ‹¬ DOI æŸ¥è¯¢ã€SciHub ä¸‹è½½ã€æ–‡ä»¶ç®¡ç†ç­‰åŠŸèƒ½
åŸºäº RecursiveScholarCrawler é¡¹ç›®çš„ä¸‹è½½åŠŸèƒ½è¿›è¡Œä¼˜åŒ–å’Œé›†æˆ
"""

import os
import re
import time
import random
import hashlib
import requests
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from urllib.parse import urljoin, quote_plus
from bs4 import BeautifulSoup
import urllib.parse

from utils.logger import LoggerMixin
from utils.file_handler import FileHandler
from utils.api_manager import api_manager
from .scihub_downloader import SciHubDownloader

logger = logging.getLogger(__name__)


class PDFDownloader(LoggerMixin):
    """PDF ä¸‹è½½å™¨ - æ”¯æŒå¤šæºä¸‹è½½ã€DOI æŸ¥è¯¢ã€æ–‡ä»¶ç®¡ç†"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ– PDF ä¸‹è½½å™¨

        Args:
            config: ä¸‹è½½é…ç½®
        """
        self.config = config
        self.download_dir = Path(config.get('download_dir', './results/pdfs'))
        self.max_retries = config.get('max_retries', 3)
        self.retry_delay = config.get('retry_delay', 5)
        self.timeout = config.get('timeout', 30)
        self.max_workers = config.get('max_workers', 4)
        self.verify_pdf = config.get('verify_pdf', True)
        self.max_file_size = config.get('max_file_size', 100 * 1024 * 1024)  # 100MB

        # SciHub é•œåƒé…ç½®
        self.scihub_mirrors = config.get('scihub_mirrors', [
            "https://sci-hub.se", "https://sci-hub.st", "https://sci-hub.ru", "https://www.sci-hub.ren",
            "https://www.sci-hub.ee"
        ])

        # ç”¨æˆ·ä»£ç†é…ç½®
        self.user_agents = config.get('user_agents', [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        ])

        # DOI API é…ç½®
        self.doi_apis = config.get('doi_apis',
                                   {'crossref': {
                                       'url': 'https://api.crossref.org/works',
                                       'enabled': True,
                                       'timeout': 15
                                   }})

        # åˆ›å»ºä¸‹è½½ç›®å½•
        self.download_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–ä¼šè¯
        self.session = requests.Session()
        self._setup_session()

        # åˆå§‹åŒ– SciHub ä¸‹è½½å™¨
        self.scihub = SciHubDownloader(mirrors=self.scihub_mirrors,
                                       user_agents=self.user_agents,
                                       timeout=self.timeout,
                                       max_retries=self.max_retries)

        # PMC å’Œå¼€æ”¾è·å–ä»“åº“é…ç½®
        self.oa_repositories = {
            'pmc': {
                'base_url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/',
                'pdf_patterns': ['/pdf', '/pdf/{pmc_id}.pdf'],
                'api_url': 'https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/',
                'enabled': True
            },
            'europepmc': {
                'base_url': 'https://europepmc.org/articles/',
                'pdf_patterns': ['?pdf=render', '/backend/ptpmcrender.fcgi?accid={pmc_id}&blobtype=pdf'],
                'enabled': True
            },
            'crossref': {
                'api_url': 'https://api.crossref.org/works/',
                'enabled': True
            }
        }

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {'total_downloads': 0, 'successful_downloads': 0, 'failed_downloads': 0, 'retries': 0, 'total_size': 0}

        self.logger.info(f"âœ… PDF ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆï¼Œä¸‹è½½ç›®å½•: {self.download_dir}")

    def _setup_session(self):
        """è®¾ç½® HTTP ä¼šè¯"""
        self.session.headers.update({
            'User-Agent': self._get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Referer': 'https://www.google.com/'
        })

    def _get_random_user_agent(self) -> str:
        """è·å–éšæœºç”¨æˆ·ä»£ç†"""
        return random.choice(self.user_agents)

    def _get_random_mirrors(self, exclude: Optional[List[str]] = None, count: Optional[int] = None) -> List[str]:
        """
        è·å–éšæœºæ’åºçš„é•œåƒåˆ—è¡¨

        Args:
            exclude: æ’é™¤çš„é•œåƒåˆ—è¡¨
            count: è¿”å›çš„é•œåƒæ•°é‡

        Returns:
            é•œåƒåˆ—è¡¨
        """
        available = list(set(self.scihub_mirrors))  # å»é‡
        if exclude:
            available = [m for m in available if m not in exclude]
        random.shuffle(available)
        if count and count < len(available):
            return available[:count]
        return available

    def _clean_filename(self, title: str, doi: Optional[str] = None, pmid: Optional[str] = None) -> str:
        """
        æ¸…ç†æ–‡ä»¶å

        Args:
            title: è®ºæ–‡æ ‡é¢˜
            doi: DOI æ ‡è¯†ç¬¦
            pmid: PMID æ ‡è¯†ç¬¦

        Returns:
            æ¸…ç†åçš„æ–‡ä»¶å
        """
        if title:
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œæˆªæ–­é•¿åº¦
            cleaned = re.sub(r'[\\/*?:"<>|]', "", title).strip()[:100].replace("", "_")
        else:
            cleaned = "unknown_paper"

        # æ·»åŠ æ ‡è¯†ç¬¦
        if doi:
            cleaned_doi = doi.replace("/", "_").replace(".", "-")
            return f"{cleaned}_{cleaned_doi}.pdf"
        elif pmid:
            return f"{cleaned}_PMID{pmid}.pdf"
        else:
            return f"{cleaned}.pdf"

    def _calculate_file_hash(self, file_path: Path) -> str:
        """
        è®¡ç®—æ–‡ä»¶ MD5 å“ˆå¸Œå€¼

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            MD5 å“ˆå¸Œå€¼
        """
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            self.logger.warning(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼å¤±è´¥: {e}")
            return ""

    def _extract_pdf_url_from_html(self, html_content: str, pmc_id: str) -> Optional[str]:
        """
        ä» PMC HTML é¡µé¢ä¸­æå– PDF ä¸‹è½½é“¾æ¥
        åŸºäº test.py çš„æˆåŠŸæ–¹æ¡ˆï¼Œä½¿ç”¨å¤šçº§å›é€€ç­–ç•¥

        Args:
            html_content: HTML é¡µé¢å†…å®¹
            pmc_id: PMC ID

        Returns:
            PDF ä¸‹è½½ URLï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        try:
            import re
            from bs4 import BeautifulSoup

            # è§£æ HTML é¡µé¢
            soup = BeautifulSoup(html_content, 'html.parser')
            host = "pmc.ncbi.nlm.nih.gov"
            base_article_url = f"https://{host}/articles/PMC{pmc_id}/"
            base_pdf_url = f"https://{host}/articles/PMC{pmc_id}/pdf"

            # ç­–ç•¥ 1ï¼šåŸºäº test.py çš„ CSS ç²¾ç¡®å®šä½
            # 1) ä¼˜å…ˆï¼šCSS ç²¾ç¡®å®šä½æ­£æ–‡ PDF æŒ‰é’®
            pdf_links = []

            # æŸ¥æ‰¾ä»¥ /pdf/ ç»“å°¾çš„é“¾æ¥
            pdf_end_links = soup.find_all('a', href=re.compile(r'/pdf/$'))
            if pdf_end_links:
                pdf_links.extend(pdf_end_links)
                self.logger.debug(f"ç­–ç•¥ 1a: æ‰¾åˆ° {len(pdf_end_links)} ä¸ªä»¥ / pdf / ç»“å°¾çš„é“¾æ¥")

            # æŸ¥æ‰¾ä»¥ /pdf ç»“å°¾çš„é“¾æ¥ï¼ˆä¸å¸¦æ–œæ ï¼‰
            pdf_links.extend(soup.find_all('a', href=re.compile(r'/pdf$')))
            self.logger.debug(f"ç­–ç•¥ 1b: æ‰¾åˆ° {len(soup.find_all('a', href=re.compile(r'/pdf$')))} ä¸ªä»¥ / pdf ç»“å°¾çš„é“¾æ¥")

            # å¦‚æœæ‰¾åˆ°é“¾æ¥ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª
            if pdf_links:
                first_link = pdf_links[0]
                href = first_link.get('href', '')
                if href:
                    pdf_url = self._build_full_url(href, host, pmc_id)
                    if pdf_url:
                        self.logger.info(f"ç­–ç•¥ 1 æˆåŠŸ: é€šè¿‡ CSS ç²¾ç¡®å®šä½æ‰¾åˆ° PDF é“¾æ¥: {pdf_url}")
                        return pdf_url

            # ç­–ç•¥ 2ï¼šARIA åç§°ä»¥ PDF å¼€å¤´çš„é“¾æ¥ï¼ˆæ’é™¤è¡¥å……ææ–™çš„æ–‡ä»¶åï¼‰
            aria_links = []
            for link in soup.find_all('a', attrs={"aria-label": True}):
                aria_label = link.get('aria-label', '')
                if re.match(r'^PDF\b', aria_label, re.I):
                    aria_links.append(link)

            if aria_links:
                aria_link = aria_links[0]
                href = aria_link.get('href', '')
                if href:
                    pdf_url = self._build_full_url(href, host, pmc_id)
                    if pdf_url:
                        self.logger.info(f"ç­–ç•¥ 2 æˆåŠŸ: é€šè¿‡ ARIA æ ‡ç­¾æ‰¾åˆ° PDF é“¾æ¥: {pdf_url}")
                        return pdf_url

            # ç­–ç•¥ 3ï¼šæ–‡æœ¬åŒ…å« "Download PDF"
            download_text_links = []
            for link in soup.find_all('a'):
                text = link.get_text(strip=True)
                if re.search(r'Download PDF', text, re.I):
                    download_text_links.append(link)

            if download_text_links:
                download_link = download_text_links[0]
                href = download_link.get('href', '')
                if href:
                    pdf_url = self._build_full_url(href, host, pmc_id)
                    if pdf_url:
                        self.logger.info(f"ç­–ç•¥ 3 æˆåŠŸ: é€šè¿‡'Download PDF'æ–‡æœ¬æ‰¾åˆ° PDF é“¾æ¥: {pdf_url}")
                        return pdf_url

            # ç­–ç•¥ 4ï¼šæŸ¥æ‰¾ç‰¹å®š class çš„ä¸‹è½½é“¾æ¥ï¼ˆåŸæœ‰æ–¹æ³•ä½œä¸ºå¤‡ç”¨ï¼‰
            class_links = soup.find_all('a', class_='usa-link display-flex usa-tooltip')
            self.logger.debug(f"ç­–ç•¥ 4: æ‰¾åˆ° {len(class_links)} ä¸ª usa-link display-flex usa-tooltip é“¾æ¥")

            if len(class_links) >= 2:
                # è·å–ç¬¬äºŒä¸ªé“¾æ¥ï¼ˆé€šå¸¸åŒ…å« PDF ä¸‹è½½é“¾æ¥ï¼‰
                second_link = class_links[1]
                href = second_link.get('href', '')
                if href:
                    pdf_url = self._build_full_url(href, host, pmc_id)
                    if pdf_url:
                        self.logger.info(f"ç­–ç•¥ 4 æˆåŠŸ: é€šè¿‡ tooltip class æ‰¾åˆ° PDF é“¾æ¥: {pdf_url}")
                        return pdf_url

            # ç­–ç•¥ 5ï¼šæŸ¥æ‰¾åŒ…å« PDF çš„æ‰€æœ‰é“¾æ¥
            all_pdf_links = []
            for link in soup.find_all('a', href=True):
                href = link.get('href', '').lower()
                text = link.get_text(strip=True).lower()
                # æŸ¥æ‰¾é“¾æ¥åœ°å€æˆ–æ–‡æœ¬ä¸­åŒ…å« pdf çš„é“¾æ¥
                if 'pdf' in href or 'pdf' in text:
                    # æ’é™¤æ˜æ˜¾çš„éæ­£æ–‡ PDF é“¾æ¥ï¼ˆå¦‚è¡¥å……ææ–™ï¼‰
                    if not any(exclude in href for exclude in ['supplementary', 'supplement', 'appendix']):
                        all_pdf_links.append(link)

            if all_pdf_links:
                # é€‰æ‹©æœ€æœ‰å¯èƒ½çš„ PDF é“¾æ¥
                for link in all_pdf_links[:3]:  # åªæ£€æŸ¥å‰ 3 ä¸ª
                    href = link.get('href', '')
                    if href:
                        pdf_url = self._build_full_url(href, host, pmc_id)
                        if pdf_url:
                            self.logger.info(f"ç­–ç•¥ 5 æˆåŠŸ: é€šè¿‡ PDF å…³é”®è¯æ‰¾åˆ° PDF é“¾æ¥: {pdf_url}")
                            return pdf_url

            # ç­–ç•¥ 6ï¼šç›´æ¥ PDF URL å°è¯•
            direct_urls = [
                f"https://{host}/articles/{pmc_id}/pdf",
                f"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmc_id}/pdf",
                f"https://{host}/articles/{pmc_id}/pdf/{pmc_id}.pdf",
            ]

            # å¿«é€ŸéªŒè¯ç›´æ¥ URL
            for test_url in direct_urls:
                try:
                    import requests
                    head_response = requests.head(test_url, timeout=5, allow_redirects=True)
                    if head_response.status_code == 200:
                        content_type = head_response.headers.get('Content-Type', '').lower()
                        if 'pdf' in content_type:
                            self.logger.info(f"ç­–ç•¥ 6 æˆåŠŸ: ç›´æ¥ PDF URL éªŒè¯æˆåŠŸ: {test_url}")
                            return test_url
                except:
                    continue

            self.logger.warning(f"æ‰€æœ‰ç­–ç•¥éƒ½æœªèƒ½æå–åˆ° PMC{pmc_id} çš„ PDF é“¾æ¥")
            return None

        except ImportError:
            self.logger.warning("ç¼ºå°‘ BeautifulSoup åº“ï¼Œæ— æ³•è§£æ HTML é¡µé¢")
            return None
        except Exception as e:
            self.logger.error(f"è§£æ HTML é¡µé¢æå– PDF é“¾æ¥å¤±è´¥: {e}")
            return None

    def _build_full_url(self, href: str, host: str, pmc_id: str) -> Optional[str]:
        """
        æ„å»ºå®Œæ•´çš„ PDF URL

        Args:
            href: ç›¸å¯¹æˆ–ç»å¯¹ URL
            host: ä¸»æœºå
            pmc_id: PMC ID

        Returns:
            å®Œæ•´çš„ URL æˆ– None
        """
        try:
            if not href:
                return None

            href = href.strip()

            if href.startswith('http'):
                return href
            elif href.startswith('//'):
                return f"https:{href}"
            elif href.startswith('/'):
                return f"https://{host}{href}"
            else:
                # ç›¸å¯¹è·¯å¾„ï¼Œæ„å»ºå®Œæ•´ URL
                return f"https://{host}/articles/PMC{pmc_id}/{href}"
        except Exception as e:
            self.logger.debug(f"æ„å»ºå®Œæ•´ URL å¤±è´¥: {e}")
            return None

    def _validate_pdf_url(self, pdf_url: str, article_url: str = None, timeout: int = 10) -> Tuple[bool, str]:
        """
        éªŒè¯ PDF URL æ˜¯å¦æœ‰æ•ˆï¼Œå¤„ç† PMC çš„å¼‚æ­¥å‡†å¤‡é¡µé¢

        Args:
            pdf_url: PDF URL
            article_url: æ–‡ç« é¡µé¢ URLï¼ˆç”¨äº Refererï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´

        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, è¯¦ç»†ä¿¡æ¯)
        """
        try:
            headers = {
                'User-Agent': self._get_random_user_agent(),
                'Referer': article_url or 'https://www.ncbi.nlm.nih.gov/pmc/',
                'Accept': 'application/pdf,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            }

            # ç¬¬ä¸€æ¬¡è¯·æ±‚
            response = self.session.head(pdf_url, timeout=timeout, headers=headers, allow_redirects=True)

            if response.status_code == 200:
                content_type = response.headers.get('Content-Type', '').lower()
                content_length = response.headers.get('Content-Length', '0')

                self.logger.debug(f"PDF URL éªŒè¯ - Status: {response.status_code}, Type: {content_type}, Size: {content_length}")

                if 'pdf' in content_type:
                    return True, f"æœ‰æ•ˆçš„ PDF é“¾æ¥: {content_type}, å¤§å°: {content_length} bytes"
                elif 'html' in content_type:
                    # å¯èƒ½æ˜¯å‡†å¤‡é¡µé¢ï¼Œè¿›è¡Œ GET è¯·æ±‚è¿›ä¸€æ­¥éªŒè¯
                    get_response = self.session.get(pdf_url, timeout=timeout, headers=headers)
                    if get_response.status_code == 200:
                        get_content_type = get_response.headers.get('Content-Type', '').lower()
                        if 'pdf' in get_content_type:
                            return True, f"æœ‰æ•ˆçš„ PDF é“¾æ¥ï¼ˆGET è¯·æ±‚ï¼‰: {get_content_type}, å¤§å°: {content_length} bytes"
                        else:
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«å‡†å¤‡é¡µé¢çš„å…³é”®è¯
                            response_text = get_response.text[:1000].lower()
                            if any(keyword in response_text for keyword in ['preparing', 'download', 'pdf', 'loading']):
                                return True, f"PMC å‡†å¤‡é¡µé¢ï¼Œå¯èƒ½éœ€è¦ç­‰å¾…: {get_content_type}"
                            else:
                                return False, f"HTML é¡µé¢ä¸æ˜¯å‡†å¤‡é¡µé¢: {get_content_type}"
                    else:
                        return False, f"GET è¯·æ±‚å¤±è´¥: HTTP {get_response.status_code}"
                else:
                    return False, f"æœªçŸ¥çš„ Content-Type: {content_type}"
            else:
                return False, f"HTTP è¯·æ±‚å¤±è´¥: {response.status_code}"

        except requests.exceptions.Timeout:
            return False, "è¯·æ±‚è¶…æ—¶"
        except Exception as e:
            return False, f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}"

    def _validate_pdf_file(self, file_path: Path) -> bool:
        """
        éªŒè¯ PDF æ–‡ä»¶æœ‰æ•ˆæ€§

        Args:
            file_path: PDF æ–‡ä»¶è·¯å¾„

        Returns:
            æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ PDF æ–‡ä»¶
        """
        if not file_path.exists():
            return False

        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            if file_size < 1024:  # å°äº 1KB å¯èƒ½ä¸æ˜¯æœ‰æ•ˆ PDF
                self.logger.warning(f"PDF æ–‡ä»¶è¿‡å°: {file_size} bytes")
                return False

            if file_size > self.max_file_size:
                self.logger.warning(f"PDF æ–‡ä»¶è¿‡å¤§: {file_size} bytes")
                return False

            # æ£€æŸ¥ PDF æ–‡ä»¶å¤´
            with open(file_path, 'rb') as f:
                header = f.read(8)
                if not header.startswith(b'%PDF'):
                    self.logger.warning("æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ PDF æ ¼å¼")
                    return False

            # å¦‚æœå¯ç”¨äº† PDF éªŒè¯ï¼Œä½¿ç”¨ PyMuPDF éªŒè¯
            if self.verify_pdf:
                try:
                    import fitz  # PyMuPDF
                    with fitz.open(str(file_path)) as doc:
                        if doc.page_count > 0:
                            self.logger.debug(f"âœ… PDF éªŒè¯æˆåŠŸ: {doc.page_count} é¡µ")
                            return True
                        else:
                            self.logger.warning("PDF æ–‡ä»¶æ²¡æœ‰é¡µé¢å†…å®¹")
                            return False
                except ImportError:
                    self.logger.warning("PyMuPDF æœªå®‰è£…ï¼Œè·³è¿‡ PDF ç»“æ„éªŒè¯")
                    return True  # åªè¿›è¡ŒåŸºæœ¬éªŒè¯
                except Exception as e:
                    self.logger.warning(f"PDF ç»“æ„éªŒè¯å¤±è´¥: {e}")
                    return False

            return True

        except Exception as e:
            self.logger.error(f"PDF æ–‡ä»¶éªŒè¯å‡ºé”™: {e}")
            return False

    def _find_pdf_link_in_html(self, html_content: str, base_url: str) -> Optional[str]:
        """
        ä» HTML å†…å®¹ä¸­æŸ¥æ‰¾ PDF ä¸‹è½½é“¾æ¥

        Args:
            html_content: HTML å†…å®¹
            base_url: åŸºç¡€ URL

        Returns:
            PDF ä¸‹è½½é“¾æ¥æˆ– None
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')

            # æŸ¥æ‰¾ embed å’Œ iframe æ ‡ç­¾
            for tag in soup.find_all(['embed', 'iframe']):
                src = tag.get('src')
                if src:
                    if src.startswith('//'):
                        return f"https:{src}"
                    if not src.startswith('http'):
                        return urljoin(base_url, src)
                    return src

            # æŸ¥æ‰¾ PDF ä¸‹è½½é“¾æ¥
            for link in soup.find_all('a', href=True):
                href = link['href']
                if ('pdf' in href.lower() or link.get('id') == 'download' or 'download' in link.get('class', [])):
                    if href.startswith('//'):
                        return f"https:{href}"
                    if not href.startswith('http'):
                        return urljoin(base_url, href)
                    return href

            return None

        except Exception as e:
            self.logger.error(f"è§£æ HTML æŸ¥æ‰¾ PDF é“¾æ¥æ—¶å‡ºé”™: {e}")
            return None

    def _download_and_save_pdf(self,
                               url: str = None,
                               response: requests.Response = None,
                               output_path: Path = None,
                               timeout: Optional[int] = None,
                               expected_size: int = None) -> Tuple[bool, Optional[str]]:
        """
        ç»Ÿä¸€çš„ PDF ä¸‹è½½å’Œä¿å­˜å‡½æ•°

        Args:
            url: ä¸‹è½½ URLï¼ˆå¦‚æœä¸æä¾› response åˆ™å¿…éœ€ï¼‰
            response: å·²æœ‰çš„ HTTP å“åº”å¯¹è±¡ï¼ˆå¯é€‰ï¼‰
            output_path: è¾“å‡ºè·¯å¾„
            timeout: è¶…æ—¶æ—¶é—´
            expected_size: æœŸæœ›çš„æ–‡ä»¶å¤§å°

        Returns:
            (æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯)
        """
        try:
            timeout = timeout or self.timeout

            # å¦‚æœæ²¡æœ‰æä¾›å“åº”å¯¹è±¡ï¼Œåˆ™ä¸‹è½½
            if response is None:
                if not url:
                    return False, "ç¼ºå°‘ URL æˆ–å“åº”å¯¹è±¡"

                # ä¸‹è½½æ–‡ä»¶
                response = self.session.get(url, timeout=timeout, stream=True)
                response.raise_for_status()

            # ç¡®ä¿è¾“å‡ºè·¯å¾„å­˜åœ¨
            if output_path is None:
                return False, "ç¼ºå°‘è¾“å‡ºè·¯å¾„"

            # ä¿å­˜æ–‡ä»¶
            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)

            # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
            if self._validate_pdf_file(output_path, expected_size):
                file_size = output_path.stat().st_size
                self.stats['total_size'] += file_size
                self.logger.info(f"âœ… PDF ä¿å­˜æˆåŠŸ: {output_path.name} ({file_size/1024:.1f}KB)")
                return True, None
            else:
                # åˆ é™¤æ— æ•ˆæ–‡ä»¶
                if output_path.exists():
                    output_path.unlink()
                return False, "ä¿å­˜çš„æ–‡ä»¶éªŒè¯å¤±è´¥ï¼Œä¸æ˜¯æœ‰æ•ˆçš„ PDF"

        except requests.exceptions.Timeout:
            return False, f"ä¸‹è½½è¶…æ—¶ ({timeout} ç§’)"
        except requests.exceptions.RequestException as e:
            return False, f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}"
        except IOError as e:
            self.logger.error(f"æ–‡ä»¶å†™å…¥å¤±è´¥: {e}")
            if output_path and output_path.exists():
                output_path.unlink(missing_ok=True)
            return False, f"æ–‡ä»¶å†™å…¥å¤±è´¥: {e}"
        except Exception as e:
            self.logger.error(f"ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {e}")
            if output_path and output_path.exists():
                output_path.unlink(missing_ok=True)
            return False, f"ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {e}"

    def get_download_stats(self) -> Dict[str, Any]:
        """
        è·å–ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        success_rate = 0
        if self.stats['total_downloads'] > 0:
            success_rate = (self.stats['successful_downloads'] / self.stats['total_downloads']) * 100

        return {
            **self.stats, 'success_rate':
            round(success_rate, 2),
            'average_file_size':
            (self.stats['total_size'] / self.stats['successful_downloads'] if self.stats['successful_downloads'] > 0 else 0)
        }

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {'total_downloads': 0, 'successful_downloads': 0, 'failed_downloads': 0, 'retries': 0, 'total_size': 0}
        self.logger.info("ğŸ“Š ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")

    def _create_download_directory(self) -> bool:
        """åˆ›å»ºä¸‹è½½ç›®å½•"""
        try:
            self.download_dir.mkdir(parents=True, exist_ok=True)
            return True
        except Exception as e:
            self.logger.error(f"åˆ›å»ºä¸‹è½½ç›®å½•å¤±è´¥: {e}")
            return False

    def _sanitize_doi(self, doi: str) -> str:
        """å°† DOI è½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶åéƒ¨åˆ†"""
        if not doi:
            return "unknown"
        safe_doi = doi.replace('/', '_').replace('\\', '_')
        safe_doi = ''.join(c for c in safe_doi if c.isalnum() or c in '._-')
        return safe_doi

    def _generate_filename(self, doi: str, source: str = "download", title: str = None) -> str:
        """
        ç”Ÿæˆç»Ÿä¸€çš„æ–‡ä»¶åæ ¼å¼

        Args:
            doi: DOI æ ‡è¯†ç¬¦
            source: ä¸‹è½½æºæ ‡è¯† (é»˜è®¤ä¸º "download")
            title: è®ºæ–‡æ ‡é¢˜ (å¯é€‰)

        Returns:
            ç»Ÿä¸€æ ¼å¼çš„æ–‡ä»¶åï¼Œå¦‚ {doi}_{source}.pdf
        """
        safe_doi = self._sanitize_doi(doi)
        suffix = (source or "download").lower()

        if title:
            # å¦‚æœæœ‰æ ‡é¢˜ï¼Œæ·»åŠ åˆ°æ–‡ä»¶åä¸­
            safe_title = ''.join(c for c in title if c.isalnum() or c in '._-')
            safe_title = safe_title.replace('_', '_')[:50]  # é™åˆ¶é•¿åº¦
            return f"{safe_doi}_{suffix}_{safe_title}.pdf"
        else:
            return f"{safe_doi}_{suffix}.pdf"

    def download_from_scihub(self, doi: str) -> Tuple[bool, Optional[Path], Optional[str]]:
        """ä» SciHub ä¸‹è½½ PDFï¼Œè¿”å› (æˆåŠŸ, è·¯å¾„, é”™è¯¯)"""
        try:
            self.logger.info(f"å°è¯•ä» SciHub ä¸‹è½½: {doi}")
            filename = f"{self._sanitize_doi(doi)}_SciHub.pdf"
            output_path = self.download_dir / filename

            success, error_msg = self.scihub.download_by_doi(doi, output_path)
            if success:
                if self._validate_pdf_file(output_path):
                    file_size = output_path.stat().st_size
                    self.logger.info(f"âœ… SciHub ä¸‹è½½æˆåŠŸ: {filename} ({file_size} bytes)")
                    return True, output_path, None
                else:
                    output_path.unlink(missing_ok=True)
                    return False, None, "ä¸‹è½½çš„ PDF éªŒè¯å¤±è´¥"
            else:
                return False, None, error_msg or "SciHub ä¸‹è½½å¤±è´¥"
        except Exception as e:
            return False, None, str(e)

    def download_with_retry(self,
                            download_callable,
                            *args,
                            max_retries: Optional[int] = None,
                            retry_delay: Optional[int] = None,
                            **kwargs) -> Tuple[bool, Optional[Path], Optional[str]]:
        """é€šç”¨é‡è¯•åŒ…è£…å™¨ï¼Œæ¥å—ä¸€ä¸ªè¿”å› (æˆåŠŸ, è·¯å¾„, é”™è¯¯) çš„ä¸‹è½½å‡½æ•°"""
        retries = max_retries if max_retries is not None else self.max_retries
        delay = retry_delay if retry_delay is not None else self.retry_delay
        last_error = None
        for attempt in range(1, retries + 1):
            try:
                success, path, error = download_callable(*args, **kwargs)
                if success:
                    return True, path, None
                last_error = error
                self.logger.info(f"é‡è¯• {attempt}/{retries} å¤±è´¥: {error}. ç­‰å¾… {delay} ç§’...")
                time.sleep(delay)
            except Exception as e:
                last_error = str(e)
                self.logger.info(f"é‡è¯• {attempt}/{retries} å¼‚å¸¸: {e}. ç­‰å¾… {delay} ç§’...")
                time.sleep(delay)
        return False, None, last_error or "é‡è¯•åä»å¤±è´¥"

    def _handle_duplicate_file(self, file_path: Path) -> Path:
        """å¤„ç†é‡å¤æ–‡ä»¶å"""
        if not file_path.exists():
            return file_path

        # ç”Ÿæˆæ–°çš„æ–‡ä»¶å
        base_path = file_path.parent
        base_name = file_path.stem
        extension = file_path.suffix

        counter = 1
        while True:
            new_name = f"{base_name}_{counter}{extension}"
            new_path = base_path / new_name
            if not new_path.exists():
                return new_path
            counter += 1

            # é˜²æ­¢æ— é™å¾ªç¯
            if counter > 1000:
                import time
                timestamp = int(time.time())
                new_name = f"{base_name}_{timestamp}{extension}"
                return base_path / new_name

    def _normalize_title(self, title: str) -> str:
        """
        æ ‡å‡†åŒ–è®ºæ–‡æ ‡é¢˜ä»¥æé«˜åŒ¹é…å‡†ç¡®æ€§

        Args:
            title: åŸå§‹æ ‡é¢˜

        Returns:
            æ ‡å‡†åŒ–åçš„æ ‡é¢˜
        """
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œè½¬æ¢ä¸ºå°å†™ï¼Œåˆå¹¶ç©ºç™½å­—ç¬¦
        clean_title = re.sub(r'[^\w\s]', ' ', title)
        clean_title = ' '.join(clean_title.lower().split())
        return clean_title

    def _calculate_similarity_score(self, title1: str, title2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦åˆ†æ•°

        Args:
            title1: æ ‡é¢˜ 1
            title2: æ ‡é¢˜ 2

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        try:
            from difflib import SequenceMatcher
            normalized1 = self._normalize_title(title1)
            normalized2 = self._normalize_title(title2)
            return SequenceMatcher(None, normalized1, normalized2).ratio()
        except ImportError:
            # å¦‚æœæ²¡æœ‰ difflibï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…
            normalized1 = self._normalize_title(title1)
            normalized2 = self._normalize_title(title2)
            if normalized1 == normalized2:
                return 1.0
            elif normalized1 in normalized2 or normalized2 in normalized1:
                return 0.8
            else:
                return 0.0

    def check_open_access_status(self, doi: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ–‡ç« çš„å¼€æ”¾è·å–çŠ¶æ€

        Args:
            doi: DOI æ ‡è¯†ç¬¦

        Returns:
            å¼€æ”¾è·å–çŠ¶æ€ä¿¡æ¯
        """
        self.logger.info(f"æ£€æŸ¥å¼€æ”¾è·å–çŠ¶æ€: {doi}")

        result = {
            'doi': doi,
            'is_open_access': False,
            'license': None,
            'pmc_id': None,
            'oa_locations': [],
            'pdf_urls': [],
            'source': None
        }

        try:
            # æŸ¥è¯¢ Crossref API
            crossref_url = f"https://api.crossref.org/works/{doi}"
            response = self.session.get(crossref_url, timeout=15)

            if response.status_code == 200:
                data = response.json()
                work = data.get('message', {})

                # æ£€æŸ¥è®¸å¯è¯ä¿¡æ¯
                licenses = work.get('license', [])
                if licenses:
                    result['license'] = licenses[0].get('URL', '')
                    if any(lic in result['license'].lower() for lic in ['cc-by', 'creative-commons']):
                        result['is_open_access'] = True

                # æ£€æŸ¥å¼€æ”¾è·å–æ ‡è®°
                if work.get('is-referenced-by-count', 0) > 0:
                    result['is_open_access'] = True

                # æŸ¥æ‰¾ PMC ID
                for link in work.get('link', []):
                    url = link.get('URL', '')
                    if 'pmc' in url.lower():
                        pmc_match = re.search(r'PMC(\d+)', url)
                        if pmc_match:
                            result['pmc_id'] = pmc_match.group(1)
                            result['is_open_access'] = True

                # æŸ¥æ‰¾ PDF é“¾æ¥
                for link in work.get('link', []):
                    if link.get('content-type') == 'application/pdf':
                        result['pdf_urls'].append(link.get('URL'))

                result['source'] = 'crossref'
                self.logger.info(f"Crossref æŸ¥è¯¢å®Œæˆ: OA={result['is_open_access']}, PMC={result['pmc_id']}")

        except Exception as e:
            self.logger.warning(f"Crossref æŸ¥è¯¢å¤±è´¥: {e}")

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° PMC IDï¼Œå°è¯• PMC ID è½¬æ¢ API
        if not result['pmc_id']:
            try:
                pmc_api_url = f"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?tool=pubminer&email=user@example.com&ids={doi}&format=json"
                response = self.session.get(pmc_api_url, timeout=10)

                if response.status_code == 200:
                    data = response.json()
                    records = data.get('records', [])
                    if records and 'pmcid' in records[0]:
                        pmc_id = records[0]['pmcid'].replace('PMC', '')
                        result['pmc_id'] = pmc_id
                        result['is_open_access'] = True
                        self.logger.info(f"PMC ID è½¬æ¢æˆåŠŸ: PMC{pmc_id}")

            except Exception as e:
                self.logger.warning(f"PMC ID è½¬æ¢å¤±è´¥: {e}")

        return result

    def download_from_pmc(self, pmc_id: str, doi: str = None) -> Tuple[bool, Optional[Path], Optional[str]]:
        """
        ä» PMC ä¸‹è½½ PDFï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨åŸºäº test.py çš„å¢å¼ºç­–ç•¥

        Args:
            pmc_id: PMC ID
            doi: DOI (å¯é€‰ï¼Œç”¨äºæ–‡ä»¶å‘½å)

        Returns:
            (æˆåŠŸæ ‡å¿—, æ–‡ä»¶è·¯å¾„, é”™è¯¯ä¿¡æ¯)
        """
        self.logger.info(f"å°è¯•ä» PMC ä¸‹è½½: PMC{pmc_id}")

        # ç­–ç•¥é¡ºåºï¼šEuropePMC é¦–é€‰ -> Playwright å¤‡é€‰ -> ä¼ ç»Ÿæ–¹æ³•å…œåº•
        # ç­–ç•¥ 1ï¼šä¼˜å…ˆä½¿ç”¨ EuropePMCï¼ˆå·²éªŒè¯æˆåŠŸç‡é«˜ï¼‰
        self.logger.info("ç­–ç•¥ 1: å°è¯• EuropePMCï¼ˆé¦–é€‰ï¼Œå·²éªŒè¯æˆåŠŸï¼‰...")
        try:
            europepmc_urls = [
                f"https://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC{pmc_id}&blobtype=pdf",
                f"https://europepmc.org/articles/PMC{pmc_id}?pdf=render"
            ]

            for i, url in enumerate(europepmc_urls):
                self.logger.info(f"å°è¯• EuropePMC URL {i+1}/{len(europepmc_urls)}: {url}")

                # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´å’Œé‡è¯•
                success, file_path, error = self.download_with_retry(urls=[url],
                                                                     output_dir=self.download_dir,
                                                                     max_retries=3,
                                                                     use_scihub_fallback=False)

                if success and file_path and file_path.exists():
                    self.logger.info(f"âœ… EuropePMC é¦–é€‰ç­–ç•¥æˆåŠŸ: {file_path.name}")
                    return True, file_path, None
                else:
                    self.logger.debug(f"EuropePMC URL {i+1} å¤±è´¥: {error}")

        except Exception as e:
            self.logger.warning(f"EuropePMC é¦–é€‰ç­–ç•¥å¤±è´¥: {e}")

        # ç­–ç•¥ 2ï¼šä½¿ç”¨ Playwright ä½œä¸ºå¤‡é€‰ç­–ç•¥
        self.logger.info("ç­–ç•¥ 2: å°è¯• Playwright ç­–ç•¥ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰...")
        try:
            playwright_success, playwright_path = self._download_with_playwright(pmc_id, doi)
            if playwright_success:
                self.logger.info("âœ… Playwright å¤‡é€‰ç­–ç•¥æˆåŠŸ")
                return True, playwright_path, None
            else:
                self.logger.warning("Playwright å¤‡é€‰ç­–ç•¥æœªæˆåŠŸ")
        except ImportError as e:
            self.logger.warning(f"Playwright ä¸å¯ç”¨: {e}")
        except Exception as e:
            self.logger.warning(f"Playwright å¤‡é€‰ç­–ç•¥å¤±è´¥: {e}")

        # ç­–ç•¥ 3ï¼šä¼ ç»Ÿæ–¹æ³•ä½œä¸ºæœ€åå…œåº•
        self.logger.info("ç­–ç•¥ 3: å°è¯•ä¼ ç»Ÿ PMC è§£ææ–¹æ³•ï¼ˆå…œåº•ï¼‰...")
        try:
            article_url = f"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmc_id}/"
            headers = {'User-Agent': self._get_random_user_agent(), 'Referer': 'https://www.google.com/'}
            response = self.session.get(article_url, timeout=30, headers=headers)

            if response.status_code == 200 and 'html' in response.headers.get('Content-Type', '').lower():
                self.logger.info("æˆåŠŸè·å– PMC æ–‡ç« é¡µé¢ï¼Œå¼€å§‹è§£æ PDF é“¾æ¥...")
                pdf_url = self._extract_pdf_url_from_html(response.text, pmc_id)

                if pdf_url:
                    filename = self._generate_filename(doi, "PMC")
                    output_path = self.download_dir / filename
                    self.logger.info(f"å°è¯•ä¼ ç»Ÿä¸‹è½½: {pdf_url}")

                    # ç®€åŒ–çš„ä¸‹è½½é€»è¾‘
                    resp = self.session.get(pdf_url, timeout=30, stream=True)
                    if resp.status_code == 200 and 'pdf' in resp.headers.get('Content-Type', '').lower():
                        success, _ = self._download_and_save_pdf(response=resp, output_path=output_path)
                        if success:
                            self.logger.info("âœ… ä¼ ç»Ÿæ–¹æ³•å…œåº•æˆåŠŸ")
                            return True, output_path, None
        except Exception as e:
            self.logger.warning(f"ä¼ ç»Ÿæ–¹æ³•å…œåº•å¤±è´¥: {e}")

        return False, None, "æ‰€æœ‰ PMC ä¸‹è½½ç­–ç•¥å‡å¤±è´¥"

    def query_doi_by_title(self, title: str, api: str = 'crossref') -> Dict[str, Any]:
        """
        é€šè¿‡æ ‡é¢˜æŸ¥è¯¢ DOI ä¿¡æ¯

        Args:
            title: è®ºæ–‡æ ‡é¢˜
            api: ä½¿ç”¨çš„ API æœåŠ¡ ('crossref')

        Returns:
            DOI æŸ¥è¯¢ç»“æœå­—å…¸
        """
        self.logger.info(f"ğŸ” æŸ¥è¯¢ DOI: {title[:50]}...")

        if api not in self.doi_apis or not self.doi_apis[api].get('enabled'):
            return {"doi": None, "error": f"API æœåŠ¡ {api} æœªå¯ç”¨"}

        api_config = self.doi_apis[api]

        try:
            if api == 'crossref':
                return self._query_crossref(title, api_config)
            else:
                return {"doi": None, "error": f"ä¸æ”¯æŒçš„ API: {api}"}

        except Exception as e:
            self.logger.error(f"DOI æŸ¥è¯¢å‡ºé”™: {e}")
            return {"doi": None, "error": str(e)}

    def _query_crossref(self, title: str, api_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨ CrossRef API æŸ¥è¯¢ DOI

        Args:
            title: è®ºæ–‡æ ‡é¢˜
            api_config: API é…ç½®

        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        url = api_config['url']
        timeout = api_config.get('timeout', 15)

        headers = {
            'User-Agent': 'PubMiner/1.0 (https://github.com/pubminer; mailto:contact@example.com)',
            'Accept': 'application/json'
        }

        params = {"query.bibliographic": title, "rows": 5, "sort": "score", "order": "desc"}

        try:
            # ä½¿ç”¨ API ç®¡ç†å™¨è¿›è¡Œé™æµ
            response = api_manager.get(url, headers=headers, params=params, timeout=timeout, api_name='crossref')

            response.raise_for_status()
            data = response.json()

            items = data.get("message", {}).get("items", [])
            if not items:
                self.logger.warning(f"CrossRef API æœªæ‰¾åˆ°ç»“æœ: {title}")
                return {"doi": None, "error": "æœªæ‰¾åˆ°ç»“æœ"}

            # æŸ¥æ‰¾æœ€ä½³åŒ¹é…
            best_match = None
            best_score = 0

            for item in items:
                item_title_list = item.get("title")
                if not item_title_list:
                    continue

                item_title = item_title_list[0]
                score = self._calculate_similarity_score(title, item_title)

                # ä½¿ç”¨è¾ƒä¸¥æ ¼çš„é˜ˆå€¼ç¡®ä¿åŒ¹é…è´¨é‡
                if score > best_score and score > 0.8:
                    best_score = score
                    best_match = {
                        "doi":
                        item.get("DOI", ""),
                        "title":
                        item_title,
                        "score":
                        score,
                        "publisher":
                        item.get("publisher", ""),
                        "type":
                        item.get("type", ""),
                        "journal": (item.get("container-title") or [""])[0],
                        "authors":
                        item.get("author", []),
                        "published":
                        item.get("published-print", {}).get("date-parts", [[]])[0] if item.get("published-print") else [],
                        "url":
                        item.get("URL", "")
                    }

            if best_match:
                self.logger.info(f"âœ… æ‰¾åˆ°æœ€ä½³ DOI åŒ¹é…: {best_match['doi']} (ç›¸ä¼¼åº¦: {best_score:.2f})")
                return best_match
            else:
                self.logger.warning(f"æœªæ‰¾åˆ°é«˜ç½®ä¿¡åº¦çš„ DOI åŒ¹é…: {title}")
                return {"doi": None, "error": "æœªæ‰¾åˆ°é«˜ç½®ä¿¡åº¦åŒ¹é…"}

        except requests.exceptions.RequestException as e:
            self.logger.error(f"CrossRef API ç½‘ç»œé”™è¯¯: {e}")
            return {"doi": None, "error": f"ç½‘ç»œé”™è¯¯: {e}"}
        except Exception as e:
            self.logger.error(f"CrossRef API æŸ¥è¯¢å¼‚å¸¸: {e}")
            return {"doi": None, "error": f"æŸ¥è¯¢å¼‚å¸¸: {e}"}

    def query_doi_batch(self, titles: List[str], max_workers: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡æŸ¥è¯¢ DOI

        Args:
            titles: æ ‡é¢˜åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°

        Returns:
            DOI æŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        max_workers = max_workers or min(self.max_workers, len(titles))

        self.logger.info(f"ğŸ“š å¼€å§‹æ‰¹é‡ DOI æŸ¥è¯¢ï¼Œå…± {len(titles)} ä¸ªæ ‡é¢˜")

        results = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_title = {executor.submit(self.query_doi_by_title, title): title for title in titles}

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_title):
                title = future_to_title[future]
                try:
                    result = future.result()
                    result['query_title'] = title
                    results.append(result)
                except Exception as e:
                    self.logger.error(f"æ‰¹é‡ DOI æŸ¥è¯¢å¤±è´¥: {title} - {e}")
                    results.append({"doi": None, "error": str(e), "query_title": title})

        successful = len([r for r in results if r.get('doi')])
        self.logger.info(f"âœ… æ‰¹é‡ DOI æŸ¥è¯¢å®Œæˆ: {successful}/{len(titles)} æˆåŠŸ")

        return results

    def download_by_doi(self, doi: str, title: Optional[str] = None, output_dir: Optional[Path] = None) -> Dict[str, Any]:
        """
        é€šè¿‡ DOI ä¸‹è½½ PDF æ–‡ä»¶

        Args:
            doi: DOI æ ‡è¯†ç¬¦
            title: è®ºæ–‡æ ‡é¢˜ï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            ä¸‹è½½ç»“æœå­—å…¸
        """
        self.stats['total_downloads'] += 1

        output_dir = output_dir or self.download_dir
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ£€æŸ¥ PMC å’Œ SciHub ä¸¤ç§å‘½åï¼‰
        safe_doi = doi.replace('/', '_').replace('\\', '_')
        pmc_filename = f"{safe_doi}_PMC.pdf"
        scihub_filename = f"{safe_doi}_SciHub.pdf"

        pmc_path = output_dir / pmc_filename
        scihub_path = output_dir / scihub_filename

        # SciHub ä¸‹è½½ä½¿ç”¨çš„æ–‡ä»¶è·¯å¾„
        output_path = scihub_path

        if pmc_path.exists() and self._validate_pdf_file(pmc_path):
            file_size = pmc_path.stat().st_size
            self.logger.info(f"âœ… PMC æ–‡ä»¶å·²å­˜åœ¨: {pmc_filename} ({file_size} bytes)")
            return {
                'success': True,
                'doi': doi,
                'title': title,
                'local_path': str(pmc_path),
                'file_size': file_size,
                'status': 'already_exists',
                'source': 'PMC',
                'error': None
            }

        if scihub_path.exists() and self._validate_pdf_file(scihub_path):
            file_size = scihub_path.stat().st_size
            self.logger.info(f"âœ… SciHub æ–‡ä»¶å·²å­˜åœ¨: {scihub_filename} ({file_size} bytes)")
            return {
                'success': True,
                'doi': doi,
                'title': title,
                'local_path': str(scihub_path),
                'file_size': file_size,
                'status': 'already_exists',
                'source': 'SciHub',
                'error': None
            }

        # é¦–å…ˆæ£€æŸ¥å¼€æ”¾è·å–çŠ¶æ€
        self.logger.info(f"æ£€æŸ¥å¼€æ”¾è·å–çŠ¶æ€: {doi}")
        oa_status = self.check_open_access_status(doi)

        # å¦‚æœæœ‰ PMC IDï¼Œä¼˜å…ˆå°è¯• PMC ä¸‹è½½ï¼ˆåªå°è¯•ä¸€æ¬¡ï¼‰
        if oa_status.get('pmc_id'):
            self.logger.info(f"å‘ç° PMC ID: PMC{oa_status['pmc_id']}ï¼Œå°è¯• PMC ä¸‹è½½")
            pmc_success, pmc_path, pmc_error = self.download_from_pmc(oa_status['pmc_id'], doi)

            if pmc_success and pmc_path:
                file_size = pmc_path.stat().st_size
                self.stats['successful_downloads'] += 1

                return {
                    'success': True,
                    'doi': doi,
                    'title': title,
                    'local_path': str(pmc_path),
                    'file_size': file_size,
                    'status': 'downloaded_from_pmc',
                    'source': 'PMC',
                    'pmc_id': oa_status['pmc_id'],
                    'is_open_access': oa_status['is_open_access'],
                    'error': None
                }
            else:
                self.logger.warning(f"PMC ä¸‹è½½å¤±è´¥: {pmc_error}")
                self.logger.info("è½¬ä¸º SciHub ä¸‹è½½ç­–ç•¥")

        # å°è¯• SciHub ä¸‹è½½ï¼ˆå¸¦é‡è¯•ï¼‰
        for attempt in range(self.max_retries):
            try:
                self.logger.info(f"ğŸ“¥ SciHub ä¸‹è½½ (å°è¯• {attempt + 1}/{self.max_retries}): {doi}")

                # ä½¿ç”¨ SciHub ä¸‹è½½
                success, error = self.scihub.download_by_doi(doi, output_path, delay=self.retry_delay)

                if success and self._validate_pdf_file(output_path):
                    file_size = output_path.stat().st_size
                    self.stats['successful_downloads'] += 1

                    return {
                        'success': True,
                        'doi': doi,
                        'title': title,
                        'local_path': str(output_path),
                        'file_size': file_size,
                        'status': 'downloaded_from_scihub',
                        'source': 'SciHub',
                        'is_open_access': oa_status['is_open_access'],
                        'pmc_id': oa_status.get('pmc_id'),
                        'error': None,
                        'attempts': attempt + 1
                    }
                else:
                    self.logger.warning(f"ä¸‹è½½å¤±è´¥ (å°è¯• {attempt + 1}): {error}")
                    self.stats['retries'] += 1

                    if attempt < self.max_retries - 1:
                        time.sleep(self.retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿

            except Exception as e:
                self.logger.error(f"ä¸‹è½½å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")
                self.stats['retries'] += 1

                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay * (attempt + 1))

        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        self.stats['failed_downloads'] += 1

        return {
            'success': False,
            'doi': doi,
            'title': title,
            'local_path': None,
            'file_size': 0,
            'status': 'failed',
            'error': f"åœ¨ {self.max_retries} æ¬¡å°è¯•åä¸‹è½½å¤±è´¥",
            'attempts': self.max_retries
        }

    def download_by_pmid(self, pmid: str, title: Optional[str] = None, output_dir: Optional[Path] = None) -> Dict[str, Any]:
        """
        é€šè¿‡ PMID ä¸‹è½½ PDF æ–‡ä»¶ï¼ˆå…ˆæŸ¥è¯¢ DOI å†ä¸‹è½½ï¼‰

        Args:
            pmid: PMID æ ‡è¯†ç¬¦
            title: è®ºæ–‡æ ‡é¢˜
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            ä¸‹è½½ç»“æœå­—å…¸
        """
        # å¦‚æœæ²¡æœ‰æä¾›æ ‡é¢˜ï¼Œå°è¯•ä»å…¶ä»–åœ°æ–¹è·å–
        if not title:
            title = f"PMID_{pmid}"

        # é¦–å…ˆæŸ¥è¯¢ DOI
        doi_result = self.query_doi_by_title(title)

        if doi_result.get('doi'):
            doi = doi_result['doi']
            self.logger.info(f"âœ… é€šè¿‡æ ‡é¢˜æ‰¾åˆ° DOI: {doi}")

            # ä½¿ç”¨æ‰¾åˆ°çš„ DOI ä¸‹è½½
            result = self.download_by_doi(doi, title, output_dir)
            result['pmid'] = pmid
            result['doi_source'] = 'title_query'
            return result
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ° DOIï¼Œå°è¯•ç›´æ¥ä½¿ç”¨ PMID æ„é€ æ–‡ä»¶å
            self.logger.warning(f"æœªæ‰¾åˆ° DOIï¼Œå°è¯•å…¶ä»–æ–¹å¼: PMID {pmid}")

            output_dir = output_dir or self.download_dir
            filename = self._clean_filename(title, pmid=pmid)

            return {
                'success': False,
                'pmid': pmid,
                'doi': None,
                'title': title,
                'local_path': None,
                'file_size': 0,
                'status': 'no_doi_found',
                'error': f"æ— æ³•æ‰¾åˆ° PMID {pmid} å¯¹åº”çš„ DOI",
                'doi_query_error': doi_result.get('error')
            }

    def download_with_fallback(self, doi: Optional[str], title: str, output_dir: Optional[Path] = None) -> Dict[str, Any]:
        """
        å¸¦å›é€€æœºåˆ¶çš„ä¸‹è½½ï¼ˆå‚è€ƒ RecursiveScholarCrawler çš„é€»è¾‘ï¼‰

        Args:
            doi: DOI æ ‡è¯†ç¬¦ï¼ˆå¯é€‰ï¼‰
            title: è®ºæ–‡æ ‡é¢˜
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            ä¸‹è½½ç»“æœå­—å…¸
        """
        # æ­¥éª¤ 1ï¼šå¦‚æœæä¾›äº† DOIï¼Œå…ˆå°è¯•ä½¿ç”¨å®ƒ
        if doi:
            self.logger.info(f"ğŸ¯ ä½¿ç”¨æä¾›çš„ DOI ä¸‹è½½: {doi}")
            result = self.download_by_doi(doi, title, output_dir)
            if result['success']:
                result['download_method'] = 'provided_doi'
                return result
            else:
                self.logger.warning(f"æä¾›çš„ DOI ä¸‹è½½å¤±è´¥: {result.get('error')}")

        # æ­¥éª¤ 2ï¼šå¦‚æœæ²¡æœ‰ DOI æˆ– DOI ä¸‹è½½å¤±è´¥ï¼Œé€šè¿‡æ ‡é¢˜æŸ¥è¯¢æ–°çš„ DOI
        if not title:
            return {
                'success': False,
                'doi': doi,
                'title': title,
                'local_path': None,
                'file_size': 0,
                'status': 'no_title_for_doi_search',
                'error': "æ²¡æœ‰ DOI ä¸”æ²¡æœ‰æ ‡é¢˜ï¼Œæ— æ³•ç»§ç»­",
                'download_method': 'failed'
            }

        self.logger.info(f"ğŸ” é€šè¿‡æ ‡é¢˜æŸ¥è¯¢æ–°çš„ DOI: {title[:70]}...")
        doi_result = self.query_doi_by_title(title)

        new_doi = doi_result.get("doi")
        if not new_doi:
            error_msg = f"æ— æ³•æ‰¾åˆ°æ ‡é¢˜å¯¹åº”çš„ DOI: {doi_result.get('error')}"
            self.logger.error(error_msg)
            return {
                'success': False,
                'doi': doi,
                'title': title,
                'local_path': None,
                'file_size': 0,
                'status': 'doi_not_found',
                'error': error_msg,
                'download_method': 'failed'
            }

        # é¿å…é‡å¤ä¸‹è½½ç›¸åŒçš„ DOI
        if new_doi == doi:
            error_msg = f"æŸ¥è¯¢åˆ°çš„ DOI ä¸å¤±è´¥çš„ DOI ç›¸åŒ: {new_doi}"
            self.logger.warning(error_msg)
            return {
                'success': False,
                'doi': doi,
                'title': title,
                'local_path': None,
                'file_size': 0,
                'status': 'same_doi_failed',
                'error': error_msg,
                'download_method': 'failed'
            }

        # æ­¥éª¤ 3ï¼šä½¿ç”¨æ–°æ‰¾åˆ°çš„ DOI ä¸‹è½½
        self.logger.info(f"âœ¨ æ‰¾åˆ°æ–°çš„ DOIï¼Œå¼€å§‹ä¸‹è½½: {new_doi}")
        result = self.download_by_doi(new_doi, title, output_dir)

        if result['success']:
            result['download_method'] = 'title_resolved_doi'
            result['original_doi'] = doi
            result['resolved_doi'] = new_doi
        else:
            result['download_method'] = 'all_methods_failed'
            result['doi_query_result'] = doi_result

        return result

    def batch_download(self,
                       items: List[Dict[str, Any]],
                       max_workers: Optional[int] = None,
                       output_dir: Optional[Path] = None) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡ä¸‹è½½ PDF æ–‡ä»¶

        Args:
            items: ä¸‹è½½é¡¹ç›®åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«'doi', 'title', 'pmid' ç­‰å­—æ®µ
            max_workers: æœ€å¤§å¹¶å‘æ•°
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            ä¸‹è½½ç»“æœåˆ—è¡¨
        """
        max_workers = max_workers or min(self.max_workers, len(items))
        output_dir = output_dir or self.download_dir

        self.logger.info(f"ğŸ“¦ å¼€å§‹æ‰¹é‡ä¸‹è½½ï¼Œå…± {len(items)} é¡¹ï¼Œå¹¶å‘æ•°: {max_workers}")

        results = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä¸‹è½½ä»»åŠ¡
            future_to_item = {}

            for item in items:
                doi = item.get('doi')
                title = item.get('title', item.get('Title', ''))
                pmid = item.get('pmid', item.get('PMID', ''))

                # é€‰æ‹©ä¸‹è½½æ–¹æ³•
                if doi and title:
                    future = executor.submit(self.download_with_fallback, doi, title, output_dir)
                elif pmid and title:
                    future = executor.submit(self.download_by_pmid, pmid, title, output_dir)
                elif doi:
                    future = executor.submit(self.download_by_doi, doi, title, output_dir)
                else:
                    # æ— æ³•ä¸‹è½½çš„é¡¹ç›®
                    results.append({
                        'success': False,
                        'doi': doi,
                        'pmid': pmid,
                        'title': title,
                        'local_path': None,
                        'file_size': 0,
                        'status': 'insufficient_info',
                        'error': 'ç¼ºå°‘ DOIã€PMID æˆ–æ ‡é¢˜ä¿¡æ¯'
                    })
                    continue

                future_to_item[future] = item

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_item):
                item = future_to_item[future]
                try:
                    result = future.result()
                    result['original_item'] = item
                    results.append(result)
                except Exception as e:
                    self.logger.error(f"æ‰¹é‡ä¸‹è½½ä»»åŠ¡å¼‚å¸¸: {e}")
                    results.append({
                        'success': False,
                        'doi': item.get('doi'),
                        'pmid': item.get('pmid'),
                        'title': item.get('title'),
                        'local_path': None,
                        'file_size': 0,
                        'status': 'exception',
                        'error': str(e),
                        'original_item': item
                    })

        # ç»Ÿè®¡ç»“æœ
        successful = len([r for r in results if r.get('success')])
        self.logger.info(f"âœ… æ‰¹é‡ä¸‹è½½å®Œæˆ: {successful}/{len(results)} æˆåŠŸ")

        return results

    def retry_failed_downloads(self,
                               failed_results: List[Dict[str, Any]],
                               max_retries: Optional[int] = None) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        é‡è¯•å¤±è´¥çš„ä¸‹è½½

        Args:
            failed_results: å¤±è´¥çš„ä¸‹è½½ç»“æœåˆ—è¡¨
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            (ä»ç„¶å¤±è´¥çš„ç»“æœ, é‡è¯•æˆåŠŸçš„ç»“æœ)
        """
        max_retries = max_retries or self.max_retries

        if not failed_results:
            self.logger.info("æ²¡æœ‰å¤±è´¥çš„ä¸‹è½½éœ€è¦é‡è¯•")
            return [], []

        self.logger.info(f"ğŸ”„ å¼€å§‹é‡è¯• {len(failed_results)} ä¸ªå¤±è´¥çš„ä¸‹è½½")

        still_failed = []
        newly_successful = []

        for i, result in enumerate(failed_results):
            doi = result.get('doi')
            title = result.get('title')
            pmid = result.get('pmid')

            retry_count = result.get('retry_count', 0) + 1

            if retry_count > max_retries:
                self.logger.warning(f"è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡: {title or doi or pmid}")
                result['retry_count'] = retry_count
                still_failed.append(result)
                continue

            self.logger.info(f"é‡è¯• {retry_count}/{max_retries} [{i+1}/{len(failed_results)}]: {title or doi or pmid}")

            # é€‰æ‹©é‡è¯•æ–¹æ³•
            if doi and title:
                retry_result = self.download_with_fallback(doi, title)
            elif pmid and title:
                retry_result = self.download_by_pmid(pmid, title)
            elif doi:
                retry_result = self.download_by_doi(doi, title)
            else:
                retry_result = {'success': False, 'error': 'ç¼ºå°‘é‡è¯•æ‰€éœ€çš„ä¿¡æ¯'}

            # æ›´æ–°ç»“æœ
            retry_result.update({
                'retry_count': retry_count,
                'retry_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'original_error': result.get('error')
            })

            if retry_result['success']:
                self.logger.info(f"âœ… é‡è¯•æˆåŠŸ: {title or doi or pmid}")
                newly_successful.append(retry_result)
            else:
                self.logger.warning(f"âŒ é‡è¯•ä»ç„¶å¤±è´¥: {retry_result.get('error')}")
                still_failed.append(retry_result)

            # é‡è¯•é—´éš”
            if i < len(failed_results) - 1:
                time.sleep(self.retry_delay)

        self.logger.info(f"ğŸ”„ é‡è¯•å®Œæˆ: {len(newly_successful)} æˆåŠŸ, {len(still_failed)} ä»ç„¶å¤±è´¥")

        return still_failed, newly_successful

    def _download_with_playwright(self, pmc_id: str, doi: str = None) -> Tuple[bool, Optional[Path]]:
        """
        ä½¿ç”¨ Playwright ä¸‹è½½ PDF

        Args:
            pmc_id: PMC ID
            doi: DOI (å¯é€‰ï¼Œç”¨äºæ–‡ä»¶å‘½å)

        Returns:
            (æˆåŠŸæ ‡å¿—, æ–‡ä»¶è·¯å¾„)
        """
        try:
            from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout
            import re
        except ImportError:
            self.logger.debug("Playwright æœªå®‰è£…ï¼Œè·³è¿‡ Playwright ç­–ç•¥")
            raise ImportError("Playwright not available")

        pmcid = f"PMC{pmc_id}"
        host = "pmc.ncbi.nlm.nih.gov"
        article_url = f"https://{host}/articles/{pmcid}/"
        pdf_url = f"https://{host}/articles/{pmcid}/pdf"

        # ç”Ÿæˆæ–‡ä»¶å
        if doi:
            safe_doi = doi.replace('/', '_').replace('\\', '_')
            filename = f"{safe_doi}_PMC_Playwright.pdf"
        else:
            filename = f"pmc_{pmc_id}_PMC_Playwright.pdf"

        output_path = self.download_dir / filename

        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            try:
                ctx = browser.new_context()
                page = ctx.new_page()

                # è¿›å…¥æ–‡ç« é¡µ
                page.goto(article_url, wait_until="domcontentloaded")

                # --- 1) ä¼˜å…ˆï¼šCSS ç²¾ç¡®å®šä½æ­£æ–‡ PDF æŒ‰é’® ---
                pdf_link = page.locator("a[href$='/pdf/']").first
                if not pdf_link.count():
                    pdf_link = page.locator("a[href$='/pdf']").first

                # --- 2) å›é€€ï¼šARIA åç§°ä»¥ PDF å¼€å¤´çš„é“¾æ¥ï¼ˆæ’é™¤è¡¥å……ææ–™çš„æ–‡ä»¶åï¼‰---
                if not pdf_link.count():
                    # ä¾‹å¦‚ "PDF (2.4 MB)"
                    pdf_link = page.get_by_role("link", name=re.compile(r"^PDF\\b", re.I)).first

                # --- 3) å…œåº•ï¼šç›´æ¥æŸ¥æ‰¾åŒ…å« PDF çš„é“¾æ¥ï¼Œæ’é™¤ tooltip ---
                if not pdf_link.count():
                    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å« "PDF" çš„å¯è§é“¾æ¥
                    pdf_links = page.locator("a:has-text('PDF')").filter(has_not_text="tooltip").first
                    if pdf_links.count() > 0:
                        pdf_link = pdf_links

                if not pdf_link.count():
                    # æœ€åå°è¯•ï¼šæŸ¥æ‰¾å®é™…çš„é“¾æ¥å…ƒç´ ï¼Œæ’é™¤ tooltip span
                    all_links = page.locator("a[href*='pdf'], a[href$='pdf/'], a[href$='.pdf']").first
                    if all_links.count() > 0:
                        pdf_link = all_links

                if not pdf_link.count():
                    self.logger.warning("æ‰¾ä¸åˆ°æ­£æ–‡ PDF æŒ‰é’®ï¼›é¡µé¢ç»“æ„å¯èƒ½å˜åŒ–ã€‚")
                    return False, None

                # æœ‰äº›ç«™ç‚¹æŠŠ PDF é“¾æ¥è®¾ä¸º target=_blankï¼Œè¿™é‡ŒåŒæ—¶ç›‘å¬å¯èƒ½çš„ popup
                popup = None
                try:
                    with page.expect_popup(timeout=2000) as pop_ctx:
                        pdf_link.click(timeout=10000)
                    popup = pop_ctx.value
                except PWTimeout:
                    # æ²¡æœ‰æ–°æ ‡ç­¾ï¼Œå°±åœ¨å½“å‰é¡µ
                    pdf_link.click(timeout=10000)

                # ç­‰æ ¡éªŒè„šæœ¬è·‘å®Œï¼ˆç»™å¾—ç¨å¾®å®½è£•ä¸€ç‚¹ï¼‰
                page.wait_for_load_state("networkidle")
                page.wait_for_timeout(2000)

                # ç”¨ä¸é¡µé¢åŒä¸€ä¼šè¯çš„ request å®¢æˆ·ç«¯è·å– PDF
                # ä¸ç›´æ¥ç”¨ popup çš„ contentï¼Œå› ä¸ºæœ‰æ—¶æ˜¯ä¸­é—´é¡µ / å‡†å¤‡é¡µ
                resp = ctx.request.get(pdf_url)
                if resp.ok and resp.headers.get("content-type", "").startswith("application/pdf"):
                    with open(output_path, "wb") as f:
                        f.write(resp.body())
                    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    file_size = output_path.stat().st_size
                    self.stats['total_size'] += file_size
                    self.logger.info(f"âœ… Playwright ä¸‹è½½æˆåŠŸ: é€šè¿‡ ctx.request ({file_size/1024:.1f}KB)")
                    return True, output_path
                else:
                    # å†å°è¯•ç›´æ¥ä»å½“å‰ DOM è¯» hrefï¼ˆæœ‰æ—¶å¸¦ query çš„çœŸå® pdf åœ°å€ï¼‰
                    try:
                        href = pdf_link.get_attribute("href")
                        if href and not href.startswith("http"):
                            href = f"https://{host}{href}"
                        if href:
                            r2 = ctx.request.get(href)
                            if r2.ok and r2.headers.get("content-type", "").startswith("application/pdf"):
                                with open(output_path, "wb") as f:
                                    f.write(r2.body())
                                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                                file_size = output_path.stat().st_size
                                self.stats['total_size'] += file_size
                                self.logger.info(f"âœ… Playwright ä¸‹è½½æˆåŠŸ: é€šè¿‡ DOM href ({file_size/1024:.1f}KB)")
                                return True, output_path
                            else:
                                self.logger.warning("Playwright: DOM href not PDF")
                        else:
                            self.logger.warning("Playwright: no href")
                    except Exception as e:
                        self.logger.warning(f"Playwright: exception reading href: {e}")

                return False, None

            finally:
                browser.close()

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œå…³é—­ä¼šè¯"""
        if hasattr(self, 'session'):
            self.session.close()
