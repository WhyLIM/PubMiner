# -*- coding: utf-8 -*-
"""
PDFä¸‹è½½æ¨¡å—

è´Ÿè´£ä»å¤šä¸ªæºä¸‹è½½æ–‡çŒ®PDFæ–‡ä»¶ï¼ŒåŒ…æ‹¬DOIæŸ¥è¯¢ã€SciHubä¸‹è½½ã€æ–‡ä»¶ç®¡ç†ç­‰åŠŸèƒ½
åŸºäºRecursiveScholarCrawleré¡¹ç›®çš„ä¸‹è½½åŠŸèƒ½è¿›è¡Œä¼˜åŒ–å’Œé›†æˆ
"""

import os
import re
import time
import random
import hashlib
import requests
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from urllib.parse import urljoin, quote_plus
from bs4 import BeautifulSoup
import urllib.parse

from utils.logger import LoggerMixin
from utils.file_handler import FileHandler
from utils.api_manager import api_manager
from .scihub_downloader import SciHubDownloader

logger = logging.getLogger(__name__)

class PDFDownloader(LoggerMixin):
    """PDFä¸‹è½½å™¨ - æ”¯æŒå¤šæºä¸‹è½½ã€DOIæŸ¥è¯¢ã€æ–‡ä»¶ç®¡ç†"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–PDFä¸‹è½½å™¨
        
        Args:
            config: ä¸‹è½½é…ç½®
        """
        self.config = config
        self.download_dir = Path(config.get('download_dir', './results/pdfs'))
        self.max_retries = config.get('max_retries', 3)
        self.retry_delay = config.get('retry_delay', 5)
        self.timeout = config.get('timeout', 30)
        self.max_workers = config.get('max_workers', 4)
        self.verify_pdf = config.get('verify_pdf', True)
        self.max_file_size = config.get('max_file_size', 100 * 1024 * 1024)  # 100MB
        
        # SciHubé•œåƒé…ç½®
        self.scihub_mirrors = config.get('scihub_mirrors', [
            "https://sci-hub.se",
            "https://sci-hub.st", 
            "https://sci-hub.ru",
            "https://www.sci-hub.ren",
            "https://www.sci-hub.ee"
        ])
        
        # ç”¨æˆ·ä»£ç†é…ç½®
        self.user_agents = config.get('user_agents', [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        ])
        
        # DOI APIé…ç½®
        self.doi_apis = config.get('doi_apis', {
            'crossref': {
                'url': 'https://api.crossref.org/works',
                'enabled': True,
                'timeout': 15
            }
        })
        
        # åˆ›å»ºä¸‹è½½ç›®å½•
        self.download_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ä¼šè¯
        self.session = requests.Session()
        self._setup_session()
        
        # åˆå§‹åŒ–SciHubä¸‹è½½å™¨
        self.scihub = SciHubDownloader(
            mirrors=self.scihub_mirrors,
            user_agents=self.user_agents,
            timeout=self.timeout,
            max_retries=self.max_retries
        )
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_downloads': 0,
            'successful_downloads': 0,
            'failed_downloads': 0,
            'retries': 0,
            'total_size': 0
        }
        
        self.logger.info(f"âœ… PDF ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆï¼Œä¸‹è½½ç›®å½•: {self.download_dir}")
    
    def _setup_session(self):
        """è®¾ç½®HTTPä¼šè¯"""
        self.session.headers.update({
            'User-Agent': self._get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Referer': 'https://www.google.com/'
        })
    
    def _get_random_user_agent(self) -> str:
        """è·å–éšæœºç”¨æˆ·ä»£ç†"""
        return random.choice(self.user_agents)
    
    def _get_random_mirrors(self, exclude: Optional[List[str]] = None, count: Optional[int] = None) -> List[str]:
        """
        è·å–éšæœºæ’åºçš„é•œåƒåˆ—è¡¨
        
        Args:
            exclude: æ’é™¤çš„é•œåƒåˆ—è¡¨
            count: è¿”å›çš„é•œåƒæ•°é‡
            
        Returns:
            é•œåƒåˆ—è¡¨
        """
        available = list(set(self.scihub_mirrors))  # å»é‡
        if exclude:
            available = [m for m in available if m not in exclude]
        random.shuffle(available)
        if count and count < len(available):
            return available[:count]
        return available
    
    def _clean_filename(self, title: str, doi: Optional[str] = None, pmid: Optional[str] = None) -> str:
        """
        æ¸…ç†æ–‡ä»¶å
        
        Args:
            title: è®ºæ–‡æ ‡é¢˜
            doi: DOI æ ‡è¯†ç¬¦
            pmid: PMID æ ‡è¯†ç¬¦
            
        Returns:
            æ¸…ç†åçš„æ–‡ä»¶å
        """
        if title:
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œæˆªæ–­é•¿åº¦
            cleaned = re.sub(r'[\\/*?:"<>|]', "", title).strip()[:100].replace(" ", "_")
        else:
            cleaned = "unknown_paper"
        
        # æ·»åŠ æ ‡è¯†ç¬¦
        if doi:
            cleaned_doi = doi.replace("/", "_").replace(".", "-")
            return f"{cleaned}_{cleaned_doi}.pdf"
        elif pmid:
            return f"{cleaned}_PMID{pmid}.pdf"
        else:
            return f"{cleaned}.pdf"
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """
        è®¡ç®—æ–‡ä»¶ MD5 å“ˆå¸Œå€¼
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            MD5 å“ˆå¸Œå€¼
        """
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            self.logger.warning(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼å¤±è´¥: {e}")
            return ""
    
    def _validate_pdf_file(self, file_path: Path) -> bool:
        """
        éªŒè¯ PDF æ–‡ä»¶æœ‰æ•ˆæ€§
        
        Args:
            file_path: PDF æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ PDF æ–‡ä»¶
        """
        if not file_path.exists():
            return False
        
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            if file_size < 1024:  # å°äº 1KB å¯èƒ½ä¸æ˜¯æœ‰æ•ˆ PDF
                self.logger.warning(f"PDF æ–‡ä»¶è¿‡å°: {file_size} bytes")
                return False
            
            if file_size > self.max_file_size:
                self.logger.warning(f"PDF æ–‡ä»¶è¿‡å¤§: {file_size} bytes")
                return False
            
            # æ£€æŸ¥PDFæ–‡ä»¶å¤´
            with open(file_path, 'rb') as f:
                header = f.read(8)
                if not header.startswith(b'%PDF'):
                    self.logger.warning("æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ PDF æ ¼å¼")
                    return False
            
            # å¦‚æœå¯ç”¨äº† PDF éªŒè¯ï¼Œä½¿ç”¨ PyMuPDF éªŒè¯
            if self.verify_pdf:
                try:
                    import fitz  # PyMuPDF
                    with fitz.open(str(file_path)) as doc:
                        if doc.page_count > 0:
                            self.logger.debug(f"âœ… PDFéªŒè¯æˆåŠŸ: {doc.page_count} é¡µ")
                            return True
                        else:
                            self.logger.warning("PDF æ–‡ä»¶æ²¡æœ‰é¡µé¢å†…å®¹")
                            return False
                except ImportError:
                    self.logger.warning("PyMuPDF æœªå®‰è£…ï¼Œè·³è¿‡ PDF ç»“æ„éªŒè¯")
                    return True  # åªè¿›è¡ŒåŸºæœ¬éªŒè¯
                except Exception as e:
                    self.logger.warning(f"PDF ç»“æ„éªŒè¯å¤±è´¥: {e}")
                    return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"PDF æ–‡ä»¶éªŒè¯å‡ºé”™: {e}")
            return False
    
    def _find_pdf_link_in_html(self, html_content: str, base_url: str) -> Optional[str]:
        """
        ä» HTML å†…å®¹ä¸­æŸ¥æ‰¾ PDF ä¸‹è½½é“¾æ¥
        
        Args:
            html_content: HTML å†…å®¹
            base_url: åŸºç¡€ URL
            
        Returns:
            PDF ä¸‹è½½é“¾æ¥æˆ– None
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æŸ¥æ‰¾ embed å’Œ iframe æ ‡ç­¾
            for tag in soup.find_all(['embed', 'iframe']):
                src = tag.get('src')
                if src:
                    if src.startswith('//'):
                        return f"https:{src}"
                    if not src.startswith('http'):
                        return urljoin(base_url, src)
                    return src
            
            # æŸ¥æ‰¾ PDF ä¸‹è½½é“¾æ¥
            for link in soup.find_all('a', href=True):
                href = link['href']
                if ('pdf' in href.lower() or 
                    link.get('id') == 'download' or
                    'download' in link.get('class', [])):
                    if href.startswith('//'):
                        return f"https:{href}"
                    if not href.startswith('http'):
                        return urljoin(base_url, href)
                    return href
            
            return None
            
        except Exception as e:
            self.logger.error(f"è§£æ HTML æŸ¥æ‰¾ PDF é“¾æ¥æ—¶å‡ºé”™: {e}")
            return None
    
    def _download_file_with_progress(self, url: str, output_path: Path, 
                                   timeout: Optional[int] = None) -> Tuple[bool, Optional[str]]:
        """
        ä¸‹è½½æ–‡ä»¶å¹¶æ˜¾ç¤ºè¿›åº¦
        
        Args:
            url: ä¸‹è½½ URL
            output_path: è¾“å‡ºè·¯å¾„
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            (æ˜¯å¦æˆåŠŸ, é”™è¯¯ä¿¡æ¯)
        """
        try:
            timeout = timeout or self.timeout
            
            # å‘é€HEADè¯·æ±‚è·å–æ–‡ä»¶å¤§å°
            head_response = self.session.head(url, timeout=timeout)
            total_size = int(head_response.headers.get('content-length', 0))
            
            # ä¸‹è½½æ–‡ä»¶
            response = self.session.get(url, timeout=timeout, stream=True)
            response.raise_for_status()
            
            downloaded_size = 0
            
            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)
            
            # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
            if self._validate_pdf_file(output_path):
                file_size = output_path.stat().st_size
                self.stats['total_size'] += file_size
                self.logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {output_path.name} ({file_size} bytes)")
                return True, None
            else:
                # åˆ é™¤æ— æ•ˆæ–‡ä»¶
                if output_path.exists():
                    output_path.unlink()
                return False, "ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ PDF"
                
        except requests.exceptions.Timeout:
            return False, f"ä¸‹è½½è¶…æ—¶ ({timeout} ç§’)"
        except requests.exceptions.RequestException as e:
            return False, f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}"
        except Exception as e:
            return False, f"ä¸‹è½½è¿‡ç¨‹é”™è¯¯: {e}"
    
    def get_download_stats(self) -> Dict[str, Any]:
        """
        è·å–ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        success_rate = 0
        if self.stats['total_downloads'] > 0:
            success_rate = (self.stats['successful_downloads'] / 
                          self.stats['total_downloads']) * 100
        
        return {
            **self.stats,
            'success_rate': round(success_rate, 2),
            'average_file_size': (self.stats['total_size'] / 
                                self.stats['successful_downloads'] 
                                if self.stats['successful_downloads'] > 0 else 0)
        }
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total_downloads': 0,
            'successful_downloads': 0,
            'failed_downloads': 0,
            'retries': 0,
            'total_size': 0
        }
        self.logger.info("ğŸ“Š ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
    
    def _create_download_directory(self) -> bool:
        """åˆ›å»ºä¸‹è½½ç›®å½•"""
        try:
            self.download_dir.mkdir(parents=True, exist_ok=True)
            return True
        except Exception as e:
            self.logger.error(f"åˆ›å»ºä¸‹è½½ç›®å½•å¤±è´¥: {e}")
            return False
    
    def _generate_safe_filename(self, doi: str, title: str = None) -> str:
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        # æ¸…ç† DOI ä½œä¸ºåŸºç¡€æ–‡ä»¶å
        safe_doi = doi.replace('/', '_').replace('\\', '_')
        safe_doi = ''.join(c for c in safe_doi if c.isalnum() or c in '._-')
        
        # å¦‚æœæœ‰æ ‡é¢˜ï¼Œæ·»åŠ åˆ°æ–‡ä»¶åä¸­
        if title:
            # æ¸…ç†æ ‡é¢˜
            safe_title = ''.join(c for c in title if c.isalnum() or c in ' ._-')
            safe_title = safe_title.replace(' ', '_')[:50]  # é™åˆ¶é•¿åº¦
            filename = f"{safe_doi}_{safe_title}.pdf"
        else:
            filename = f"{safe_doi}.pdf"
        
        return filename
    
    def _check_file_integrity(self, file_path: Path, expected_size: int = None) -> bool:
        """æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§"""
        try:
            if not file_path.exists():
                return False
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            if file_size == 0:
                return False
            
            # å¦‚æœæä¾›äº†æœŸæœ›å¤§å°ï¼Œè¿›è¡Œæ¯”è¾ƒ
            if expected_size and abs(file_size - expected_size) > 1024:  # å…è®¸1KBå·®å¼‚
                self.logger.warning(f"æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ› {expected_size}, å®é™… {file_size}")
            
            # éªŒè¯ PDF æ ¼å¼
            with open(file_path, 'rb') as f:
                content = f.read(1024)  # è¯»å–å‰ 1KB è¿›è¡Œå¿«é€ŸéªŒè¯
                if not content.startswith(b'%PDF-'):
                    return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§å¤±è´¥: {e}")
            return False
    
    def _handle_duplicate_file(self, file_path: Path) -> Path:
        """å¤„ç†é‡å¤æ–‡ä»¶å"""
        if not file_path.exists():
            return file_path
        
        # ç”Ÿæˆæ–°çš„æ–‡ä»¶å
        base_path = file_path.parent
        base_name = file_path.stem
        extension = file_path.suffix
        
        counter = 1
        while True:
            new_name = f"{base_name}_{counter}{extension}"
            new_path = base_path / new_name
            if not new_path.exists():
                return new_path
            counter += 1
            
            # é˜²æ­¢æ— é™å¾ªç¯
            if counter > 1000:
                import time
                timestamp = int(time.time())
                new_name = f"{base_name}_{timestamp}{extension}"
                return base_path / new_name

    def _normalize_title(self, title: str) -> str:
        """
        æ ‡å‡†åŒ–è®ºæ–‡æ ‡é¢˜ä»¥æé«˜åŒ¹é…å‡†ç¡®æ€§
        
        Args:
            title: åŸå§‹æ ‡é¢˜
            
        Returns:
            æ ‡å‡†åŒ–åçš„æ ‡é¢˜
        """
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œè½¬æ¢ä¸ºå°å†™ï¼Œåˆå¹¶ç©ºç™½å­—ç¬¦
        clean_title = re.sub(r'[^\w\s]', ' ', title)
        clean_title = ' '.join(clean_title.lower().split())
        return clean_title
    
    def _calculate_similarity_score(self, title1: str, title2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦åˆ†æ•°
        
        Args:
            title1: æ ‡é¢˜1
            title2: æ ‡é¢˜2
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        try:
            from difflib import SequenceMatcher
            normalized1 = self._normalize_title(title1)
            normalized2 = self._normalize_title(title2)
            return SequenceMatcher(None, normalized1, normalized2).ratio()
        except ImportError:
            # å¦‚æœæ²¡æœ‰difflibï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…
            normalized1 = self._normalize_title(title1)
            normalized2 = self._normalize_title(title2)
            if normalized1 == normalized2:
                return 1.0
            elif normalized1 in normalized2 or normalized2 in normalized1:
                return 0.8
            else:
                return 0.0
    
    def query_doi_by_title(self, title: str, api: str = 'crossref') -> Dict[str, Any]:
        """
        é€šè¿‡æ ‡é¢˜æŸ¥è¯¢ DOI ä¿¡æ¯
        
        Args:
            title: è®ºæ–‡æ ‡é¢˜
            api: ä½¿ç”¨çš„ API æœåŠ¡ ('crossref')
            
        Returns:
            DOI æŸ¥è¯¢ç»“æœå­—å…¸
        """
        self.logger.info(f"ğŸ” æŸ¥è¯¢ DOI: {title[:50]}...")
        
        if api not in self.doi_apis or not self.doi_apis[api].get('enabled'):
            return {"doi": None, "error": f"API æœåŠ¡ {api} æœªå¯ç”¨"}
        
        api_config = self.doi_apis[api]
        
        try:
            if api == 'crossref':
                return self._query_crossref(title, api_config)
            else:
                return {"doi": None, "error": f"ä¸æ”¯æŒçš„ API: {api}"}
                
        except Exception as e:
            self.logger.error(f"DOI æŸ¥è¯¢å‡ºé”™: {e}")
            return {"doi": None, "error": str(e)}
    
    def _query_crossref(self, title: str, api_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨ CrossRef API æŸ¥è¯¢ DOI
        
        Args:
            title: è®ºæ–‡æ ‡é¢˜
            api_config: API é…ç½®
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        url = api_config['url']
        timeout = api_config.get('timeout', 15)
        
        headers = {
            'User-Agent': 'PubMiner/1.0 (https://github.com/pubminer; mailto:contact@example.com)',
            'Accept': 'application/json'
        }
        
        params = {
            "query.bibliographic": title,
            "rows": 5,
            "sort": "score",
            "order": "desc"
        }
        
        try:
            # ä½¿ç”¨ API ç®¡ç†å™¨è¿›è¡Œé™æµ
            response = api_manager.get(
                url,
                headers=headers,
                params=params,
                timeout=timeout,
                api_name='crossref'
            )
            
            response.raise_for_status()
            data = response.json()
            
            items = data.get("message", {}).get("items", [])
            if not items:
                self.logger.warning(f"CrossRef API æœªæ‰¾åˆ°ç»“æœ: {title}")
                return {"doi": None, "error": "æœªæ‰¾åˆ°ç»“æœ"}
            
            # æŸ¥æ‰¾æœ€ä½³åŒ¹é…
            best_match = None
            best_score = 0
            
            for item in items:
                item_title_list = item.get("title")
                if not item_title_list:
                    continue
                
                item_title = item_title_list[0]
                score = self._calculate_similarity_score(title, item_title)
                
                # ä½¿ç”¨è¾ƒä¸¥æ ¼çš„é˜ˆå€¼ç¡®ä¿åŒ¹é…è´¨é‡
                if score > best_score and score > 0.8:
                    best_score = score
                    best_match = {
                        "doi": item.get("DOI", ""),
                        "title": item_title,
                        "score": score,
                        "publisher": item.get("publisher", ""),
                        "type": item.get("type", ""),
                        "journal": (item.get("container-title") or [""])[0],
                        "authors": item.get("author", []),
                        "published": item.get("published-print", {}).get("date-parts", [[]])[0] if item.get("published-print") else [],
                        "url": item.get("URL", "")
                    }
            
            if best_match:
                self.logger.info(f"âœ… æ‰¾åˆ°æœ€ä½³ DOI åŒ¹é…: {best_match['doi']} (ç›¸ä¼¼åº¦: {best_score:.2f})")
                return best_match
            else:
                self.logger.warning(f"æœªæ‰¾åˆ°é«˜ç½®ä¿¡åº¦çš„ DOI åŒ¹é…: {title}")
                return {"doi": None, "error": "æœªæ‰¾åˆ°é«˜ç½®ä¿¡åº¦åŒ¹é…"}
                
        except requests.exceptions.RequestException as e:
            self.logger.error(f"CrossRef API ç½‘ç»œé”™è¯¯: {e}")
            return {"doi": None, "error": f"ç½‘ç»œé”™è¯¯: {e}"}
        except Exception as e:
            self.logger.error(f"CrossRef API æŸ¥è¯¢å¼‚å¸¸: {e}")
            return {"doi": None, "error": f"æŸ¥è¯¢å¼‚å¸¸: {e}"}
    
    def query_doi_batch(self, titles: List[str], max_workers: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡æŸ¥è¯¢DOI
        
        Args:
            titles: æ ‡é¢˜åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            DOIæŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        max_workers = max_workers or min(self.max_workers, len(titles))
        
        self.logger.info(f"ğŸ“š å¼€å§‹æ‰¹é‡ DOI æŸ¥è¯¢ï¼Œå…± {len(titles)} ä¸ªæ ‡é¢˜")
        
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_title = {
                executor.submit(self.query_doi_by_title, title): title
                for title in titles
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_title):
                title = future_to_title[future]
                try:
                    result = future.result()
                    result['query_title'] = title
                    results.append(result)
                except Exception as e:
                    self.logger.error(f"æ‰¹é‡ DOI æŸ¥è¯¢å¤±è´¥: {title} - {e}")
                    results.append({
                        "doi": None,
                        "error": str(e),
                        "query_title": title
                    })
        
        successful = len([r for r in results if r.get('doi')])
        self.logger.info(f"âœ… æ‰¹é‡ DOI æŸ¥è¯¢å®Œæˆ: {successful}/{len(titles)} æˆåŠŸ")
        
        return results
    
    def download_by_doi(self, doi: str, title: Optional[str] = None, 
                       output_dir: Optional[Path] = None) -> Dict[str, Any]:
        """
        é€šè¿‡ DOI ä¸‹è½½ PDF æ–‡ä»¶
        
        Args:
            doi: DOI æ ‡è¯†ç¬¦
            title: è®ºæ–‡æ ‡é¢˜ï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            ä¸‹è½½ç»“æœå­—å…¸
        """
        self.stats['total_downloads'] += 1
        
        output_dir = output_dir or self.download_dir
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        filename = self._clean_filename(title or "unknown", doi=doi)
        output_path = output_dir / filename
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if output_path.exists() and self._validate_pdf_file(output_path):
            file_size = output_path.stat().st_size
            self.logger.info(f"âœ… æ–‡ä»¶å·²å­˜åœ¨: {filename} ({file_size} bytes)")
            return {
                'success': True,
                'doi': doi,
                'title': title,
                'local_path': str(output_path),
                'file_size': file_size,
                'status': 'already_exists',
                'error': None
            }
        
        # å°è¯•ä¸‹è½½
        for attempt in range(self.max_retries):
            try:
                self.logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½ (å°è¯• {attempt + 1}/{self.max_retries}): {doi}")
                
                # ä½¿ç”¨SciHubä¸‹è½½
                success, error = self.scihub.download_by_doi(doi, output_path, delay=self.retry_delay)
                
                if success and self._validate_pdf_file(output_path):
                    file_size = output_path.stat().st_size
                    self.stats['successful_downloads'] += 1
                    
                    return {
                        'success': True,
                        'doi': doi,
                        'title': title,
                        'local_path': str(output_path),
                        'file_size': file_size,
                        'status': 'downloaded',
                        'error': None,
                        'attempts': attempt + 1
                    }
                else:
                    self.logger.warning(f"ä¸‹è½½å¤±è´¥ (å°è¯• {attempt + 1}): {error}")
                    self.stats['retries'] += 1
                    
                    if attempt < self.max_retries - 1:
                        time.sleep(self.retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
                    
            except Exception as e:
                self.logger.error(f"ä¸‹è½½å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")
                self.stats['retries'] += 1
                
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay * (attempt + 1))
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        self.stats['failed_downloads'] += 1
        
        return {
            'success': False,
            'doi': doi,
            'title': title,
            'local_path': None,
            'file_size': 0,
            'status': 'failed',
            'error': f"åœ¨ {self.max_retries} æ¬¡å°è¯•åä¸‹è½½å¤±è´¥",
            'attempts': self.max_retries
        }
    
    def download_by_pmid(self, pmid: str, title: Optional[str] = None,
                        output_dir: Optional[Path] = None) -> Dict[str, Any]:
        """
        é€šè¿‡ PMID ä¸‹è½½ PDF æ–‡ä»¶ï¼ˆå…ˆæŸ¥è¯¢ DOI å†ä¸‹è½½ï¼‰
        
        Args:
            pmid: PMID æ ‡è¯†ç¬¦
            title: è®ºæ–‡æ ‡é¢˜
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            ä¸‹è½½ç»“æœå­—å…¸
        """
        # å¦‚æœæ²¡æœ‰æä¾›æ ‡é¢˜ï¼Œå°è¯•ä»å…¶ä»–åœ°æ–¹è·å–
        if not title:
            title = f"PMID_{pmid}"
        
        # é¦–å…ˆæŸ¥è¯¢DOI
        doi_result = self.query_doi_by_title(title)
        
        if doi_result.get('doi'):
            doi = doi_result['doi']
            self.logger.info(f"âœ… é€šè¿‡æ ‡é¢˜æ‰¾åˆ° DOI: {doi}")
            
            # ä½¿ç”¨æ‰¾åˆ°çš„DOIä¸‹è½½
            result = self.download_by_doi(doi, title, output_dir)
            result['pmid'] = pmid
            result['doi_source'] = 'title_query'
            return result
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ° DOIï¼Œå°è¯•ç›´æ¥ä½¿ç”¨ PMID æ„é€ æ–‡ä»¶å
            self.logger.warning(f"æœªæ‰¾åˆ° DOIï¼Œå°è¯•å…¶ä»–æ–¹å¼: PMID {pmid}")
            
            output_dir = output_dir or self.download_dir
            filename = self._clean_filename(title, pmid=pmid)
            
            return {
                'success': False,
                'pmid': pmid,
                'doi': None,
                'title': title,
                'local_path': None,
                'file_size': 0,
                'status': 'no_doi_found',
                'error': f"æ— æ³•æ‰¾åˆ° PMID {pmid} å¯¹åº”çš„ DOI",
                'doi_query_error': doi_result.get('error')
            }
    
    def download_with_fallback(self, doi: Optional[str], title: str,
                              output_dir: Optional[Path] = None) -> Dict[str, Any]:
        """
        å¸¦å›é€€æœºåˆ¶çš„ä¸‹è½½ï¼ˆå‚è€ƒ RecursiveScholarCrawler çš„é€»è¾‘ï¼‰
        
        Args:
            doi: DOI æ ‡è¯†ç¬¦ï¼ˆå¯é€‰ï¼‰
            title: è®ºæ–‡æ ‡é¢˜
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            ä¸‹è½½ç»“æœå­—å…¸
        """
        # æ­¥éª¤ 1ï¼šå¦‚æœæä¾›äº† DOIï¼Œå…ˆå°è¯•ä½¿ç”¨å®ƒ
        if doi:
            self.logger.info(f"ğŸ¯ ä½¿ç”¨æä¾›çš„ DOI ä¸‹è½½: {doi}")
            result = self.download_by_doi(doi, title, output_dir)
            if result['success']:
                result['download_method'] = 'provided_doi'
                return result
            else:
                self.logger.warning(f"æä¾›çš„ DOI ä¸‹è½½å¤±è´¥: {result.get('error')}")
        
        # æ­¥éª¤ 2ï¼šå¦‚æœæ²¡æœ‰ DOI æˆ– DOI ä¸‹è½½å¤±è´¥ï¼Œé€šè¿‡æ ‡é¢˜æŸ¥è¯¢æ–°çš„ DOI
        if not title:
            return {
                'success': False,
                'doi': doi,
                'title': title,
                'local_path': None,
                'file_size': 0,
                'status': 'no_title_for_doi_search',
                'error': "æ²¡æœ‰ DOI ä¸”æ²¡æœ‰æ ‡é¢˜ï¼Œæ— æ³•ç»§ç»­",
                'download_method': 'failed'
            }
        
        self.logger.info(f"ğŸ” é€šè¿‡æ ‡é¢˜æŸ¥è¯¢æ–°çš„ DOI: {title[:70]}...")
        doi_result = self.query_doi_by_title(title)
        
        new_doi = doi_result.get("doi")
        if not new_doi:
            error_msg = f"æ— æ³•æ‰¾åˆ°æ ‡é¢˜å¯¹åº”çš„ DOI: {doi_result.get('error')}"
            self.logger.error(error_msg)
            return {
                'success': False,
                'doi': doi,
                'title': title,
                'local_path': None,
                'file_size': 0,
                'status': 'doi_not_found',
                'error': error_msg,
                'download_method': 'failed'
            }
        
        # é¿å…é‡å¤ä¸‹è½½ç›¸åŒçš„DOI
        if new_doi == doi:
            error_msg = f"æŸ¥è¯¢åˆ°çš„ DOI ä¸å¤±è´¥çš„ DOI ç›¸åŒ: {new_doi}"
            self.logger.warning(error_msg)
            return {
                'success': False,
                'doi': doi,
                'title': title,
                'local_path': None,
                'file_size': 0,
                'status': 'same_doi_failed',
                'error': error_msg,
                'download_method': 'failed'
            }
        
        # æ­¥éª¤3ï¼šä½¿ç”¨æ–°æ‰¾åˆ°çš„DOIä¸‹è½½
        self.logger.info(f"âœ¨ æ‰¾åˆ°æ–°çš„ DOIï¼Œå¼€å§‹ä¸‹è½½: {new_doi}")
        result = self.download_by_doi(new_doi, title, output_dir)
        
        if result['success']:
            result['download_method'] = 'title_resolved_doi'
            result['original_doi'] = doi
            result['resolved_doi'] = new_doi
        else:
            result['download_method'] = 'all_methods_failed'
            result['doi_query_result'] = doi_result
        
        return result
    
    def batch_download(self, items: List[Dict[str, Any]], 
                      max_workers: Optional[int] = None,
                      output_dir: Optional[Path] = None) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡ä¸‹è½½ PDF æ–‡ä»¶
        
        Args:
            items: ä¸‹è½½é¡¹ç›®åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« 'doi', 'title', 'pmid' ç­‰å­—æ®µ
            max_workers: æœ€å¤§å¹¶å‘æ•°
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            ä¸‹è½½ç»“æœåˆ—è¡¨
        """
        max_workers = max_workers or min(self.max_workers, len(items))
        output_dir = output_dir or self.download_dir
        
        self.logger.info(f"ğŸ“¦ å¼€å§‹æ‰¹é‡ä¸‹è½½ï¼Œå…± {len(items)} é¡¹ï¼Œå¹¶å‘æ•°: {max_workers}")
        
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä¸‹è½½ä»»åŠ¡
            future_to_item = {}
            
            for item in items:
                doi = item.get('doi')
                title = item.get('title', item.get('Title', ''))
                pmid = item.get('pmid', item.get('PMID', ''))
                
                # é€‰æ‹©ä¸‹è½½æ–¹æ³•
                if doi and title:
                    future = executor.submit(self.download_with_fallback, doi, title, output_dir)
                elif pmid and title:
                    future = executor.submit(self.download_by_pmid, pmid, title, output_dir)
                elif doi:
                    future = executor.submit(self.download_by_doi, doi, title, output_dir)
                else:
                    # æ— æ³•ä¸‹è½½çš„é¡¹ç›®
                    results.append({
                        'success': False,
                        'doi': doi,
                        'pmid': pmid,
                        'title': title,
                        'local_path': None,
                        'file_size': 0,
                        'status': 'insufficient_info',
                        'error': 'ç¼ºå°‘ DOIã€PMID æˆ–æ ‡é¢˜ä¿¡æ¯'
                    })
                    continue
                
                future_to_item[future] = item
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_item):
                item = future_to_item[future]
                try:
                    result = future.result()
                    result['original_item'] = item
                    results.append(result)
                except Exception as e:
                    self.logger.error(f"æ‰¹é‡ä¸‹è½½ä»»åŠ¡å¼‚å¸¸: {e}")
                    results.append({
                        'success': False,
                        'doi': item.get('doi'),
                        'pmid': item.get('pmid'),
                        'title': item.get('title'),
                        'local_path': None,
                        'file_size': 0,
                        'status': 'exception',
                        'error': str(e),
                        'original_item': item
                    })
        
        # ç»Ÿè®¡ç»“æœ
        successful = len([r for r in results if r.get('success')])
        self.logger.info(f"âœ… æ‰¹é‡ä¸‹è½½å®Œæˆ: {successful}/{len(results)} æˆåŠŸ")
        
        return results
    
    def retry_failed_downloads(self, failed_results: List[Dict[str, Any]],
                              max_retries: Optional[int] = None) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """
        é‡è¯•å¤±è´¥çš„ä¸‹è½½
        
        Args:
            failed_results: å¤±è´¥çš„ä¸‹è½½ç»“æœåˆ—è¡¨
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            (ä»ç„¶å¤±è´¥çš„ç»“æœ, é‡è¯•æˆåŠŸçš„ç»“æœ)
        """
        max_retries = max_retries or self.max_retries
        
        if not failed_results:
            self.logger.info("æ²¡æœ‰å¤±è´¥çš„ä¸‹è½½éœ€è¦é‡è¯•")
            return [], []
        
        self.logger.info(f"ğŸ”„ å¼€å§‹é‡è¯• {len(failed_results)} ä¸ªå¤±è´¥çš„ä¸‹è½½")
        
        still_failed = []
        newly_successful = []
        
        for i, result in enumerate(failed_results):
            doi = result.get('doi')
            title = result.get('title')
            pmid = result.get('pmid')
            
            retry_count = result.get('retry_count', 0) + 1
            
            if retry_count > max_retries:
                self.logger.warning(f"è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡: {title or doi or pmid}")
                result['retry_count'] = retry_count
                still_failed.append(result)
                continue
            
            self.logger.info(f"é‡è¯• {retry_count}/{max_retries} [{i+1}/{len(failed_results)}]: {title or doi or pmid}")
            
            # é€‰æ‹©é‡è¯•æ–¹æ³•
            if doi and title:
                retry_result = self.download_with_fallback(doi, title)
            elif pmid and title:
                retry_result = self.download_by_pmid(pmid, title)
            elif doi:
                retry_result = self.download_by_doi(doi, title)
            else:
                retry_result = {
                    'success': False,
                    'error': 'ç¼ºå°‘é‡è¯•æ‰€éœ€çš„ä¿¡æ¯'
                }
            
            # æ›´æ–°ç»“æœ
            retry_result.update({
                'retry_count': retry_count,
                'retry_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'original_error': result.get('error')
            })
            
            if retry_result['success']:
                self.logger.info(f"âœ… é‡è¯•æˆåŠŸ: {title or doi or pmid}")
                newly_successful.append(retry_result)
            else:
                self.logger.warning(f"âŒ é‡è¯•ä»ç„¶å¤±è´¥: {retry_result.get('error')}")
                still_failed.append(retry_result)
            
            # é‡è¯•é—´éš”
            if i < len(failed_results) - 1:
                time.sleep(self.retry_delay)
        
        self.logger.info(f"ğŸ”„ é‡è¯•å®Œæˆ: {len(newly_successful)} æˆåŠŸ, {len(still_failed)} ä»ç„¶å¤±è´¥")
        
        return still_failed, newly_successful

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œå…³é—­ä¼šè¯"""
        if hasattr(self, 'session'):
            self.session.close()