# -*- coding: utf-8 -*-
"""
PubMed æ–‡çŒ®åŸºæœ¬ä¿¡æ¯è·å–æ¨¡å—

åŸºäº PubEx é¡¹ç›®ï¼Œæä¾›é«˜æ•ˆçš„æ–‡çŒ®ä¿¡æ¯çˆ¬å–åŠŸèƒ½
åŒ…å«æ–­ç‚¹ç»­ä¼ ã€æ‰¹é‡å¤„ç†ã€å¼•ç”¨å…³ç³»åˆ†æç­‰ç‰¹æ€§
"""

import sys
import pandas as pd
from Bio import Entrez, Medline
import time
from urllib.error import HTTPError
import re
import ssl
import urllib3
from datetime import datetime
from tqdm import tqdm
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
import logging

from utils.logger import LoggerMixin
from utils.file_handler import FileHandler
from utils.api_manager import api_manager

# æ”¹è¿› SSL é…ç½®
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


# åˆ›å»ºæ›´å¥å£®çš„ SSL ä¸Šä¸‹æ–‡
def create_ssl_context():
    """åˆ›å»ºå¥å£®çš„ SSL ä¸Šä¸‹æ–‡"""
    try:
        # åˆ›å»ºä¸€ä¸ªæ›´å®½æ¾çš„ SSL ä¸Šä¸‹æ–‡
        context = ssl.create_default_context()
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        # è®¾ç½®æ›´å…¼å®¹çš„åŠ å¯†å¥—ä»¶
        context.set_ciphers('DEFAULT@SECLEVEL=1')
        # è®¾ç½®æ›´é•¿çš„è¶…æ—¶
        import socket
        socket.setdefaulttimeout(60)
        return context
    except Exception as e:
        logger.warning(f"åˆ›å»º SSL ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        # å›é€€åˆ°ä¸éªŒè¯çš„ä¸Šä¸‹æ–‡
        return ssl._create_unverified_context()


# åº”ç”¨æ”¹è¿›çš„ SSL é…ç½®
try:
    ssl_context = create_ssl_context()
    ssl._create_default_https_context = lambda: ssl_context
except:
    ssl_context = ssl._create_unverified_context()
    ssl._create_default_https_context = ssl._create_unverified_context

logger = logging.getLogger(__name__)


# ä¸º BioPython Entrez é…ç½® SSL ä¸Šä¸‹æ–‡å’Œé‡è¯•è®¾ç½®
def configure_entrez_ssl():
    """ä¸º Entrez é…ç½® SSL å’Œé‡è¯•è®¾ç½®"""
    try:
        from Bio import Entrez
        # è®¾ç½® SSL ä¸Šä¸‹æ–‡
        Entrez.ssl_context = ssl_context
        # è®¾ç½®é‡è¯•æ¬¡æ•°
        Entrez.max_retries = 3
        # è®¾ç½®è¶…æ—¶æ—¶é—´
        Entrez.timeout = 60
        logger.info("å·²ä¸º Entrez é…ç½® SSL ä¸Šä¸‹æ–‡")
    except Exception as e:
        logger.warning(f"ä¸º Entrez é…ç½® SSL ä¸Šä¸‹æ–‡å¤±è´¥: {e}")


# é…ç½® Entrez
configure_entrez_ssl()


class PubMedFetcher(LoggerMixin):
    """PubMed æ–‡çŒ®ä¿¡æ¯è·å–å™¨"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ– PubMed è·å–å™¨

        Args:
            config: PubMed é…ç½®
        """
        self.config = config
        self.email = config.get('email', '')
        self.api_key = config.get('api_key', '')
        self.batch_size = config.get('batch_size', 50)
        self.max_retries = config.get('max_retries', 5)
        self.retry_wait_time = config.get('retry_wait_time', 5)
        self.output_dir = Path(config.get('output_dir', './results'))
        self.log_dir = Path(config.get('log_dir', './logs'))

        # å¼•ç”¨è¯¦æƒ…é…ç½®
        citation_config = config.get('citation_details', {})
        self.fetch_detailed_pmid_lists = citation_config.get('fetch_detailed_pmid_lists', True)

        # æ ¹æ®æ˜¯å¦æœ‰ api_key è®¾ç½® API ç­‰å¾…æ—¶é—´
        cfg_wait = config.get('api_wait_time', None)
        if cfg_wait is None:
            self.api_wait_time = 0.1 if self.api_key else 0.4
        else:
            self.api_wait_time = float(cfg_wait)

        # è®¾ç½® Entrez å‚æ•°
        if self.email:
            Entrez.email = self.email
        if self.api_key:
            Entrez.api_key = self.api_key

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # æå–æ—¥æœŸçš„æ­£åˆ™è¡¨è¾¾å¼
        self.date_pattern = re.compile(r'\b\d{4}\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2}\b')

        # è®¾ç½® API é™æµ
        api_name = 'pubmed_with_key' if self.api_key else 'pubmed_no_key'
        self.api_name = api_name

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_articles": 0,
            "fetched_articles": 0,
            "retries": 0,
            "errors": 0,
            "start_time": datetime.now(),
        }

    def _fetch_with_retry(self, fetch_function, *args, **kwargs):
        """
        å¸¦é‡è¯•çš„ API è¯·æ±‚ï¼ˆæ”¹è¿›çš„ SSL å’Œç½‘ç»œé”™è¯¯å¤„ç†ï¼‰

        Args:
            fetch_function: Entrez å‡½æ•°
            *args, **kwargs: å‡½æ•°å‚æ•°

        Returns:
            API å“åº”ç»“æœ
        """
        max_retries = kwargs.pop('max_retries', self.max_retries)
        retry_delay = kwargs.pop('retry_delay', self.retry_wait_time)

        for attempt in range(max_retries + 1):
            try:
                # åº”ç”¨é™æµ
                if self.api_name in api_manager.rate_limiters:
                    api_manager.rate_limiters[self.api_name].wait_if_needed()

                # è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
                if 'timeout' not in kwargs:
                    kwargs['timeout'] = 60  # 60 ç§’è¶…æ—¶

                result = fetch_function(*args, **kwargs)
                return result

            except HTTPError as e:
                self.logger.warning(f"HTTP é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries + 1}): {e.code} - {e.reason}")
                if attempt < max_retries:
                    wait_time = retry_delay * (2**attempt)  # æŒ‡æ•°é€€é¿
                    self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    import time
                    time.sleep(wait_time)
                    self.stats["retries"] += 1
                else:
                    self.logger.error(f"HTTP é”™è¯¯ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                    raise

            except urllib3.exceptions.SSLError as e:
                self.logger.warning(f"SSL é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries + 1}): {e}")
                if attempt < max_retries:
                    wait_time = retry_delay * (2**attempt)
                    self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    import time
                    time.sleep(wait_time)
                    self.stats["retries"] += 1
                else:
                    self.logger.error(f"SSL é”™è¯¯ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                    raise

            except urllib3.exceptions.ConnectionError as e:
                self.logger.warning(f"è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries + 1}): {e}")
                if attempt < max_retries:
                    wait_time = retry_delay * (2**attempt)
                    self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    import time
                    time.sleep(wait_time)
                    self.stats["retries"] += 1
                else:
                    self.logger.error(f"è¿æ¥é”™è¯¯ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                    raise

            except Exception as e:
                error_msg = str(e).lower()
                # ç‰¹åˆ«å¤„ç† SSL ç›¸å…³é”™è¯¯
                if any(keyword in error_msg for keyword in ['ssl', 'eof', 'certificate', 'handshake', 'connection reset']):
                    self.logger.warning(f"ç½‘ç»œ / SSL é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries + 1}): {e}")
                    if attempt < max_retries:
                        wait_time = retry_delay * (2**attempt) + 5  # é¢å¤–å¢åŠ  5 ç§’
                        self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        import time
                        time.sleep(wait_time)
                    self.stats["retries"] += 1
                else:
                    self.logger.error(f"æœªçŸ¥é”™è¯¯ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                    raise

    def extract_publication_date(self, record: Dict[str, Any]) -> str:
        """
        ä»è®°å½•ä¸­æå–å‡ºç‰ˆæ—¥æœŸ

        Args:
            record: æ–‡çŒ®è®°å½•

        Returns:
            æ ¼å¼åŒ–çš„å‡ºç‰ˆæ—¥æœŸ
        """
        if 'DP' in record:
            match = self.date_pattern.search(record['DP'])
            if match:
                return match.group()

        if 'SO' in record:
            match = self.date_pattern.search(record['SO'])
            if match:
                return match.group()

        return 'NA'

    def fetch_citation_data_batch(self, pmid_list: List[str]) -> Dict[str, tuple]:
        """
        æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯

        Args:
            pmid_list: PMID åˆ—è¡¨

        Returns:
            å¼•ç”¨ä¿¡æ¯å­—å…¸ {PMID: (cited_by_list, references_list)}
        """
        citation_dict = {}

        if not pmid_list:
            return citation_dict

        # å¦‚æœä¸éœ€è¦è¯¦ç»† PMID åˆ—è¡¨ï¼Œåªè·å–æ•°é‡
        if not self.fetch_detailed_pmid_lists:
            self.logger.debug(f"åªè·å–å¼•ç”¨æ•°é‡ï¼Œä¸è·å–è¯¦ç»† PMID åˆ—è¡¨")
            return self._fetch_citation_counts_only(pmid_list)

        # è·å–è¯¦ç»†çš„ PMID åˆ—è¡¨
        self.logger.debug(f"è·å–è¯¦ç»†çš„å¼•ç”¨ PMID åˆ—è¡¨")

        # ä½¿ç”¨é‡è¯•æœºåˆ¶æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯
        try:
            # æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯
            handle_elink = self._fetch_with_retry(Entrez.elink,
                                                  db="pubmed",
                                                  id=pmid_list,
                                                  linkname="pubmed_pubmed_citedin,pubmed_pubmed_refs",
                                                  retmode="xml",
                                                  cmd="neighbor",
                                                  max_retries=self.max_retries,
                                                  retry_delay=self.retry_wait_time)

            # æ£€æŸ¥å“åº”å†…å®¹æ˜¯å¦æœ‰æ•ˆ
            if handle_elink is None:
                raise Exception("è·å–å¼•ç”¨ä¿¡æ¯å¤±è´¥ï¼šAPI è¿”å›ç©ºå“åº”")

            records_elink = Entrez.read(handle_elink)
            handle_elink.close()

            # å¤„ç†æ¯ä¸ª PMID çš„ç»“æœ
            for i, record in enumerate(records_elink):
                pmid = pmid_list[i] if i < len(pmid_list) else None
                if not pmid:
                    continue

                linked = []
                references = []

                if "LinkSetDb" in record:
                    for linkset in record["LinkSetDb"]:
                        if linkset["LinkName"] == "pubmed_pubmed_citedin" and "Link" in linkset:
                            linked.extend(link["Id"] for link in linkset["Link"] if link.get("Id"))
                        elif linkset["LinkName"] == "pubmed_pubmed_refs" and "Link" in linkset:
                            references.extend(link["Id"] for link in linkset["Link"] if link.get("Id"))

                citation_dict[pmid] = (linked, references)

        except RuntimeError as e:
            # ç‰¹åˆ«å¤„ç† XML è§£æé”™è¯¯
            if "Couldn't resolve" in str(e) or "address table is empty" in str(e):
                self.logger.warning(f"PubMed API XML è§£æé”™è¯¯ï¼Œå¯èƒ½æ˜¯ä¸´æ—¶æœåŠ¡é—®é¢˜: {e}")
                # ä¸ºæ¯ä¸ª PMID è®¾ç½®ç©ºå¼•ç”¨ä¿¡æ¯
                for pmid in pmid_list:
                    citation_dict[pmid] = ([], [])
                self.stats["errors"] += 1
            else:
                self.logger.error(f"æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯å¤±è´¥ (RuntimeError): {e}")
                self.stats["errors"] += 1
                # é‡æ–°æŠ›å‡ºå…¶ä»– RuntimeError
                raise
        except Exception as e:
            self.logger.error(f"æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯å¤±è´¥: {e}")
            self.stats["errors"] += 1
            # ç¡®ä¿å³ä½¿å‡ºé”™ä¹Ÿè¿”å›ç©ºç»“æœè€Œä¸æ˜¯å´©æºƒ
            for pmid in pmid_list:
                if pmid not in citation_dict:
                    citation_dict[pmid] = ([], [])

        return citation_dict

    def _fetch_citation_counts_only(self, pmid_list: List[str]) -> Dict[str, tuple]:
        """
        åªè·å–å¼•ç”¨æ•°é‡ï¼Œä¸è·å–è¯¦ç»†çš„ PMID åˆ—è¡¨

        Args:
            pmid_list: PMID åˆ—è¡¨

        Returns:
            å¼•ç”¨ä¿¡æ¯å­—å…¸ {PMID: (cited_count, references_count)}
        """
        citation_dict = {}

        # ä½¿ç”¨é‡è¯•æœºåˆ¶æ‰¹é‡è·å–å¼•ç”¨æ•°é‡
        try:
            # æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯ï¼ˆåªè·å–æ•°é‡ï¼‰
            handle_elink = self._fetch_with_retry(Entrez.elink,
                                                  db="pubmed",
                                                  id=pmid_list,
                                                  linkname="pubmed_pubmed_citedin,pubmed_pubmed_refs",
                                                  retmode="xml",
                                                  cmd="neighbor",
                                                  max_retries=self.max_retries,
                                                  retry_delay=self.retry_wait_time)

            # æ£€æŸ¥å“åº”å†…å®¹æ˜¯å¦æœ‰æ•ˆ
            if handle_elink is None:
                raise Exception("è·å–å¼•ç”¨æ•°é‡å¤±è´¥ï¼šAPI è¿”å›ç©ºå“åº”")

            records_elink = Entrez.read(handle_elink)
            handle_elink.close()

            # å¤„ç†æ¯ä¸ª PMID çš„ç»“æœï¼Œåªè®¡ç®—æ•°é‡
            for i, record in enumerate(records_elink):
                pmid = pmid_list[i] if i < len(pmid_list) else None
                if not pmid:
                    continue

                cited_count = 0
                references_count = 0

                if "LinkSetDb" in record:
                    for linkset in record["LinkSetDb"]:
                        if linkset["LinkName"] == "pubmed_pubmed_citedin" and "Link" in linkset:
                            cited_count = len(linkset["Link"])
                        elif linkset["LinkName"] == "pubmed_pubmed_refs" and "Link" in linkset:
                            references_count = len(linkset["Link"])

                # ä½¿ç”¨ COUNT_ONLY æ ‡è®°ä¼ é€’æ•°é‡ä¿¡æ¯
                citation_dict[pmid] = (
                    [f"COUNT_ONLY:{cited_count}"],  # ç‰¹æ®Šæ ‡è®°è¡¨ç¤ºåªæœ‰æ•°é‡
                    [f"COUNT_ONLY:{references_count}"])

        except RuntimeError as e:
            # ç‰¹åˆ«å¤„ç† XML è§£æé”™è¯¯
            if "Couldn't resolve" in str(e) or "address table is empty" in str(e):
                self.logger.warning(f"PubMed API XML è§£æé”™è¯¯ï¼Œå¯èƒ½æ˜¯ä¸´æ—¶æœåŠ¡é—®é¢˜: {e}")
                # ä¸ºæ¯ä¸ª PMID è®¾ç½®ç©ºå¼•ç”¨ä¿¡æ¯
                for pmid in pmid_list:
                    citation_dict[pmid] = ([f"COUNT_ONLY:0"], [f"COUNT_ONLY:0"])
                self.stats["errors"] += 1
            else:
                self.logger.error(f"æ‰¹é‡è·å–å¼•ç”¨æ•°é‡å¤±è´¥ (RuntimeError): {e}")
                self.stats["errors"] += 1
                # é‡æ–°æŠ›å‡ºå…¶ä»– RuntimeError
                raise
        except Exception as e:
            self.logger.error(f"æ‰¹é‡è·å–å¼•ç”¨æ•°é‡å¤±è´¥: {e}")
            self.stats["errors"] += 1
            # ç¡®ä¿å³ä½¿å‡ºé”™ä¹Ÿè¿”å›ç©ºç»“æœè€Œä¸æ˜¯å´©æºƒ
            for pmid in pmid_list:
                if pmid not in citation_dict:
                    citation_dict[pmid] = ([f"COUNT_ONLY:0"], [f"COUNT_ONLY:0"])

        return citation_dict

    def create_record_dict(self, record: Dict[str, Any], publication_date: str, cited_by: List[str],
                           references: List[str]) -> Dict[str, Any]:
        """
        åˆ›å»ºæ ‡å‡†åŒ–çš„æ–‡çŒ®è®°å½•å­—å…¸

        Args:
            record: åŸå§‹è®°å½•
            publication_date: å‡ºç‰ˆæ—¥æœŸ
            cited_by: è¢«å¼•ç”¨åˆ—è¡¨æˆ–æ•°é‡æ ‡è®°
            references: å‚è€ƒæ–‡çŒ®åˆ—è¡¨æˆ–æ•°é‡æ ‡è®°

        Returns:
            æ ‡å‡†åŒ–è®°å½•å­—å…¸
        """
        # å¤„ç†åªæœ‰æ•°é‡çš„æƒ…å†µ
        if cited_by and len(cited_by) == 1 and str(cited_by[0]).startswith("COUNT_ONLY:"):
            cited_count = int(str(cited_by[0]).replace("COUNT_ONLY:", ""))
            cited_by = []  # æ¸…ç©ºåˆ—è¡¨ï¼Œåªä¿ç•™æ•°é‡
        else:
            cited_count = len(cited_by) if cited_by else 0

        if references and len(references) == 1 and str(references[0]).startswith("COUNT_ONLY:"):
            references_count = int(str(references[0]).replace("COUNT_ONLY:", ""))
            references = []  # æ¸…ç©ºåˆ—è¡¨ï¼Œåªä¿ç•™æ•°é‡
        else:
            references_count = len(references) if references else 0
        return {
            'Title': record.get('TI', 'NA'),
            'Status': record.get('STAT', 'NA'),
            'Last_Revision_Date': record.get('LR', 'NA'),
            'ISSN': record.get('IS', 'NA'),
            'Type': record.get('PT', 'NA'),
            'Year_of_Publication': record.get('DP', 'NA').split(' ')[0] if 'DP' in record else 'NA',
            'Date_of_Electronic_Publication': record.get('DEP', 'NA'),
            'Publication_Date': publication_date,
            'Place_of_Publication': record.get('PL', 'NA'),
            'First_Author': record.get('FAU', 'NA'),
            'Authors': record.get('AU', 'NA'),
            'Affiliation': record.get('AD', 'NA'),
            'Abstract': record.get('AB', 'NA'),
            'Language': record.get('LA', 'NA'),
            'Keywords': record.get('OT', 'NA'),
            'PMID': record.get('PMID', 'NA'),
            'Medline_Volume': record.get('VI', 'NA'),
            'Medline_Issue': record.get('IP', 'NA'),
            'Medline_Pagination': record.get('PG', 'NA'),
            'DOI': record.get('LID', 'NA').split(' ')[0] if 'LID' in record else 'NA',
            'PMC': record.get('PMC', 'NA'),
            'Processing_History': record.get('PHST', 'NA'),
            'Publication_Status': record.get('PST', 'NA'),
            'Journal_Title_Abbreviation': record.get('TA', 'NA'),
            'Journal_Title': record.get('JT', 'NA'),
            'Journal_ID': record.get('JID', 'NA'),
            'Source': record.get('SO', 'NA'),
            'Grant_List': record.get('GR', 'NA'),
            'Cited_Count': cited_count,
            'Cited_By': cited_by,
            'References_Count': references_count,
            'References_PMID': references,
        }

    def check_existing_data(self, output_dir: Path) -> tuple:
        """
        æ£€æŸ¥ç°æœ‰æ•°æ®ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰

        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„

        Returns:
            (å·²å¤„ç†çš„ PMID é›†åˆ , å·²æœ‰æ•°æ®åˆ—è¡¨, æœ€æ–°æ–‡ä»¶è·¯å¾„)
        """
        try:
            if not output_dir.exists():
                return set(), [], None

            # æŸ¥æ‰¾æœ€æ–°çš„ CSV æ–‡ä»¶
            csv_files = list(output_dir.glob("*.csv"))
            if not csv_files:
                return set(), [], None

            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„æ–‡ä»¶
            latest_file = max(csv_files, key=lambda f: f.stat().st_mtime)

            self.logger.info(f"ğŸ“ æ£€æµ‹åˆ°ç°æœ‰æ•°æ®æ–‡ä»¶ : {latest_file}")
            existing_df = pd.read_csv(latest_file, keep_default_na=False, na_values=[''])
            self.logger.info(f"ğŸ“Š å·²æœ‰ {len(existing_df)} ç¯‡æ–‡çŒ®ï¼Œå°†è¿›è¡Œæ–­ç‚¹ç»­ä¼ ")

            valid_pmids = existing_df['PMID'].dropna().astype(str)
            valid_pmids = [pmid for pmid in valid_pmids if pmid.strip() and pmid != 'nan']
            return set(valid_pmids), existing_df.to_dict('records'), latest_file
        except Exception as e:
            self.logger.error(f"è¯»å–ç°æœ‰æ•°æ®æ—¶å‡ºé”™ : {e}")
            return set(), [], None

    def _log_completion_stats(self, data: List[Dict[str, Any]], output_dir: Path = None):
        """
        è®°å½•å®Œæˆç»Ÿè®¡ä¿¡æ¯

        Args:
            data: æ•°æ®åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        elapsed = (datetime.now() - self.stats["start_time"]).total_seconds() / 60
        self.logger.info(f"âœ… å®Œæˆ ! æ€»å…±å¤„ç† {len(data)} ç¯‡æ–‡çŒ®")
        self.logger.info(f"â±ï¸ ç”¨æ—¶ : {elapsed:.2f} åˆ†é’Ÿ")
        if output_dir:
            self.logger.info(f"ğŸ“ è¾“å‡ºç›®å½• : {output_dir}")
        else:
            self.logger.info(f"ğŸ“„ æ•°æ®å·²è·å–ï¼Œå¯é€šè¿‡ DataProcessor ä¿å­˜ä¸ºæ–‡ä»¶")

    def _process_batch_with_progress(self,
                                     records: List,
                                     batch_pmids: List[str],
                                     data: List[Dict[str, Any]],
                                     output_file: Path,
                                     batch_progress,
                                     resume: bool = False,
                                     existing_pmids: set = None) -> int:
        """
        å¤„ç†ä¸€æ‰¹æ•°æ®å¹¶ä¿å­˜ç»“æœ

        Args:
            records: Medline è®°å½•åˆ—è¡¨
            batch_pmids: PMID åˆ—è¡¨
            data: å·²æœ‰æ•°æ®åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            batch_progress: è¿›åº¦æ¡å¯¹è±¡
            resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ 
            existing_pmids: å·²å¤„ç†çš„ PMID é›†åˆ

        Returns:
            å¤„ç†çš„è®°å½•æ•°é‡
        """
        if existing_pmids is None:
            existing_pmids = set()

        # è¿‡æ»¤å‡ºéœ€è¦å¤„ç†çš„è®°å½•
        if resume:
            records_to_process = [r for r in records if r.get('PMID') and r.get('PMID') not in existing_pmids]
        else:
            records_to_process = records

        if not records_to_process:
            return 0

        # æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯
        citation_data = self.fetch_citation_data_batch(batch_pmids)

        # å¤„ç†æ¯ç¯‡æ–‡çŒ®
        processed_count = 0
        for record in records_to_process:
            pmid = record.get('PMID', 'NA')

            # è·å–å¼•ç”¨ä¿¡æ¯
            cited_by, references = citation_data.get(pmid, ([], []))

            # æå–å‘å¸ƒæ—¥æœŸ
            publication_date = self.extract_publication_date(record)

            # åˆ›å»ºè®°å½•
            record_dict = self.create_record_dict(record, publication_date, cited_by, references)
            data.append(record_dict)

            processed_count += 1
            self.stats["fetched_articles"] += 1

        return processed_count

    def fetch_by_query(self, query: str, resume: bool = True, max_results: int = None) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æŸ¥è¯¢è¯è·å–æ–‡çŒ®ä¿¡æ¯

        Args:
            query: PubMed æŸ¥è¯¢è¯
            resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ 
            max_results: æœ€å¤§ç»“æœæ•°é‡é™åˆ¶

        Returns:
            æ–‡çŒ®ä¿¡æ¯åˆ—è¡¨
        """
        self.logger.info(f"ğŸ” å¼€å§‹æ£€ç´¢ : {query}")

        # æ£€æŸ¥ç°æœ‰æ•°æ®
        existing_pmids, existing_data, latest_file = self.check_existing_data(self.output_dir) if resume else (set(), [], None)

        # æœç´¢æ–‡çŒ®
        self.logger.info("ğŸ“Š æ­£åœ¨æœç´¢æ–‡çŒ® ...")
        handle = self._fetch_with_retry(Entrez.esearch, db="pubmed", term=query, usehistory="y", retmax=0)
        search_results = Entrez.read(handle)
        handle.close()

        count = int(search_results.get("Count", 0) or 0)
        webenv = search_results.get("WebEnv")
        query_key = search_results.get("QueryKey")

        # åº”ç”¨æœ€å¤§ç»“æœæ•°é™åˆ¶
        if max_results is not None and max_results > 0:
            count = min(count, max_results)
            self.logger.info(f"ğŸ“š æ‰¾åˆ°æ–‡çŒ®æ€»æ•° : {search_results.get('Count', 0)}, é™åˆ¶è·å– : {count} ç¯‡")
        else:
            self.logger.info(f"ğŸ“š æ‰¾åˆ° {count} ç¯‡æ–‡çŒ®")

        self.stats["total_articles"] = count

        if count == 0:
            self.logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡çŒ®")
            return []

        # è·å–æ‰€æœ‰ PMIDï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ åˆ¤æ–­ï¼‰
        if resume and existing_pmids:
            self.logger.info("ğŸ” è·å– PMID åˆ—è¡¨ç”¨äºæ–­ç‚¹ç»­ä¼  ...")
            all_pmids = []
            pmid_batch_size = 9999

            for start in range(0, count, pmid_batch_size):
                retmax = min(pmid_batch_size, count - start)
                handle = self._fetch_with_retry(Entrez.esearch,
                                                db="pubmed",
                                                term=query,
                                                retstart=start,
                                                retmax=retmax,
                                                usehistory="n")
                results = Entrez.read(handle)
                handle.close()
                all_pmids.extend(results["IdList"])
                time.sleep(self.api_wait_time)

            # è¿‡æ»¤å·²å¤„ç†çš„ PMID
            new_pmids = [pmid for pmid in all_pmids if pmid not in existing_pmids]
            self.logger.info(f"ğŸ“Š éœ€è¦å¤„ç† {len(new_pmids)} ç¯‡æ–°æ–‡çŒ®")

            if len(new_pmids) == 0:
                self.logger.info("âœ… æ‰€æœ‰æ–‡çŒ®éƒ½å·²å¤„ç†å®Œæˆ")
                return existing_data

        # æ‰¹é‡è·å–æ–‡çŒ®è¯¦æƒ…
        self.logger.info("ğŸ“š å¼€å§‹æ‰¹é‡è·å–æ–‡çŒ®è¯¦æƒ… ...")
        data = list(existing_data)  # å¤åˆ¶ç°æœ‰æ•°æ®

        # è¿›åº¦æ¡
        total_batches = (count + self.batch_size - 1) // self.batch_size
        batch_progress = tqdm(total=total_batches, desc="ğŸ“¦ æ‰¹æ¬¡è¿›åº¦", unit="batch", position=0)

        processed_count = 0

        for start in range(0, count, self.batch_size):
            # è®¡ç®—å½“å‰æ‰¹æ¬¡åº”è¯¥è·å–çš„æ•°é‡
            current_batch_size = min(self.batch_size, count - start)

            # ä½¿ç”¨ WebEnv å’Œ QueryKey è·å–æ•°æ®
            handle = self._fetch_with_retry(Entrez.efetch,
                                            db="pubmed",
                                            rettype='medline',
                                            retmode="text",
                                            retstart=start,
                                            retmax=current_batch_size,
                                            webenv=webenv,
                                            query_key=query_key)
            records = list(Medline.parse(handle))
            handle.close()

            # è·å–æ‰€æœ‰ PMIDï¼ˆç”¨äºå¼•ç”¨ä¿¡æ¯è·å–ï¼‰
            batch_pmids = [record.get('PMID') for record in records]

            # ä½¿ç”¨é€šç”¨æ‰¹å¤„ç†æ–¹æ³•
            batch_processed = self._process_batch_with_progress(records=records,
                                                                batch_pmids=batch_pmids,
                                                                data=data,
                                                                output_file=self.output_dir,
                                                                batch_progress=batch_progress,
                                                                resume=resume,
                                                                existing_pmids=existing_pmids)

            processed_count += batch_processed

            batch_progress.update(1)
            time.sleep(self.api_wait_time)

        batch_progress.close()

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        self._log_completion_stats(data, self.output_dir)

        return data

    def fetch_by_pmid_list(self, pmid_list: List[str], resume: bool = True) -> List[Dict[str, Any]]:
        """
        æ ¹æ® PMID åˆ—è¡¨è·å–æ–‡çŒ®ä¿¡æ¯

        Args:
            pmid_list: PMID åˆ—è¡¨
            resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ 

        Returns:
            æ–‡çŒ®ä¿¡æ¯åˆ—è¡¨
        """
        pmid_list = [str(pmid).strip() for pmid in pmid_list if str(pmid).strip()]
        self.logger.info(f"ğŸ“‹ æ ¹æ® PMID åˆ—è¡¨è·å–æ–‡çŒ®ä¿¡æ¯ï¼Œå…± {len(pmid_list)} ä¸ª PMID")

        if not pmid_list:
            self.logger.warning("âš ï¸ PMID åˆ—è¡¨ä¸ºç©º")
            return []

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        filename = self._generate_output_filename("pubminer_pmids")
        output_file = self.output_dir / filename

        # æ£€æŸ¥ç°æœ‰æ•°æ®
        existing_pmids, existing_data = self.check_existing_data(output_file) if resume else (set(), [])

        # è¿‡æ»¤å·²å¤„ç†çš„ PMID
        if resume and existing_pmids:
            pmid_list = [pmid for pmid in pmid_list if pmid not in existing_pmids]
            self.logger.info(f"ğŸ“Š éœ€è¦å¤„ç† {len(pmid_list)} ä¸ªæ–° PMID")

        if not pmid_list:
            self.logger.info("âœ… æ‰€æœ‰ PMID éƒ½å·²å¤„ç†å®Œæˆ")
            return existing_data

        self.stats["total_articles"] = len(pmid_list)
        data = list(existing_data)

        # æ‰¹é‡å¤„ç†
        self.logger.info("ğŸ“š å¼€å§‹æ‰¹é‡è·å–æ–‡çŒ®è¯¦æƒ… ...")

        total_batches = (len(pmid_list) + self.batch_size - 1) // self.batch_size
        batch_progress = tqdm(total=total_batches, desc="ğŸ“¦ æ‰¹æ¬¡è¿›åº¦", unit="batch")

        for i in range(0, len(pmid_list), self.batch_size):
            batch_pmids = pmid_list[i:i + self.batch_size]

            try:
                # è·å–æ–‡çŒ®è¯¦æƒ…
                handle = self._fetch_with_retry(Entrez.efetch, db="pubmed", id=batch_pmids, rettype='medline', retmode="text")
                records = list(Medline.parse(handle))
                handle.close()

                # ä½¿ç”¨é€šç”¨æ‰¹å¤„ç†æ–¹æ³•
                self._process_batch_with_progress(records=records,
                                                  batch_pmids=batch_pmids,
                                                  data=data,
                                                  output_file=output_file,
                                                  batch_progress=batch_progress,
                                                  resume=resume,
                                                  existing_pmids=existing_pmids)

            except Exception as e:
                self.logger.error(f"âŒ å¤„ç†æ‰¹æ¬¡å¤±è´¥ : {e}")
                continue

            batch_progress.update(1)
            time.sleep(self.api_wait_time)

        batch_progress.close()

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        self._log_completion_stats(data, output_file)

        return data
