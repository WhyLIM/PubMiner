# -*- coding: utf-8 -*-
"""
PubMed æ–‡çŒ®åŸºæœ¬ä¿¡æ¯è·å–æ¨¡å—

åŸºäº PubEx é¡¹ç›®ï¼Œæä¾›é«˜æ•ˆçš„æ–‡çŒ®ä¿¡æ¯çˆ¬å–åŠŸèƒ½
åŒ…å«æ–­ç‚¹ç»­ä¼ ã€æ‰¹é‡å¤„ç†ã€å¼•ç”¨å…³ç³»åˆ†æç­‰ç‰¹æ€§
"""

import sys
import pandas as pd
from Bio import Entrez, Medline
import time
from urllib.error import HTTPError
import re
import ssl
import urllib3
from datetime import datetime
from tqdm import tqdm
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
import logging

from utils.logger import LoggerMixin
from utils.file_handler import FileHandler
from utils.api_manager import api_manager

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
ssl._create_default_https_context = ssl._create_unverified_context

logger = logging.getLogger(__name__)

class PubMedFetcher(LoggerMixin):
    """PubMed æ–‡çŒ®ä¿¡æ¯è·å–å™¨ """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ– PubMed è·å–å™¨
        
        Args:
            config: PubMed é…ç½®
        """
        self.config = config
        self.email = config.get('email', '')
        self.api_key = config.get('api_key', '')
        self.batch_size = config.get('batch_size', 50)
        self.max_retries = config.get('max_retries', 5)
        self.retry_wait_time = config.get('retry_wait_time', 5)
        self.output_dir = Path(config.get('output_dir', './results'))
        self.log_dir = Path(config.get('log_dir', './logs'))
        
        # å¼•ç”¨è¯¦æƒ…é…ç½®
        citation_config = config.get('citation_details', {})
        self.fetch_detailed_pmid_lists = citation_config.get('fetch_detailed_pmid_lists', True)
        
        # æ ¹æ®æ˜¯å¦æœ‰ api_key è®¾ç½® API ç­‰å¾…æ—¶é—´
        cfg_wait = config.get('api_wait_time', None)
        if cfg_wait is None:
            self.api_wait_time = 0.1 if self.api_key else 0.4
        else:
            self.api_wait_time = float(cfg_wait)
        
        # è®¾ç½® Entrez å‚æ•°
        if self.email:
            Entrez.email = self.email
        if self.api_key:
            Entrez.api_key = self.api_key
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # æå–æ—¥æœŸçš„æ­£åˆ™è¡¨è¾¾å¼
        self.date_pattern = re.compile(r'\b\d{4}\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2}\b')
        
        # è®¾ç½® API é™æµ
        api_name = 'pubmed_with_key' if self.api_key else 'pubmed_no_key'
        self.api_name = api_name
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_articles": 0,
            "fetched_articles": 0,
            "retries": 0,
            "start_time": datetime.now(),
        }
    
    @api_manager.with_retry(max_retries=5, retry_delay=2.0)
    def _fetch_with_retry(self, fetch_function, *args, **kwargs):
        """
        å¸¦é‡è¯•çš„ API è¯·æ±‚
        
        Args:
            fetch_function: Entrez å‡½æ•°
            *args, **kwargs: å‡½æ•°å‚æ•°
            
        Returns:
            API å“åº”ç»“æœ
        """
        try:
            # åº”ç”¨é™æµ
            if self.api_name in api_manager.rate_limiters:
                api_manager.rate_limiters[self.api_name].wait_if_needed()
            
            result = fetch_function(*args, **kwargs)
            return result
            
        except HTTPError as e:
            self.logger.warning(f"HTTP é”™è¯¯ : {e.code} - {e.reason}")
            self.stats["retries"] += 1
            raise
        except Exception as e:
            self.logger.warning(f"API è°ƒç”¨é”™è¯¯ : {e}")
            self.stats["retries"] += 1
            raise
    
    def extract_publication_date(self, record: Dict[str, Any]) -> str:
        """
        ä»è®°å½•ä¸­æå–å‡ºç‰ˆæ—¥æœŸ
        
        Args:
            record: æ–‡çŒ®è®°å½•
            
        Returns:
            æ ¼å¼åŒ–çš„å‡ºç‰ˆæ—¥æœŸ
        """
        if 'DP' in record:
            match = self.date_pattern.search(record['DP'])
            if match:
                return match.group()
        
        if 'SO' in record:
            match = self.date_pattern.search(record['SO'])
            if match:
                return match.group()
        
        return 'NA'
    
    def fetch_citation_data_batch(self, pmid_list: List[str]) -> Dict[str, tuple]:
        """
        æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯
        
        Args:
            pmid_list: PMID åˆ—è¡¨
            
        Returns:
            å¼•ç”¨ä¿¡æ¯å­—å…¸ {PMID: (cited_by_list, references_list)}
        """
        citation_dict = {}
        
        if not pmid_list:
            return citation_dict
        
        # å¦‚æœä¸éœ€è¦è¯¦ç»† PMID åˆ—è¡¨ï¼Œåªè·å–æ•°é‡
        if not self.fetch_detailed_pmid_lists:
            self.logger.debug(f" åªè·å–å¼•ç”¨æ•°é‡ï¼Œä¸è·å–è¯¦ç»† PMID åˆ—è¡¨ ")
            return self._fetch_citation_counts_only(pmid_list)
        
        # è·å–è¯¦ç»†çš„ PMID åˆ—è¡¨
        self.logger.debug(f" è·å–è¯¦ç»†çš„å¼•ç”¨ PMID åˆ—è¡¨ ")
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯
        for attempt in range(self.max_retries):
            try:
                # æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯
                handle_elink = Entrez.elink(
                    db="pubmed",
                    id=pmid_list,
                    linkname="pubmed_pubmed_citedin,pubmed_pubmed_refs",
                    retmode="xml",
                    cmd="neighbor"
                )
                records_elink = Entrez.read(handle_elink)
                handle_elink.close()
                
                # å¤„ç†æ¯ä¸ª PMID çš„ç»“æœ
                for i, record in enumerate(records_elink):
                    pmid = pmid_list[i] if i < len(pmid_list) else None
                    if not pmid:
                        continue
                        
                    linked = []
                    references = []
                    
                    if "LinkSetDb" in record:
                        for linkset in record["LinkSetDb"]:
                            if linkset["LinkName"] == "pubmed_pubmed_citedin" and "Link" in linkset:
                                linked.extend(link["Id"] for link in linkset["Link"] if link.get("Id"))
                            elif linkset["LinkName"] == "pubmed_pubmed_refs" and "Link" in linkset:
                                references.extend(link["Id"] for link in linkset["Link"] if link.get("Id"))
                    
                    citation_dict[pmid] = (linked, references)
                
                break
                
            except HTTPError as e:
                if e.code in [429, 500, 502, 503, 504]:
                    wait_time = self.retry_wait_time * (attempt + 1)
                    self.logger.warning(f" æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯ HTTP é”™è¯¯ {e.code}ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯• ( å°è¯• {attempt + 1}/{self.max_retries})...")
                    self.stats["retries"] += 1
                    time.sleep(wait_time)
                    continue
                else:
                    self.logger.error(f" æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯ HTTP é”™è¯¯ : {e}")
                    break
            except Exception as e:
                error_msg = str(e).lower()
                if any(keyword in error_msg for keyword in ['eof', 'connection', 'closed', 'timeout', 'read failed']):
                    wait_time = self.retry_wait_time * (attempt + 1)
                    self.logger.warning(f" æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯• ( å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                    self.stats["retries"] += 1
                    time.sleep(wait_time)
                    if attempt < self.max_retries - 1:
                        continue
                else:
                    self.logger.warning(f" æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯å¤±è´¥ : {e}")
                    if attempt < self.max_retries - 1:
                        self.stats["retries"] += 1
                        time.sleep(self.retry_wait_time)
                        continue
                break
        
        return citation_dict
    
    def _fetch_citation_counts_only(self, pmid_list: List[str]) -> Dict[str, tuple]:
        """
        åªè·å–å¼•ç”¨æ•°é‡ï¼Œä¸è·å–è¯¦ç»†çš„ PMID åˆ—è¡¨
        
        Args:
            pmid_list: PMID åˆ—è¡¨
            
        Returns:
            å¼•ç”¨ä¿¡æ¯å­—å…¸ {PMID: (cited_count, references_count)}
        """
        citation_dict = {}
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶æ‰¹é‡è·å–å¼•ç”¨æ•°é‡
        for attempt in range(self.max_retries):
            try:
                # æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯ï¼ˆåªè·å–æ•°é‡ï¼‰
                handle_elink = Entrez.elink(
                    db="pubmed",
                    id=pmid_list,
                    linkname="pubmed_pubmed_citedin,pubmed_pubmed_refs",
                    retmode="xml",
                    cmd="neighbor"
                )
                records_elink = Entrez.read(handle_elink)
                handle_elink.close()
                
                # å¤„ç†æ¯ä¸ª PMID çš„ç»“æœï¼Œåªè®¡ç®—æ•°é‡
                for i, record in enumerate(records_elink):
                    pmid = pmid_list[i] if i < len(pmid_list) else None
                    if not pmid:
                        continue
                        
                    cited_count = 0
                    references_count = 0
                    
                    if "LinkSetDb" in record:
                        for linkset in record["LinkSetDb"]:
                            if linkset["LinkName"] == "pubmed_pubmed_citedin" and "Link" in linkset:
                                cited_count = len(linkset["Link"])
                            elif linkset["LinkName"] == "pubmed_pubmed_refs" and "Link" in linkset:
                                references_count = len(linkset["Link"])
                    
                    # ä½¿ç”¨ COUNT_ONLY æ ‡è®°ä¼ é€’æ•°é‡ä¿¡æ¯
                    citation_dict[pmid] = (
                        [f"COUNT_ONLY:{cited_count}"],  # ç‰¹æ®Šæ ‡è®°è¡¨ç¤ºåªæœ‰æ•°é‡
                        [f"COUNT_ONLY:{references_count}"]
                    )
                
                break
                
            except HTTPError as e:
                if e.code in [429, 500, 502, 503, 504]:
                    wait_time = self.retry_wait_time * (attempt + 1)
                    self.logger.warning(f" æ‰¹é‡è·å–å¼•ç”¨æ•°é‡ HTTP é”™è¯¯ {e.code}ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯• ( å°è¯• {attempt + 1}/{self.max_retries})...")
                    self.stats["retries"] += 1
                    time.sleep(wait_time)
                    continue
                else:
                    self.logger.error(f" æ‰¹é‡è·å–å¼•ç”¨æ•°é‡ HTTP é”™è¯¯ : {e}")
                    break
            except Exception as e:
                error_msg = str(e).lower()
                if any(keyword in error_msg for keyword in ['eof', 'connection', 'closed', 'timeout', 'read failed']):
                    wait_time = self.retry_wait_time * (attempt + 1)
                    self.logger.warning(f" æ‰¹é‡è·å–å¼•ç”¨æ•°é‡ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯• ( å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                    self.stats["retries"] += 1
                    time.sleep(wait_time)
                    if attempt < self.max_retries - 1:
                        continue
                else:
                    self.logger.warning(f" æ‰¹é‡è·å–å¼•ç”¨æ•°é‡å¤±è´¥ : {e}")
                    if attempt < self.max_retries - 1:
                        self.stats["retries"] += 1
                        time.sleep(self.retry_wait_time)
                        continue
                break
        
        return citation_dict
    
    def create_record_dict(self, record: Dict[str, Any], publication_date: str, 
                          cited_by: List[str], references: List[str]) -> Dict[str, Any]:
        """
        åˆ›å»ºæ ‡å‡†åŒ–çš„æ–‡çŒ®è®°å½•å­—å…¸
        
        Args:
            record: åŸå§‹è®°å½•
            publication_date: å‡ºç‰ˆæ—¥æœŸ
            cited_by: è¢«å¼•ç”¨åˆ—è¡¨æˆ–æ•°é‡æ ‡è®°
            references: å‚è€ƒæ–‡çŒ®åˆ—è¡¨æˆ–æ•°é‡æ ‡è®°
            
        Returns:
            æ ‡å‡†åŒ–è®°å½•å­—å…¸
        """
        # å¤„ç†åªæœ‰æ•°é‡çš„æƒ…å†µ
        if cited_by and len(cited_by) == 1 and str(cited_by[0]).startswith("COUNT_ONLY:"):
            cited_count = int(str(cited_by[0]).replace("COUNT_ONLY:", ""))
            cited_by = []  # æ¸…ç©ºåˆ—è¡¨ï¼Œåªä¿ç•™æ•°é‡
        else:
            cited_count = len(cited_by) if cited_by else 0
            
        if references and len(references) == 1 and str(references[0]).startswith("COUNT_ONLY:"):
            references_count = int(str(references[0]).replace("COUNT_ONLY:", ""))
            references = []  # æ¸…ç©ºåˆ—è¡¨ï¼Œåªä¿ç•™æ•°é‡
        else:
            references_count = len(references) if references else 0
        return {
            'Title': record.get('TI', 'NA'),
            'Status': record.get('STAT', 'NA'),
            'Last_Revision_Date': record.get('LR', 'NA'),
            'ISSN': record.get('IS', 'NA'),
            'Type': record.get('PT', 'NA'),
            'Year_of_Publication': record.get('DP', 'NA').split(' ')[0] if 'DP' in record else 'NA',
            'Date_of_Electronic_Publication': record.get('DEP', 'NA'),
            'Publication_Date': publication_date,
            'Place_of_Publication': record.get('PL', 'NA'),
            'First_Author': record.get('FAU', 'NA'),
            'Authors': record.get('AU', 'NA'),
            'Affiliation': record.get('AD', 'NA'),
            'Abstract': record.get('AB', 'NA'),
            'Language': record.get('LA', 'NA'),
            'Keywords': record.get('OT', 'NA'),
            'PMID': record.get('PMID', 'NA'),
            'Medline_Volume': record.get('VI', 'NA'),
            'Medline_Issue': record.get('IP', 'NA'),
            'Medline_Pagination': record.get('PG', 'NA'),
            'DOI': record.get('LID', 'NA').split(' ')[0] if 'LID' in record else 'NA',
            'PMC': record.get('PMC', 'NA'),
            'Processing_History': record.get('PHST', 'NA'),
            'Publication_Status': record.get('PST', 'NA'),
            'Journal_Title_Abbreviation': record.get('TA', 'NA'),
            'Journal_Title': record.get('JT', 'NA'),
            'Journal_ID': record.get('JID', 'NA'),
            'Source': record.get('SO', 'NA'),
            'Grant_List': record.get('GR', 'NA'),
            'Cited_Count': cited_count,
            'Cited_By': cited_by,
            'References_Count': references_count,
            'References_PMID': references,
        }
    
    def check_existing_data(self, output_file: Path) -> tuple:
        """
        æ£€æŸ¥ç°æœ‰æ•°æ®ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            ( å·²å¤„ç†çš„ PMID é›†åˆ , å·²æœ‰æ•°æ®åˆ—è¡¨ )
        """
        try:
            if output_file.exists() and output_file.stat().st_size > 0:
                self.logger.info(f" ğŸ“ æ£€æµ‹åˆ°ç°æœ‰æ•°æ®æ–‡ä»¶ : {output_file}")
                existing_df = pd.read_csv(output_file, keep_default_na=False, na_values=[''])
                self.logger.info(f" ğŸ“Š å·²æœ‰ {len(existing_df)} ç¯‡æ–‡çŒ®ï¼Œå°†è¿›è¡Œæ–­ç‚¹ç»­ä¼  ")
                
                valid_pmids = existing_df['PMID'].dropna().astype(str)
                valid_pmids = [pmid for pmid in valid_pmids if pmid.strip() and pmid != 'nan']
                return set(valid_pmids), existing_df.to_dict('records')
            else:
                return set(), []
        except Exception as e:
            self.logger.error(f" è¯»å–ç°æœ‰æ•°æ®æ—¶å‡ºé”™ : {e}")
            return set(), []
    
    def save_results(self, data: List[Dict[str, Any]], output_file: Path):
        """
        ä¿å­˜ç»“æœåˆ° CSV æ–‡ä»¶ï¼ˆä½¿ç”¨ DataProcessor å¤„ç†å¼•ç”¨è¯¦æƒ…é…ç½®ï¼‰
        
        Args:
            data: æ•°æ®åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            # ä½¿ç”¨ DataProcessor æ¥å¤„ç† CSV ç”Ÿæˆï¼Œæ”¯æŒå¼•ç”¨è¯¦æƒ…é…ç½®
            from core.data_processor import DataProcessor
            
            # ä½¿ç”¨å½“å‰ PubMedFetcher çš„é…ç½®ï¼ˆåŒ…å«æ­£ç¡®çš„å¼•ç”¨è¯¦æƒ…è®¾ç½®ï¼‰
            output_config = {
                'csv_encoding': 'utf-8-sig',
                'date_format': '%Y-%m-%d',
                'citation_details': {
                    'fetch_detailed_pmid_lists': self.fetch_detailed_pmid_lists,
                    'citation_dir': 'citations'
                }
            }
            
            # åˆ›å»º DataProcessor å®ä¾‹
            processor = DataProcessor(output_config)
            
            # åˆ›å»ºç®€å•çš„æ¨¡æ¿ï¼ˆåªåŒ…å«åŸºæœ¬å­—æ®µï¼Œä¸åŒ…å«æå–å­—æ®µï¼‰
            basic_template = {
                'name': 'Basic PubMed Data',
                'version': '1.0',
                'description': 'Basic PubMed paper information',
                'fields': {}  # ç©ºå­—æ®µï¼Œåªä½¿ç”¨åŸºæœ¬ä¿¡æ¯
            }
            
            # ä½¿ç”¨ DataProcessor ç”Ÿæˆ CSVï¼ˆä¼šåº”ç”¨å¼•ç”¨è¯¦æƒ…é…ç½®ï¼‰
            success = processor.generate_csv(data, basic_template, output_file)
            
            if success:
                self.logger.debug(f" âœ… ç»“æœå·²ä¿å­˜ : {output_file}")
            else:
                # å¦‚æœ DataProcessor å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•
                self.logger.warning(" âš ï¸ DataProcessor ä¿å­˜å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ–¹æ³• ")
                self._save_results_basic(data, output_file)
            
        except Exception as e:
            self.logger.error(f" âŒ ä¿å­˜æ•°æ®æ—¶å‡ºé”™ : {e}")
            # å‡ºé”™æ—¶ä½¿ç”¨åŸºç¡€ä¿å­˜æ–¹æ³•
            self._save_results_basic(data, output_file)
    
    def _save_results_basic(self, data: List[Dict[str, Any]], output_file: Path):
        """
        åŸºç¡€ CSV ä¿å­˜æ–¹æ³•ï¼ˆä¸æ”¯æŒå¼•ç”¨è¯¦æƒ…é…ç½®ï¼‰
        
        Args:
            data: æ•°æ®åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            df = pd.DataFrame(data)
            df = df.fillna('NA')
            df.to_csv(output_file, index=False, na_rep='NA', encoding='utf-8-sig')
            
            # åˆ›å»ºå¤‡ä»½åˆ° results/backup ç›®å½•
            backup_dir = Path('results') / 'backup'
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            backup_filename = f"{output_file.stem}_backup.csv"
            backup_file = backup_dir / backup_filename
            
            df.to_csv(backup_file, index=False, na_rep='NA', encoding='utf-8-sig')
            self.logger.debug(f" âœ… åˆ›å»ºå¤‡ä»½æ–‡ä»¶ : {backup_file}")
            
            self.logger.debug(f" âœ… ç»“æœå·²ä¿å­˜ : {output_file}")
            
        except Exception as e:
            self.logger.error(f" âŒ åŸºç¡€ä¿å­˜æ–¹æ³•å¤±è´¥ : {e}")
            raise
    
    def fetch_by_query(self, query: str, resume: bool = True, max_results: int = None) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æŸ¥è¯¢è¯è·å–æ–‡çŒ®ä¿¡æ¯
        
        Args:
            query: PubMed æŸ¥è¯¢è¯
            resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ 
            max_results: æœ€å¤§ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            æ–‡çŒ®ä¿¡æ¯åˆ—è¡¨
        """
        self.logger.info(f" ğŸ” å¼€å§‹æ£€ç´¢ : {query}")
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        safe_query = re.sub(r'[ ^ \w\s-]', '', query.replace(' ', '_'))[:50]
        output_file = self.output_dir / f"pubminer_{safe_query}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        # æ£€æŸ¥ç°æœ‰æ•°æ®
        existing_pmids, existing_data = self.check_existing_data(output_file) if resume else (set(), [])
        
        # æœç´¢æ–‡çŒ®
        self.logger.info(" ğŸ“Š æ­£åœ¨æœç´¢æ–‡çŒ® ...")
        handle = self._fetch_with_retry(
            Entrez.esearch,
            db="pubmed",
            term=query,
            usehistory="y",
            retmax=0
        )
        search_results = Entrez.read(handle)
        handle.close()
        
        count = int(search_results.get("Count", 0) or 0)
        webenv = search_results.get("WebEnv")
        query_key = search_results.get("QueryKey")
        
        # åº”ç”¨æœ€å¤§ç»“æœæ•°é™åˆ¶
        if max_results is not None and max_results > 0:
            count = min(count, max_results)
            self.logger.info(f" ğŸ“š æ‰¾åˆ°æ–‡çŒ®æ€»æ•° : {search_results.get('Count', 0)}, é™åˆ¶è·å– : {count} ç¯‡ ")
        else:
            self.logger.info(f" ğŸ“š æ‰¾åˆ° {count} ç¯‡æ–‡çŒ® ")
        
        self.stats["total_articles"] = count
        
        if count == 0:
            self.logger.warning(" âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡çŒ® ")
            return []
        
        # è·å–æ‰€æœ‰ PMIDï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ åˆ¤æ–­ï¼‰
        if resume and existing_pmids:
            self.logger.info(" ğŸ” è·å– PMID åˆ—è¡¨ç”¨äºæ–­ç‚¹ç»­ä¼  ...")
            all_pmids = []
            pmid_batch_size = 9999
            
            for start in range(0, count, pmid_batch_size):
                retmax = min(pmid_batch_size, count - start)
                handle = self._fetch_with_retry(
                    Entrez.esearch,
                    db="pubmed",
                    term=query,
                    retstart=start,
                    retmax=retmax,
                    usehistory="n"
                )
                results = Entrez.read(handle)
                handle.close()
                all_pmids.extend(results["IdList"])
                time.sleep(self.api_wait_time)
            
            # è¿‡æ»¤å·²å¤„ç†çš„ PMID
            new_pmids = [pmid for pmid in all_pmids if pmid not in existing_pmids]
            self.logger.info(f" ğŸ“Š éœ€è¦å¤„ç† {len(new_pmids)} ç¯‡æ–°æ–‡çŒ® ")
            
            if len(new_pmids) == 0:
                self.logger.info(" âœ… æ‰€æœ‰æ–‡çŒ®éƒ½å·²å¤„ç†å®Œæˆ ")
                return existing_data
        
        # æ‰¹é‡è·å–æ–‡çŒ®è¯¦æƒ…
        self.logger.info(" ğŸ“š å¼€å§‹æ‰¹é‡è·å–æ–‡çŒ®è¯¦æƒ… ...")
        data = list(existing_data)  # å¤åˆ¶ç°æœ‰æ•°æ®
        
        # è¿›åº¦æ¡
        total_batches = (count + self.batch_size - 1) // self.batch_size
        batch_progress = tqdm(
            total=total_batches,
            desc=" ğŸ“¦ æ‰¹æ¬¡è¿›åº¦ ",
            unit="batch",
            position=0
        )
        
        processed_count = 0
        
        for start in range(0, count, self.batch_size):
            # è®¡ç®—å½“å‰æ‰¹æ¬¡åº”è¯¥è·å–çš„æ•°é‡
            current_batch_size = min(self.batch_size, count - start)
            
            # ä½¿ç”¨ WebEnv å’Œ QueryKey è·å–æ•°æ®
            handle = self._fetch_with_retry(
                Entrez.efetch,
                db="pubmed",
                rettype='medline',
                retmode="text",
                retstart=start,
                retmax=current_batch_size,
                webenv=webenv,
                query_key=query_key
            )
            records = list(Medline.parse(handle))
            handle.close()
            
            # è¿‡æ»¤å‡ºéœ€è¦å¤„ç†çš„è®°å½•
            if resume:
                records_to_process = [
                    r for r in records
                    if r.get('PMID') and r.get('PMID') not in existing_pmids
                ]
            else:
                records_to_process = records
            
            if not records_to_process:
                batch_progress.update(1)
                time.sleep(self.api_wait_time)
                continue
            
            # æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯
            batch_pmids = [record.get('PMID') for record in records_to_process]
            citation_data = self.fetch_citation_data_batch(batch_pmids)
            
            # å¤„ç†æ¯ç¯‡æ–‡çŒ®
            for record in records_to_process:
                pmid = record.get('PMID', 'NA')
                
                # è·å–å¼•ç”¨ä¿¡æ¯
                cited_by, references = citation_data.get(pmid, ([], []))
                
                # æå–å‘å¸ƒæ—¥æœŸ
                publication_date = self.extract_publication_date(record)
                
                # åˆ›å»ºè®°å½•
                record_dict = self.create_record_dict(record, publication_date, cited_by, references)
                data.append(record_dict)
                
                processed_count += 1
                self.stats["fetched_articles"] += 1
            
            # å®šæœŸä¿å­˜
            if processed_count > 0:
                self.save_results(data, output_file)
            
            batch_progress.update(1)
            time.sleep(self.api_wait_time)
        
        batch_progress.close()
        
        # æœ€ç»ˆä¿å­˜
        self.save_results(data, output_file)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        elapsed = (datetime.now() - self.stats["start_time"]).total_seconds() / 60
        self.logger.info(f" âœ… å®Œæˆ ! æ€»å…±å¤„ç† {len(data)} ç¯‡æ–‡çŒ® ")
        self.logger.info(f" â±ï¸ ç”¨æ—¶ : {elapsed:.2f} åˆ†é’Ÿ ")
        self.logger.info(f" ğŸ’¾ ç»“æœä¿å­˜è‡³ : {output_file}")
        
        return data
    
    def fetch_by_pmid_list(self, pmid_list: List[str], resume: bool = True) -> List[Dict[str, Any]]:
        """
        æ ¹æ® PMID åˆ—è¡¨è·å–æ–‡çŒ®ä¿¡æ¯
        
        Args:
            pmid_list: PMID åˆ—è¡¨
            resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ 
            
        Returns:
            æ–‡çŒ®ä¿¡æ¯åˆ—è¡¨
        """
        pmid_list = [str(pmid).strip() for pmid in pmid_list if str(pmid).strip()]
        self.logger.info(f" ğŸ“‹ æ ¹æ® PMID åˆ—è¡¨è·å–æ–‡çŒ®ä¿¡æ¯ï¼Œå…± {len(pmid_list)} ä¸ª PMID")
        
        if not pmid_list:
            self.logger.warning(" âš ï¸ PMID åˆ—è¡¨ä¸ºç©º ")
            return []
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        output_file = self.output_dir / f"pubminer_pmids_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        # æ£€æŸ¥ç°æœ‰æ•°æ®
        existing_pmids, existing_data = self.check_existing_data(output_file) if resume else (set(), [])
        
        # è¿‡æ»¤å·²å¤„ç†çš„ PMID
        if resume and existing_pmids:
            pmid_list = [pmid for pmid in pmid_list if pmid not in existing_pmids]
            self.logger.info(f" ğŸ“Š éœ€è¦å¤„ç† {len(pmid_list)} ä¸ªæ–° PMID")
        
        if not pmid_list:
            self.logger.info(" âœ… æ‰€æœ‰ PMID éƒ½å·²å¤„ç†å®Œæˆ ")
            return existing_data
        
        self.stats["total_articles"] = len(pmid_list)
        data = list(existing_data)
        
        # æ‰¹é‡å¤„ç†
        self.logger.info(" ğŸ“š å¼€å§‹æ‰¹é‡è·å–æ–‡çŒ®è¯¦æƒ… ...")
        
        total_batches = (len(pmid_list) + self.batch_size - 1) // self.batch_size
        batch_progress = tqdm(
            total=total_batches,
            desc=" ğŸ“¦ æ‰¹æ¬¡è¿›åº¦ ",
            unit="batch"
        )
        
        for i in range(0, len(pmid_list), self.batch_size):
            batch_pmids = pmid_list[i:i + self.batch_size]
            
            try:
                # è·å–æ–‡çŒ®è¯¦æƒ…
                handle = self._fetch_with_retry(
                    Entrez.efetch,
                    db="pubmed",
                    id=batch_pmids,
                    rettype='medline',
                    retmode="text"
                )
                records = list(Medline.parse(handle))
                handle.close()
                
                # æ‰¹é‡è·å–å¼•ç”¨ä¿¡æ¯
                citation_data = self.fetch_citation_data_batch(batch_pmids)
                
                # å¤„ç†æ¯ç¯‡æ–‡çŒ®
                for record in records:
                    pmid = record.get('PMID', 'NA')
                    
                    # è·å–å¼•ç”¨ä¿¡æ¯
                    cited_by, references = citation_data.get(pmid, ([], []))
                    
                    # æå–å‘å¸ƒæ—¥æœŸ
                    publication_date = self.extract_publication_date(record)
                    
                    # åˆ›å»ºè®°å½•
                    record_dict = self.create_record_dict(record, publication_date, cited_by, references)
                    data.append(record_dict)
                    
                    self.stats["fetched_articles"] += 1
                
                # å®šæœŸä¿å­˜
                self.save_results(data, output_file)
                
            except Exception as e:
                self.logger.error(f" âŒ å¤„ç†æ‰¹æ¬¡å¤±è´¥ : {e}")
                continue
            
            batch_progress.update(1)
            time.sleep(self.api_wait_time)
        
        batch_progress.close()
        
        # æœ€ç»ˆä¿å­˜
        self.save_results(data, output_file)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        elapsed = (datetime.now() - self.stats["start_time"]).total_seconds() / 60
        self.logger.info(f" âœ… å®Œæˆ ! æ€»å…±å¤„ç† {len(data)} ç¯‡æ–‡çŒ® ")
        self.logger.info(f" â±ï¸ ç”¨æ—¶ : {elapsed:.2f} åˆ†é’Ÿ ")
        self.logger.info(f" ğŸ’¾ ç»“æœä¿å­˜è‡³ : {output_file}")
        
        return data