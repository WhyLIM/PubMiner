# -*- coding: utf-8 -*-
"""
æ•°æ®å¤„ç†æ¨¡å—

è´Ÿè´£å¤„ç†åˆ†æç»“æœã€ç”Ÿæˆ CSV è¾“å‡ºã€æ•°æ®éªŒè¯å’Œç»Ÿè®¡æŠ¥å‘Š
"""

import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
from datetime import datetime
import json
import re

from utils.logger import LoggerMixin
from utils.file_handler import FileHandler

logger = logging.getLogger(__name__)


class DataProcessor(LoggerMixin):
    """æ•°æ®å¤„ç†å™¨"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨

        Args:
            config: è¾“å‡ºé…ç½®
        """
        self.config = config
        self.csv_encoding = config.get('csv_encoding', 'utf-8-sig')
        self.date_format = config.get('date_format', '%Y-%m-%d')
        self.na_values = config.get('na_values', ['NA', 'N/A', 'æœªæåŠ', 'æœªæ˜ç¡®', ''])
        self.max_cell_length = config.get('max_cell_length', 32767)
        self.enable_backup = config.get('enable_backup', True)

        # å¼•ç”¨è¯¦æƒ…é…ç½® - æ§åˆ¶æ˜¯å¦è·å–è¯¦ç»† PMID åˆ—è¡¨
        citation_config = config.get('citation_details', {})
        self.fetch_detailed_pmid_lists = citation_config.get('fetch_detailed_pmid_lists', True)
        self.citation_dir = citation_config.get('citation_dir', 'citations')
        self.citation_fields = ['Cited_By', 'References_PMID']  # å¼•ç”¨å­—æ®µåˆ—è¡¨

    def _generate_csv_filename(self, prefix: str = "", identifier: str = "") -> str:
        """
        ç”Ÿæˆ CSV æ–‡ä»¶å

        Args:
            prefix: æ–‡ä»¶åå‰ç¼€
            identifier: æ ‡è¯†ç¬¦ï¼ˆå¯é€‰ï¼‰

        Returns:
            ç”Ÿæˆçš„æ–‡ä»¶å
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # æ¸…ç†æ ‡è¯†ç¬¦
        if identifier:
            safe_identifier = re.sub(r'[^\w\s-]', ' ', identifier.replace(' ', '_'))[:50]
        else:
            safe_identifier = ""

        # æ„å»ºæ–‡ä»¶å
        parts = []
        if prefix:
            parts.append(prefix)
        if safe_identifier:
            parts.append(safe_identifier)
        parts.append(timestamp)

        return "_".join(parts) + ".csv"

    def _clean_cell_content(self, content: Any) -> str:
        """
        æ¸…ç†å•å…ƒæ ¼å†…å®¹

        Args:
            content: åŸå§‹å†…å®¹

        Returns:
            æ¸…ç†åçš„å†…å®¹
        """
        if content is None:
            return 'NA'

        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        content_str = str(content).strip()

        # å¤„ç†ç©ºå€¼
        if content_str in self.na_values:
            return 'NA'

        # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
        content_str = re.sub(r'\s+', ' ', content_str)  # åˆå¹¶å¤šä¸ªç©ºç™½å­—ç¬¦
        content_str = content_str.replace('\r\n', '').replace('\n', ' ').replace('\r', ' ')

        # é•¿åº¦é™åˆ¶
        if len(content_str) > self.max_cell_length:
            content_str = content_str[:self.max_cell_length - 3] + '...'
            self.logger.debug(f"å•å…ƒæ ¼å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­åˆ° {self.max_cell_length} å­—ç¬¦")

        return content_str

    def _needs_detailed_pmid_lists(self, paper: Dict[str, Any]) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦è·å–è¯¦ç»†çš„ PMID åˆ—è¡¨

        Args:
            paper: æ–‡çŒ®æ•°æ®

        Returns:
            æ˜¯å¦éœ€è¦è¯¦ç»† PMID åˆ—è¡¨
        """
        return self.fetch_detailed_pmid_lists

    def _save_citation_file(self, paper: Dict[str, Any], output_dir: Path) -> Optional[str]:
        """
        ä¿å­˜å¼•ç”¨ä¿¡æ¯åˆ°å•ç‹¬çš„ JSON æ–‡ä»¶

        Args:
            paper: æ–‡çŒ®æ•°æ®
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            å¼•ç”¨æ–‡ä»¶åï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        try:
            pmid = paper.get('PMID', 'unknown')
            citation_filename = f"citations_{pmid}.json"

            # åˆ›å»ºå¼•ç”¨ç›®å½•
            citation_dir = output_dir / self.citation_dir
            citation_dir.mkdir(exist_ok=True)

            citation_file_path = citation_dir / citation_filename

            # æ„å»ºå¼•ç”¨ä¿¡æ¯æ•°æ®
            citation_info = {
                'PMID': pmid,
                'Cited_By': paper.get('Cited_By', []),
                'Cited_Count': len(paper.get('Cited_By', [])),
                'References_PMID': paper.get('References_PMID', []),
                'References_Count': len(paper.get('References_PMID', [])),
                'Last_Updated': datetime.now().isoformat(),
                'Data_Source': 'PubMed API',
                'Fetcher_Version': 'PubMiner v1.0'
            }

            # ä¿å­˜ JSON æ–‡ä»¶
            with open(citation_file_path, 'w', encoding='utf-8') as f:
                json.dump(citation_info, f, ensure_ascii=False, indent=2)

            self.logger.debug(f"âœ… ä¿å­˜å¼•ç”¨æ–‡ä»¶: {citation_filename}")
            return citation_filename

        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜å¼•ç”¨æ–‡ä»¶å¤±è´¥ PMID {pmid}: {e}")
            return None

    def _create_csv_headers(self, template: Dict[str, Any]) -> List[str]:
        """
        åˆ›å»º CSV è¡¨å¤´

        Args:
            template: æå–æ¨¡æ¿

        Returns:
            è¡¨å¤´åˆ—è¡¨
        """
        # åŸºæœ¬ä¿¡æ¯è¡¨å¤´ - æ£€æŸ¥æ•°æ®ä¸­å®é™…å­˜åœ¨çš„å­—æ®µ
        basic_headers = [
            'PMID', 'Title', 'Authors', 'Year_of_Publication', 'Journal_Title', 'DOI', 'Abstract', 'Keywords'
        ]

        # å¯é€‰çš„åŸºç¡€å­—æ®µï¼ˆå¯èƒ½ä¸å­˜åœ¨äºæ‰€æœ‰è®°å½•ä¸­ï¼‰
        optional_headers = ['Text_Source', 'Extraction_Status', 'Publication_Date', 'First_Author', 'Language']

        # å¼•ç”¨ä¿¡æ¯è¡¨å¤´ï¼ˆç»Ÿä¸€åªåŒ…å«ç»Ÿè®¡å­—æ®µï¼Œä¸åŒ…å«è¯¦ç»†åˆ—è¡¨å­—æ®µï¼‰
        citation_headers = ['Cited_Count', 'References_Count']

        # å¦‚æœå¯ç”¨äº†è¯¦ç»† PMID åˆ—è¡¨åŠŸèƒ½ï¼Œæ·»åŠ ç›¸å…³ç®¡ç†å­—æ®µ
        if self.fetch_detailed_pmid_lists:
            citation_headers.extend(['Storage_Type', 'Citation_File', 'Last_Updated'])

        # æå–å­—æ®µè¡¨å¤´
        extraction_headers = []
        fields = template.get('fields', {})

        for field_key, field_info in fields.items():
            csv_header = field_info.get('csv_header', field_key)
            extraction_headers.append(csv_header)

        return basic_headers + optional_headers + citation_headers + extraction_headers

    def _prepare_row_data(self, paper: Dict[str, Any], template: Dict[str, Any], output_dir: Path) -> Dict[str, str]:
        """
        å‡†å¤‡å•è¡Œæ•°æ®

        Args:
            paper: æ–‡çŒ®è®°å½•
            template: æå–æ¨¡æ¿
            output_dir: è¾“å‡ºç›®å½•ï¼ˆç”¨äºä¿å­˜å¼•ç”¨æ–‡ä»¶ï¼‰

        Returns:
            è¡Œæ•°æ®å­—å…¸
        """
        pmid = paper.get('PMID', '')

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¯¦ç»† PMID åˆ—è¡¨
        needs_detailed_lists = self._needs_detailed_pmid_lists(paper)

        # åŸºæœ¬ä¿¡æ¯
        row_data = {
            'PMID': self._clean_cell_content(pmid),
            'Title': self._clean_cell_content(paper.get('Title', '')),
            'Authors': self._clean_cell_content(paper.get('Authors', '')),
            'Year_of_Publication': self._clean_cell_content(paper.get('Year_of_Publication', '')),
            'Journal_Title': self._clean_cell_content(paper.get('Journal_Title', '')),
            'DOI': self._clean_cell_content(paper.get('DOI', '')),
            'Abstract': self._clean_cell_content(paper.get('Abstract', '')),
            'Keywords': self._clean_cell_content(paper.get('Keywords', '')),
            'Text_Source': self._clean_cell_content(paper.get('text_source', '')),
            'Extraction_Status': self._clean_cell_content(paper.get('extraction_status', '')),
            'Publication_Date': self._clean_cell_content(paper.get('Publication_Date', '')),
            'First_Author': self._clean_cell_content(paper.get('First_Author', '')),
            'Language': self._clean_cell_content(paper.get('Language', ''))
        }

        # å¤„ç†å¼•ç”¨ä¿¡æ¯
        if self.fetch_detailed_pmid_lists:
            # è¯¦ç»†æ¨¡å¼ï¼šä¿å­˜å®Œæ•´ PMID åˆ—è¡¨åˆ° JSON æ–‡ä»¶
            citation_filename = self._save_citation_file(paper, output_dir)

            if citation_filename:
                self.logger.info(
                    f"PMID {pmid}: ä¿å­˜è¯¦ç»†å¼•ç”¨åˆ—è¡¨ (è¢«å¼•ç”¨æ•°: {len(paper.get('Cited_By', []))}, å‚è€ƒæ–‡çŒ®æ•°: {len(paper.get('References_PMID', []))})"
                )

                # è¯¦ç»†æ¨¡å¼çš„ CSV å­—æ®µ
                row_data.update({
                    'Cited_Count': len(paper.get('Cited_By', [])),
                    'References_Count': len(paper.get('References_PMID', [])),
                    'Storage_Type': 'detailed_lists',
                    'Citation_File': citation_filename,
                    'Last_Updated': datetime.now().isoformat()
                })
            else:
                # JSON æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼Œå›é€€åˆ°ç»Ÿè®¡æ¨¡å¼
                self.logger.warning(f"PMID {pmid}: è¯¦ç»†åˆ—è¡¨ä¿å­˜å¤±è´¥ï¼Œå›é€€åˆ°ç»Ÿè®¡æ¨¡å¼")
                row_data.update({
                    'Cited_Count': len(paper.get('Cited_By', [])),
                    'References_Count': len(paper.get('References_PMID', [])),
                    'Storage_Type': 'counts_only',
                    'Citation_File': 'NA',
                    'Last_Updated': datetime.now().isoformat()
                })
        else:
            # ä»…æ•°é‡æ¨¡å¼ï¼šåªä¿å­˜å¼•ç”¨ç»Ÿè®¡ï¼Œä¸åŒ…å«é¢å¤–å­—æ®µ
            cited_count = paper.get('Cited_Count', len(paper.get('Cited_By', [])))
            references_count = paper.get('References_Count', len(paper.get('References_PMID', [])))

            self.logger.info(f"PMID {pmid}: ä»…æ•°é‡æ¨¡å¼ (è¢«å¼•ç”¨æ•°: {cited_count}, å‚è€ƒæ–‡çŒ®æ•°: {references_count})")
            row_data.update({'Cited_Count': cited_count, 'References_Count': references_count})

        # æå–çš„å­—æ®µä¿¡æ¯
        fields = template.get('fields', {})
        for field_key, field_info in fields.items():
            csv_header = field_info.get('csv_header', field_key)
            field_value = paper.get(field_key, '')
            row_data[csv_header] = self._clean_cell_content(field_value)

        return row_data

    def generate_csv(self, papers: List[Dict[str, Any]], template: Dict[str, Any], output_path: Path,
                    filename_prefix: str = "", identifier: str = "") -> bool:
        """
        ç”Ÿæˆ CSV è¾“å‡ºæ–‡ä»¶

        Args:
            papers: æ–‡çŒ®åˆ—è¡¨
            template: æå–æ¨¡æ¿
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥æ˜¯ç›®å½•æˆ–å®Œæ•´æ–‡ä»¶è·¯å¾„ï¼‰
            filename_prefix: æ–‡ä»¶åå‰ç¼€ï¼ˆå¯é€‰ï¼‰
            identifier: æ ‡è¯†ç¬¦ï¼ˆå¯é€‰ï¼‰

        Returns:
            æ˜¯å¦ç”ŸæˆæˆåŠŸ
        """
        try:
            # å¦‚æœ output_path æ˜¯ç›®å½•ï¼Œç”Ÿæˆæ–‡ä»¶å
            if output_path.is_dir():
                filename = self._generate_csv_filename(filename_prefix, identifier)
                output_path = output_path / filename

            self.logger.info(f"ğŸ“Š å¼€å§‹ç”Ÿæˆ CSV æ–‡ä»¶: {output_path}")

            if not papers:
                self.logger.warning("âš ï¸ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
                return False

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = output_path.parent
            output_dir.mkdir(parents=True, exist_ok=True)

            # ç»Ÿè®¡å­˜å‚¨æ–¹å¼ä½¿ç”¨æƒ…å†µ
            detailed_lists_count = 0
            counts_only_count = 0

            # åˆ›å»ºè¡¨å¤´
            headers = self._create_csv_headers(template)

            # å‡†å¤‡æ•°æ®
            rows_data = []
            for paper in papers:
                row_data = self._prepare_row_data(paper, template, output_dir)
                rows_data.append(row_data)

                # ç»Ÿè®¡å­˜å‚¨æ–¹å¼
                storage_type = row_data.get('Storage_Type', 'counts_only')
                if storage_type == 'detailed_lists':
                    detailed_lists_count += 1
                else:
                    counts_only_count += 1

            # åˆ›å»º DataFrame
            df = pd.DataFrame(rows_data, columns=headers)

            # å¡«å……ç¼ºå¤±å€¼
            df = df.fillna('NA')

            # ä¿å­˜ CSV æ–‡ä»¶
            df.to_csv(output_path, index=False, encoding=self.csv_encoding, na_rep='NA')

            # åˆ›å»ºå¤‡ä»½åˆ° results/backup ç›®å½•
            if self.enable_backup:
                backup_dir = Path('results') / 'backup'
                backup_dir.mkdir(parents=True, exist_ok=True)

                backup_filename = f"{output_path.stem}_backup.csv"
                backup_path = backup_dir / backup_filename

                df.to_csv(backup_path, index=False, encoding=self.csv_encoding, na_rep='NA')
                self.logger.debug(f"âœ… åˆ›å»ºå¤‡ä»½æ–‡ä»¶: {backup_path}")

            # è®°å½•å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
            self.logger.info(f"âœ… CSV æ–‡ä»¶ç”ŸæˆæˆåŠŸ: {output_path}")
            self.logger.info(f"ğŸ“‹ åŒ…å« {len(df)} è¡Œæ•°æ®ï¼Œ{len(df.columns)} åˆ—")
            self.logger.info(f"ğŸ“Š å­˜å‚¨æ–¹å¼ç»Ÿè®¡: è¯¦ç»†åˆ—è¡¨ {detailed_lists_count} ç¯‡ï¼Œä»…ç»Ÿè®¡ {counts_only_count} ç¯‡")

            if detailed_lists_count > 0:
                citation_dir = output_path.parent / self.citation_dir
                self.logger.info(f"ğŸ“ è¯¦ç»†å¼•ç”¨åˆ—è¡¨ä¿å­˜åœ¨: {citation_dir}")
                self.logger.info(f"ğŸ’¡ è¯¦ç»†åˆ—è¡¨æ¨¡å¼æä¾›å®Œæ•´çš„ PMID å¼•ç”¨ä¿¡æ¯")

            return True

        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆ CSV æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def load_citation_data(self, csv_file_path: Path, pmid: str) -> Optional[Dict[str, Any]]:
        """
        ä»åˆ†ç¦»å­˜å‚¨ä¸­åŠ è½½å¼•ç”¨æ•°æ®

        Args:
            csv_file_path: CSV æ–‡ä»¶è·¯å¾„
            pmid: æ–‡çŒ® PMID

        Returns:
            å¼•ç”¨æ•°æ®å­—å…¸ï¼Œå¦‚æœå¤±è´¥è¿”å› None
        """
        try:
            # è¯»å– CSV æ–‡ä»¶è·å–å¼•ç”¨æ–‡ä»¶å
            df = pd.read_csv(csv_file_path)
            paper_row = df[df['PMID'].astype(str) == str(pmid)]

            if paper_row.empty:
                self.logger.warning(f"æœªæ‰¾åˆ° PMID {pmid} çš„æ•°æ®")
                return None

            storage_type = paper_row['Storage_Type'].iloc[0] if 'Storage_Type' in paper_row.columns else 'counts_only'

            if storage_type == 'detailed_lists':
                citation_file = paper_row['Citation_File'].iloc[0]
                citation_path = csv_file_path.parent / citation_file

                if citation_path.exists():
                    with open(citation_path, 'r', encoding='utf-8') as f:
                        return json.load(f)
                else:
                    self.logger.error(f"å¼•ç”¨æ–‡ä»¶ä¸å­˜åœ¨: {citation_path}")
                    return None
            else:
                # å†…è”å­˜å‚¨ï¼Œä» CSV ä¸­ç›´æ¥è¯»å–
                citation_data = {
                    'PMID':
                    pmid,
                    'Cited_By':
                    paper_row.get('Cited_By', []).iloc[0] if 'Cited_By' in paper_row.columns else [],
                    'References_PMID':
                    paper_row.get('References_PMID', []).iloc[0] if 'References_PMID' in paper_row.columns else [],
                    'Cited_Count':
                    paper_row.get('Cited_Count', 0).iloc[0] if 'Cited_Count' in paper_row.columns else 0,
                    'References_Count':
                    paper_row.get('References_Count', 0).iloc[0] if 'References_Count' in paper_row.columns else 0,
                    'Storage_Type':
                    storage_type
                }
                return citation_data

        except Exception as e:
            self.logger.error(f"åŠ è½½å¼•ç”¨æ•°æ®å¤±è´¥ PMID {pmid}: {e}")
            return None

    def get_storage_statistics(self, csv_file_path: Path) -> Dict[str, Any]:
        """
        è·å–å­˜å‚¨æ–¹å¼ç»Ÿè®¡ä¿¡æ¯

        Args:
            csv_file_path: CSV æ–‡ä»¶è·¯å¾„

        Returns:
            å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            df = pd.read_csv(csv_file_path)

            if 'Storage_Type' not in df.columns:
                return {
                    'total_papers': len(df),
                    'detailed_lists': 0,
                    'counts_only': len(df),
                    'storage_types': {
                        'counts_only': len(df)
                    }
                }

            storage_counts = df['Storage_Type'].value_counts().to_dict()

            stats = {
                'total_papers': len(df),
                'detailed_lists': storage_counts.get('detailed_lists', 0),
                'counts_only': storage_counts.get('counts_only', 0),
                'storage_types': storage_counts,
                'detailed_lists_rate': storage_counts.get('detailed_lists', 0) / len(df) * 100 if len(df) > 0 else 0
            }

            return stats

        except Exception as e:
            self.logger.error(f"è·å–å­˜å‚¨ç»Ÿè®¡å¤±è´¥: {e}")
            return {}

    def generate_statistics(self, papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¤„ç†ç»Ÿè®¡ä¿¡æ¯

        Args:
            papers: æ–‡çŒ®åˆ—è¡¨

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not papers:
            return {'total_papers': 0, 'analyzed_papers': 0, 'extracted_fields': 0, 'processing_time': 0, 'success_rate': 0}

        # åŸºæœ¬ç»Ÿè®¡
        total_papers = len(papers)
        analyzed_papers = len([p for p in papers if p.get('extraction_status') == 'success'])

        # æ–‡æœ¬æ¥æºç»Ÿè®¡
        text_sources = {}
        for paper in papers:
            source = paper.get('text_source', 'unknown')
            text_sources[source] = text_sources.get(source, 0) + 1

        # æå–çŠ¶æ€ç»Ÿè®¡
        extraction_statuses = {}
        for paper in papers:
            status = paper.get('extraction_status', 'unknown')
            extraction_statuses[status] = extraction_statuses.get(status, 0) + 1

        # å­—æ®µæå–ç»Ÿè®¡
        extracted_fields = 0
        field_stats = {}

        for paper in papers:
            if paper.get('extraction_status') == 'success':
                for key, value in paper.items():
                    if (not key.startswith(('PMID', 'Title', 'Authors', 'Year', 'Journal', 'DOI', 'Abstract'))
                            and not key.endswith(('_status', '_error', '_time', '_source', '_length')) and value
                            and value != 'NA' and value != 'æœªæåŠ'):

                        field_stats[key] = field_stats.get(key, 0) + 1
                        extracted_fields += 1

        # å¤„ç†æ—¶é—´ç»Ÿè®¡
        processing_times = [p.get('extraction_time', 0) for p in papers if p.get('extraction_time')]

        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0

        # æˆåŠŸç‡
        success_rate = analyzed_papers / total_papers if total_papers > 0 else 0

        stats = {
            'total_papers': total_papers,
            'analyzed_papers': analyzed_papers,
            'extracted_fields': extracted_fields,
            'success_rate': round(success_rate * 100, 2),
            'avg_processing_time': round(avg_processing_time, 2),
            'text_sources': text_sources,
            'extraction_statuses': extraction_statuses,
            'field_statistics': field_stats,
            'processing_time': sum(processing_times)
        }

        return stats

    def generate_report(self, papers: List[Dict[str, Any]], template: Dict[str, Any], output_dir: Path) -> bool:
        """
        ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š

        Args:
            papers: æ–‡çŒ®åˆ—è¡¨
            template: æå–æ¨¡æ¿
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            æ˜¯å¦ç”ŸæˆæˆåŠŸ
        """
        try:
            self.logger.info("ğŸ“‹ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir.mkdir(parents=True, exist_ok=True)

            # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            stats = self.generate_statistics(papers)

            # åˆ›å»ºæŠ¥å‘Šå†…å®¹
            report = {
                'generation_time': datetime.now().strftime(self.date_format + '%H:%M:%S'),
                'template_info': {
                    'name': template.get('name', 'Unknown'),
                    'description': template.get('description', ''),
                    'version': template.get('version', '1.0'),
                    'fields_count': len(template.get('fields', {}))
                },
                'processing_statistics': stats,
                'field_details': []
            }

            # å­—æ®µè¯¦ç»†ä¿¡æ¯
            fields = template.get('fields', {})
            for field_key, field_info in fields.items():
                csv_header = field_info.get('csv_header', field_key)
                field_name = field_info.get('name', field_key)

                # ç»Ÿè®¡è¯¥å­—æ®µçš„æå–æƒ…å†µ
                extracted_count = 0
                sample_values = []

                for paper in papers:
                    if paper.get('extraction_status') == 'success':
                        value = paper.get(field_key, '')
                        if value and value != 'NA' and value != 'æœªæåŠ':
                            extracted_count += 1
                            if len(sample_values) < 3:  # æ”¶é›†æ ·æœ¬å€¼
                                sample_values.append(value[:100])  # æˆªå–å‰ 100 å­—ç¬¦

                field_detail = {
                    'field_key':
                    field_key,
                    'field_name':
                    field_name,
                    'csv_header':
                    csv_header,
                    'description':
                    field_info.get('description', ''),
                    'required':
                    field_info.get('required', False),
                    'extracted_count':
                    extracted_count,
                    'extraction_rate':
                    round(extracted_count / stats['analyzed_papers'] * 100, 2) if stats['analyzed_papers'] > 0 else 0,
                    'sample_values':
                    sample_values
                }

                report['field_details'].append(field_detail)

            # ä¿å­˜æŠ¥å‘Š
            report_file = output_dir / 'analysis_report.json'
            FileHandler.save_json(report, report_file)

            # ç”Ÿæˆç®€åŒ–çš„æ–‡æœ¬æŠ¥å‘Š
            text_report = self._generate_text_report(report)
            text_report_file = output_dir / 'analysis_report.txt'
            FileHandler.save_text(text_report, text_report_file)

            self.logger.info(f"âœ… åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            return True

        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆåˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            return False

    def _generate_text_report(self, report: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„æŠ¥å‘Š

        Args:
            report: æŠ¥å‘Šæ•°æ®

        Returns:
            æ–‡æœ¬æŠ¥å‘Šå†…å®¹
        """
        lines = []

        # æ ‡é¢˜
        lines.append("=" * 60)
        lines.append("PubMiner æ–‡çŒ®åˆ†ææŠ¥å‘Š")
        lines.append("=" * 60)
        lines.append(f"ç”Ÿæˆæ—¶é—´: {report['generation_time']}")
        lines.append("")

        # æ¨¡æ¿ä¿¡æ¯
        template_info = report['template_info']
        lines.append("ğŸ“‹ æå–æ¨¡æ¿ä¿¡æ¯:")
        lines.append(f"åç§°: {template_info['name']}")
        lines.append(f"æè¿°: {template_info['description']}")
        lines.append(f"ç‰ˆæœ¬: {template_info['version']}")
        lines.append(f"å­—æ®µæ•°é‡: {template_info['fields_count']}")
        lines.append("")

        # å¤„ç†ç»Ÿè®¡
        stats = report['processing_statistics']
        lines.append("ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        lines.append(f"æ€»æ–‡çŒ®æ•°: {stats['total_papers']}")
        lines.append(f"æˆåŠŸåˆ†æ: {stats['analyzed_papers']}")
        lines.append(f"æˆåŠŸç‡: {stats['success_rate']}%")
        lines.append(f"æå–å­—æ®µæ€»æ•°: {stats['extracted_fields']}")
        lines.append(f"å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']} ç§’")
        lines.append("")

        # æ–‡æœ¬æ¥æºç»Ÿè®¡
        if 'text_sources' in stats:
            lines.append("ğŸ“„ æ–‡æœ¬æ¥æºåˆ†å¸ƒ:")
            for source, count in stats['text_sources'].items():
                lines.append(f"{source}: {count}")
            lines.append("")

        # æå–çŠ¶æ€ç»Ÿè®¡
        if 'extraction_statuses' in stats:
            lines.append("ğŸ¯ æå–çŠ¶æ€åˆ†å¸ƒ:")
            for status, count in stats['extraction_statuses'].items():
                lines.append(f"{status}: {count}")
            lines.append("")

        # å­—æ®µæå–è¯¦æƒ…
        lines.append("ğŸ” å­—æ®µæå–è¯¦æƒ…:")
        lines.append("-" * 40)

        for field_detail in report['field_details']:
            lines.append(f"å­—æ®µ: {field_detail['field_name']} ({field_detail['csv_header']})")
            lines.append(f"æå–æ•°é‡: {field_detail['extracted_count']}")
            lines.append(f"æå–ç‡: {field_detail['extraction_rate']}%")
            lines.append(f"æ˜¯å¦å¿…éœ€: {'æ˜¯'if field_detail['required'] else'å¦'}")

            if field_detail['sample_values']:
                lines.append("æ ·æœ¬å€¼:")
                for i, sample in enumerate(field_detail['sample_values'], 1):
                    lines.append(f"{i}. {sample}")

            lines.append("")

        lines.append("=" * 60)
        lines.append("æŠ¥å‘Šç»“æŸ")

        return "\n".join(lines)

    def validate_data_quality(self, papers: List[Dict[str, Any]], template: Dict[str, Any]) -> Dict[str, Any]:
        """
        éªŒè¯æ•°æ®è´¨é‡

        Args:
            papers: æ–‡çŒ®åˆ—è¡¨
            template: æå–æ¨¡æ¿

        Returns:
            è´¨é‡éªŒè¯ç»“æœ
        """
        validation_result = {
            'total_papers': len(papers),
            'quality_issues': [],
            'field_completeness': {},
            'data_consistency': {},
            'overall_quality_score': 0
        }

        if not papers:
            return validation_result

        fields = template.get('fields', {})

        # æ£€æŸ¥å¿…éœ€å­—æ®µçš„å®Œæ•´æ€§
        for field_key, field_info in fields.items():
            if field_info.get('required', False):
                field_name = field_info.get('name', field_key)
                missing_count = 0

                for paper in papers:
                    if paper.get('extraction_status') == 'success':
                        value = paper.get(field_key, '')
                        if not value or value in self.na_values:
                            missing_count += 1

                completeness_rate = 1 - (missing_count / len(papers))
                validation_result['field_completeness'][field_name] = {
                    'completeness_rate': round(completeness_rate * 100, 2),
                    'missing_count': missing_count
                }

                if completeness_rate < 0.8:  # å®Œæ•´ç‡ä½äº 80% å‘å‡ºè­¦å‘Š
                    validation_result['quality_issues'].append(f"å¿…éœ€å­—æ®µ'{field_name}'å®Œæ•´ç‡è¾ƒä½: {completeness_rate*100:.1f}%")

        # è®¡ç®—æ•´ä½“è´¨é‡åˆ†æ•°
        success_rate = len([p for p in papers if p.get('extraction_status') == 'success']) / len(papers)
        avg_completeness = sum(info['completeness_rate'] for info in validation_result['field_completeness'].values()) / len(
            validation_result['field_completeness']) if validation_result['field_completeness'] else 0

        validation_result['overall_quality_score'] = round((success_rate * 0.5 + avg_completeness / 100 * 0.5) * 100, 2)

        return validation_result
